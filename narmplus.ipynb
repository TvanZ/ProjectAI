{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NARM+\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing functions.\n",
    "import pandas\n",
    "import csv\n",
    "from collections import Counter, OrderedDict, defaultdict, namedtuple\n",
    "import itertools\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# PyTorch can run on CPU or on Nvidia GPU (video card) using CUDA\n",
    "# This cell selects the GPU if one is available.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.synchronize() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.24.2'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11832\n",
      "67172\n"
     ]
    }
   ],
   "source": [
    "# Create sessions\n",
    "sample = pandas.read_pickle(\"./data/processedData.pkl\")\n",
    "sample['SESSION'] = pandas.to_datetime(sample['TIMESTAMP'],unit='s').dt.date\n",
    "\n",
    "print(len(sample[\"USERID\"].unique()))\n",
    "print(len(sample[\"PRODUCTID\"].unique()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of sessions per user\n",
      "5.765128465179175\n",
      "Average number of clicks per session\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11.017386715142274"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Average number of sessions per user\")\n",
    "print(sample.groupby('USERID')['SESSION'].nunique().mean())\n",
    "\n",
    "print(\"Average number of clicks per session\")\n",
    "sample.groupby(['USERID', 'SESSION'])['ACTION'].count().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATASET CREATION\n",
    "\n",
    "# read CSV file\n",
    "# sample = pandas.read_csv(\"./data/UserBehavior.csv\", names=[\"USERID\", \"PRODUCTID\", \"CATEGORYID\", \"ACTION\", \"TIMESTAMP\"])\n",
    "\n",
    "# edit size and print size of userID\n",
    "# sample.set_index(\"USERID\")\n",
    "# sampleNew = sample[sample[\"USERID\"] < 10319]\n",
    "# print(len(sampleNew[\"USERID\"].unique()))\n",
    "# print(len(sampleNew[\"PRODUCTID\"].unique()))\n",
    "\n",
    "#sampleNew.to_pickle('./data/datasetName.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### DO NOT RUN THIS CELL IF YOU READ THE PICKLE processedData\n",
    "\n",
    "# ##Preprocessing the Data\n",
    "# #Remove products that occur less than 5 times\n",
    "# productsize = sample.groupby([\"PRODUCTID\"]).size() > 4\n",
    "# indexProducts = productsize.index[productsize]\n",
    "# productTotal = sample[sample[\"PRODUCTID\"].isin(indexProducts)]\n",
    "\n",
    "# #Remove sessions that have a length of 1 or less\n",
    "# #Remove users that have 2 or less sessions\n",
    "# sessionList = productTotal.groupby([\"USERID\",\"SESSION\"]).size() > 1\n",
    "# sessionBase = sessionList.to_frame(name = \"realSESSION\").reset_index()\n",
    "# enoughSessions = sessionBase.groupby([\"USERID\", \"realSESSION\"]).size() > 2\n",
    "# trueSessionBase = enoughSessions.to_frame(name = \"enoughSESSIONS\").reset_index()\n",
    "\n",
    "\n",
    "# #Merge DataSets such that original Dataset is restored with newly added filters\n",
    "# totalDataSet = productTotal.merge(trueSessionBase)\n",
    "# totalDataSet = totalDataSet[totalDataSet[\"enoughSESSIONS\"] == True]\n",
    "\n",
    "# #Drop useless columns\n",
    "# totalDataSet = totalDataSet.drop(\"enoughSESSIONS\", axis=1)\n",
    "# totalDataSet = totalDataSet.drop(\"realSESSION\", axis=1)\n",
    "# sample = totalDataSet\n",
    "\n",
    "# #Piece of mind stuff\n",
    "# userList = totalDataSet[\"USERID\"].unique()\n",
    "# productList = totalDataSet[\"PRODUCTID\"].unique()\n",
    "# print(len(totalDataSet2))\n",
    "# print(len(userList))\n",
    "# print(len(productList))\n",
    "\n",
    "\n",
    "# sample.to_pickle(\"./data/processedData.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "userList = sample[\"USERID\"].unique()\n",
    "productList = sample[\"PRODUCTID\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we first define a class that can map a product to an ID (p2i)\n",
    "# and back (i2p).\n",
    "\n",
    "class OrderedCounter(Counter, OrderedDict):\n",
    "    \"\"\"Counter that remembers the order elements are first seen\"\"\"\n",
    "    def __repr__(self):\n",
    "        return '%s(%r)' % (self.__class__.__name__,\n",
    "                      OrderedDict(self))\n",
    "    def __reduce__(self):\n",
    "        return self.__class__, (OrderedDict(self),)\n",
    "\n",
    "\n",
    "class Vocabulary:\n",
    "    \"\"\"A vocabulary, assigns IDs to tokens\"\"\"\n",
    "    def __init__(self):\n",
    "        self.freqs = OrderedCounter()\n",
    "        self.users = []\n",
    "        self.u2i = {}\n",
    "        self.i2u = []\n",
    "        self.p2i = {}\n",
    "        self.i2p = []\n",
    "\n",
    "    def count_product(self, t):\n",
    "        self.freqs[t] += 1\n",
    "    \n",
    "    def count_user(self, t):\n",
    "        self.users.append(t)\n",
    "\n",
    "    def add_product(self, t):\n",
    "        self.p2i[t] = len(self.p2i)\n",
    "        self.i2p.append(t) \n",
    "        \n",
    "    def add_user(self, t):\n",
    "        self.u2i[t] = len(self.u2i)\n",
    "        self.i2u.append(t)\n",
    "\n",
    "    def build(self, min_freq=0):\n",
    "        self.add_product(\"<unk>\")  # reserve 0 for <unk> (unknown products (products only occuring in test set))\n",
    "        self.add_user(\"<unk>\")\n",
    "        tok_freq = list(self.freqs.items())\n",
    "        tok_freq.sort(key=lambda x: x[1], reverse=True)\n",
    "        for tok, freq in tok_freq:\n",
    "            if freq >= min_freq:\n",
    "                self.add_product(tok)\n",
    "        for user in self.users:\n",
    "            self.add_user(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 67173\n"
     ]
    }
   ],
   "source": [
    "# This process should be deterministic and should have the same result \n",
    "# if run multiple times on the same data set.\n",
    "\n",
    "def build_voc(userList, productList):\n",
    "    v = Vocabulary()\n",
    "    for product in productList:\n",
    "        v.count_product(product)\n",
    "    for user in userList:\n",
    "        v.count_user(user)\n",
    "    v.build()\n",
    "    return v\n",
    "\n",
    "v = build_voc(userList, productList)\n",
    "print(\"Vocabulary size:\", len(v.p2i))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2268318, 2333346], [4365585, 230380], [2951368, 3108797], [2734026, 4152983, 266784, 266784, 1305059], [2087357, 3157558], [2087357, 1340922, 4954999], [3219016, 2028434, 3219016], [4954999, 818610, 271696]]\n"
     ]
    }
   ],
   "source": [
    "# Create nested list of sessions and items per user\n",
    "userBase = sample.groupby(['USERID', 'SESSION'])['PRODUCTID'].apply(list).groupby('USERID').apply(list)\n",
    "print(userBase[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example(userID=1, history=[2268318, 2333346, 4365585, 230380, 2951368, 3108797, 2734026, 4152983, 266784, 266784, 1305059, 2087357, 3157558, 2087357, 1340922, 4954999], inputs=[3219016, 2028434], target=[2028434, 3219016])\n",
      "\n",
      "Example(userID=1, history=[2268318, 2333346, 4365585, 230380, 2951368, 3108797, 2734026, 4152983, 266784, 266784, 1305059, 2087357, 3157558, 2087357, 1340922, 4954999, 3219016, 2028434, 3219016], inputs=[4954999, 818610], target=[818610, 271696])\n"
     ]
    }
   ],
   "source": [
    "# More efficient create examples function\n",
    "# A simple way to define a class is using namedtuple.\n",
    "Example = namedtuple(\"Example\", [\"userID\", \"history\", \"inputs\", \"target\"])\n",
    "\n",
    "def f(userid, sessions, train):\n",
    "    #print(sessions)\n",
    "    non_empty_sessions = [ses for ses in sessions if len(ses) >= 2]\n",
    "    if len(non_empty_sessions) < 3:\n",
    "        print('Not enough sessions')\n",
    "        return\n",
    "    if train:\n",
    "        object_train = Example(userID = userid, history = \n",
    "                               [item for sublist in non_empty_sessions[:-2] for item in sublist], \n",
    "                               inputs = non_empty_sessions[-2][:-1], target = non_empty_sessions[-2][1:])\n",
    "        return object_train\n",
    "    else:\n",
    "        return Example(userID = userid, history = \n",
    "                       [item for sublist in non_empty_sessions[:-1] for item in sublist], \n",
    "                       inputs = non_empty_sessions[-1][:-1], \n",
    "                       target = non_empty_sessions[-1][1:])\n",
    "\n",
    "def createExamples(userBase):\n",
    "    ''' Create training and testing set '''\n",
    "    userBase = pandas.DataFrame(userBase)\n",
    "    userBase.reset_index(level = 0, inplace = True)\n",
    "    trainData = [x for x in \n",
    "                 userBase.apply(lambda x: f(x['USERID'], x['PRODUCTID'], True), axis = 1).tolist() \n",
    "                 if x is not None]\n",
    "    testData = [x for x in \n",
    "                userBase.apply(lambda x: f(x['USERID'], x['PRODUCTID'], False), axis = 1).tolist() \n",
    "                if x is not None]\n",
    "    return trainData, testData\n",
    "\n",
    "trainData, testData = createExamples(userBase)\n",
    "print(trainData[0])\n",
    "print('')\n",
    "print(testData[0])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HELPER FUNCTIONS\n",
    "\n",
    "# function to yield one example at a time\n",
    "def get_examples(data, shuffle=True, **kwargs):\n",
    "    \"\"\"Shuffle data set and return 1 example at a time (until nothing left)\"\"\"\n",
    "    if shuffle:\n",
    "#         print(\"Shuffling training data\")\n",
    "        random.shuffle(data)  # shuffle training data each epoch\n",
    "    for example in data:\n",
    "        yield example\n",
    "    \n",
    "# function to prepare an example for usage by the model\n",
    "def prepare_example(example, vocab):\n",
    "    \"\"\"\n",
    "    Turn an example into tensors of inputs and target.\n",
    "    \"\"\"\n",
    "    u = vocab.u2i.get(example.userID,0)\n",
    "    v = torch.LongTensor([u])\n",
    "    v = v.to(device)\n",
    "    \n",
    "    w = torch.LongTensor([vocab.p2i.get(t, 0) for t in example.history])\n",
    "    w = w.to(device)\n",
    "    \n",
    "    x = torch.LongTensor([vocab.p2i.get(t, 0) for t in example.inputs])[:,None]\n",
    "    x = x.to(device)\n",
    "\n",
    "    y = torch.LongTensor([vocab.p2i.get(t, 0) for t in example.target])[:,None]\n",
    "    y = y.to(device)\n",
    "\n",
    "    return (v, w, x), y\n",
    "\n",
    "# simple evaluation function\n",
    "def simple_evaluate(model, data, prep_fn=prepare_example, **kwargs):\n",
    "    \"\"\"Precision of a model on given data set.\"\"\"\n",
    "    model.eval()  # disable dropout\n",
    "    targets = []\n",
    "    predictions = []\n",
    "    \n",
    "    vocab = model.vocab\n",
    "    for example in data:\n",
    "        # convert the example input and targets to PyTorch tensors\n",
    "        targets.extend([vocab.p2i.get(t,0) for t in example.target])\n",
    "        x, target = prepare_example(example, vocab)\n",
    "\n",
    "        # forward pass\n",
    "        # get the output from the neural network for input x\n",
    "        with torch.no_grad():\n",
    "            output, alphas = model(x)\n",
    "        # output shape: (sequence length, score for each item)\n",
    "        prediction = torch.argmax(output, dim=1).tolist()\n",
    "        predictions.extend(prediction)\n",
    "            \n",
    "    precision = sum([1 if p==t else 0 for p,t in zip(predictions,targets)])/len(targets)\n",
    "\n",
    "    return precision, None\n",
    "\n",
    "def recall(model, data, prep_fn=prepare_example, at=5, **kwargs):\n",
    "    model.eval() # disable dropout\n",
    "    targets = []\n",
    "    predictions = []\n",
    "    \n",
    "    vocab = model.vocab\n",
    "    for example in data:\n",
    "        # convert the example input and targets to PyTorch tensors\n",
    "        targets.extend([vocab.p2i.get(t,0) for t in example.target])\n",
    "        x, target = prepare_example(example, vocab)\n",
    "\n",
    "        # forward pass\n",
    "        # get the output from the neural network for input x\n",
    "        with torch.no_grad():\n",
    "            output, alphas = model(x)\n",
    "        # output shape: (sequence length, score for each item)\n",
    "        prediction = torch.argsort(output, dim=1, descending=True)[:,:at].tolist()\n",
    "        predictions.extend(prediction)\n",
    "        \n",
    "    recall = sum([1 if t in p else 0 for t,p in zip(targets,predictions)])/len(targets)\n",
    "    \n",
    "    return recall, None\n",
    "\n",
    "def mrr(model, data, prep_fn=prepare_example, at=5, **kwargs):\n",
    "    model.eval() # disable dropout\n",
    "    targets = []\n",
    "    predictions = []\n",
    "    \n",
    "    vocab = model.vocab\n",
    "    for example in data:\n",
    "        # convert the example input and targets to PyTorch tensors\n",
    "        targets.extend([vocab.p2i.get(t,0) for t in example.target])\n",
    "        x, target = prepare_example(example, vocab)\n",
    "\n",
    "        # forward pass\n",
    "        # get the output from the neural network for input x\n",
    "        with torch.no_grad():\n",
    "            output, alphas = model(x)\n",
    "        # output shape: (sequence length, nr of products)\n",
    "        prediction = torch.argsort(output, dim=1, descending=True)[:,:at].tolist()\n",
    "        predictions.extend(prediction)\n",
    "        \n",
    "    mrr = sum([1/(p.index(t) + 1) if (t != 0 and t in p) else 0 for t,p in zip(targets,predictions)])/len(targets)\n",
    "    \n",
    "    return mrr, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom NN\n",
    "\n",
    "class NarmPlus(nn.Module):\n",
    "    def __init__(self, \n",
    "                 item_embedding_dim, user_embedding_dim, hidden_size, output_dim, num_layers, \n",
    "                 vocab, \n",
    "                 activation_fn=nn.RReLU(), dropout=0.2):\n",
    "        super(NarmPlus, self).__init__()\n",
    "        # Store parameters\n",
    "        self.item_embedding_dim = item_embedding_dim\n",
    "        self.user_embedding_dim = user_embedding_dim\n",
    "        self.hidden_size = hidden_size # hidden size is also user embedding dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_layers = num_layers\n",
    "        # Shape of hidden_state: (num_layers * num_directions, batch, hidden_size)\n",
    "        self.hidden_state_dim = (num_layers, 1, hidden_size)\n",
    "        self.hidden_state_size = num_layers * hidden_size\n",
    "        self.vocab = vocab\n",
    "        num_users = len(vocab.u2i)\n",
    "        num_items = len(vocab.p2i)\n",
    "        \n",
    "        # General part\n",
    "        self.ActivationFn = activation_fn\n",
    "        self.Softmax = nn.Softmax(dim=1)\n",
    "        self.loss = self.top1loss\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # History part\n",
    "        self.UserEmbedding = nn.Embedding(num_users, user_embedding_dim)\n",
    "        self.ItemEmbedding = nn.Embedding(num_items, item_embedding_dim)\n",
    "        self.LatentItemHistory = nn.Linear(item_embedding_dim, user_embedding_dim)\n",
    "        self.ProfileToHidden = nn.Linear(user_embedding_dim, self.hidden_state_size)\n",
    "        \n",
    "        # NARM Part\n",
    "        # Input to the GRU is the item embedding: input_size = embedding_size\n",
    "        # Hidden size is something we can experiment with\n",
    "        self.Local = nn.GRU(item_embedding_dim, hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.Global = nn.GRU(item_embedding_dim, hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.Decoder = nn.Bilinear(item_embedding_dim, 2*hidden_size, output_dim)\n",
    "        \n",
    "        # Inner working of NARM attention part\n",
    "        # Latent space for alpha: what value to pick?\n",
    "        # I assume no bias, based on the paper\n",
    "        latent_space = hidden_size\n",
    "        self.A1 = nn.Linear(hidden_size,latent_space,bias=False)\n",
    "        self.A2 = nn.Linear(hidden_size,latent_space,bias=False)\n",
    "        self.v = nn.Linear(latent_space,1,bias=False)\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        user, history, inputs = x\n",
    "        # user shape (1)\n",
    "        # history shape (history_length)\n",
    "        # inputs shape (seq_length,1)\n",
    "        item_embeds_h = self.dropout(self.ItemEmbedding(history))\n",
    "#         print(f'Item embedding shape (history): {item_embeds_h.shape}')\n",
    "        # item_embeds_h shape (history_length, embedding_size)\n",
    "        user_embed = self.dropout(self.UserEmbedding(user))\n",
    "#         print(f'User embedding shape: {user_embed.shape}')\n",
    "        # user_embed shape (1, embedding_size) <- shouldn't this be (1, hidden_state_size)?\n",
    "        dense = self.LatentItemHistory(item_embeds_h)\n",
    "        dense = self.ActivationFn(dense)\n",
    "#         print(f'Dense shape: {dense.shape}')\n",
    "        # dense shape (history_length, hidden_state_size)\n",
    "        alpha1 = self.Softmax(torch.matmul(user_embed,torch.transpose(dense, 0, 1)))\n",
    "#         print(f'Alpha1 shape: {alpha1.shape}')\n",
    "        # alpha shape (history_length)\n",
    "        profile = torch.sum(torch.mul(alpha1,torch.transpose(dense,0,1)),1)\n",
    "#         print(f'Profile shape: {profile.shape}')\n",
    "        # profile shape (hidden_state_size)\n",
    "        # reshape to correct hidden state dimensions\n",
    "        h_0 = torch.reshape(self.ProfileToHidden(profile),self.hidden_state_dim)\n",
    "#         print(f'h_0 shape: {h_0.shape}')\n",
    "        \n",
    "        # Embed the items of the current session\n",
    "        item_embeds_c = self.dropout(torch.transpose(self.ItemEmbedding(inputs),0,1))\n",
    "#         print(f'Item embedding shape (current): {item_embeds_c.shape}')\n",
    "        # shape (seq_length, embedding_size)\n",
    "        # add a batch dimension to the front, necessary for GRU\n",
    "#         item_embeds_c = item_embeds_c[None,:,:] \n",
    "#         print(f'Item embedding shape (current): {item_embeds_c.shape}')\n",
    "        \n",
    "        out_local, _ = self.Local(item_embeds_c,h_0)\n",
    "#         print(f'Out_local shape: {out_local.shape}')\n",
    "        out_global, _ = self.Global(item_embeds_c,h_0)\n",
    "#         print(f'Out_global shape: {out_global.shape}')\n",
    "        # out shape (batch_size, seq_length, hidden_size), containing hidden state output for every step\n",
    "        # Shape of c_global and c_local should be: (sequence, hidden_size)\n",
    "        c_global = out_global[0,:,:]\n",
    "#         print(f'c_global shape: {c_global.shape}')\n",
    "        # c_global shape (seq_length, hidden_size)\n",
    "        c_local, alphas = self.calculate_c_local(out_local)\n",
    "#         print(f'c_local shape: {c_local.shape}')\n",
    "        \n",
    "        # Shape of c_global and c_local should be: (sequence, hidden_size)\n",
    "        c = self.dropout(torch.cat((c_global, c_local), dim=1))\n",
    "        # Decoder takes as inputs: embeddings for each item and c\n",
    "        embeds = self.ItemEmbedding(torch.LongTensor([i for i in range(len(self.vocab.p2i))]).to(device))\n",
    "        # Make embeds and c the same shape\n",
    "        embeds = embeds.repeat(c.shape[0],1,1).contiguous()\n",
    "        c = c[:,None,:].repeat(1,embeds.shape[1],1).contiguous()\n",
    "#         print(f'c shape: {c.shape}')\n",
    "#         print(f'All item embeds shape: {embeds.shape}')\n",
    "        out = self.Decoder(embeds, c)[:,:,0]\n",
    "#         print(f'Output shape: {out.shape}')\n",
    "        output = self.Softmax(out)\n",
    "        return output, alphas\n",
    "    \n",
    "    def calculate_c_local(self,H):\n",
    "        # H: hidden states returned from the GRU\n",
    "        # H shape (batch, sequence, hidden size)\n",
    "        H = H[0,:,:]\n",
    "        # H shape (sequence, hidden size)\n",
    "        # Initialise c_local with the output hidden states: every hidden state has a similarity of \n",
    "        # 1 with itself, so the entire hidden state is taken into account\n",
    "        c_local_base = H.clone().detach().requires_grad_(True)\n",
    "        \n",
    "  \n",
    "        alphas = torch.ones((H.shape[0],H.shape[0]),dtype=torch.float32).to(device)\n",
    "        cs = [torch.zeros((1,H.shape[1]), dtype=torch.float32).to(device)]\n",
    "        for t in range(H.shape[0]):\n",
    "            # If it is the first hidden state, then there are no previous hidden states to calculate the \n",
    "            # alpha from and the current hidden state has already been saved in c_local.\n",
    "            if t == 0:\n",
    "                continue\n",
    "            \n",
    "            # Technically we do not need to store the alphas, but maybe we want to do something with these values\n",
    "            alphas_t = torch.zeros(H.shape[0], dtype=torch.float32).to(device)\n",
    "            A1 = self.A1(H[t])\n",
    "            for j in range(t):\n",
    "                A2 = self.A2(H[j])\n",
    "                # next three lines could be done in one line\n",
    "                alphas_t[j] = self.v(self.ActivationFn(A1 + A2))\n",
    "#                 ct_j = torch.mul(alphas_t[j],H[j])\n",
    "#                 c_local[t] = torch.add(c_local[t],ct_j)\n",
    "            ct = torch.sum(alphas_t[:t].unsqueeze(1) * H[:t,:],dim=0).unsqueeze(0)\n",
    "            cs.append(ct)\n",
    "            alphas[t] = alphas_t\n",
    "        c_local = torch.cat(cs,dim=0) + c_local_base\n",
    "        return c_local, alphas\n",
    "    \n",
    "    def top1(self, yhat):\n",
    "        ''' Top1 loss, yhat is vector with softmax probabilities '''\n",
    "        # Not sure if you can just call backward to this, but I think it should work. Code from:\n",
    "        # https://github.com/mquad/hgru4rec/blob/master/src/hgru4rec.py\n",
    "        yhatT = torch.transpose(yhat, 0, 1)\n",
    "        loss = torch.mean(torch.mean(nn.sigmoid( - torch.diag(yhat) + yhatT) + nn.sigmoid(yhat ** 2), dim = 0) \n",
    "                          - nn.sigmoid(T.diag(yhat ** 2)))\n",
    "        return loss\n",
    "    \n",
    "    def top1loss(self, output, targets):\n",
    "        scores_for_targets = torch.gather(output, 1, targets)\n",
    "        loss = torch.mean(torch.sigmoid(output - scores_for_targets) +\n",
    "                torch.sigmoid(output**2))\n",
    "        return loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to train a model\n",
    "name_extension = ''\n",
    "def train_model(model, optimizer, num_epochs=200, \n",
    "                print_every=1, eval_every=1,\n",
    "                batch_fn=get_examples, \n",
    "                prep_fn=prepare_example,\n",
    "                eval_fn=simple_evaluate,\n",
    "                batch_size=5, eval_batch_size=None,\n",
    "                predict=False\n",
    "               ):\n",
    "    \"\"\"Train a model.\"\"\"  \n",
    "    train_loss = 0.\n",
    "    start = time.time()\n",
    "    best_eval = 0.\n",
    "    best_iter = 0\n",
    "    criterion=model.loss\n",
    "\n",
    "    # store train loss and validation accuracy during training\n",
    "    # so we can plot them afterwards\n",
    "    losses = []\n",
    "    accuracies = []  \n",
    "\n",
    "    if eval_batch_size is None:\n",
    "        eval_batch_size = batch_size\n",
    "    \n",
    "    vocab = model.vocab\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        batch_iter = 0\n",
    "        batch_out = []\n",
    "        batch_targets = []\n",
    "        for example in batch_fn(train_data): # goes through the entire training data once, a.k.a. an epoch\n",
    "\n",
    "            # forward pass, make sure the model is in train modus\n",
    "            model.train()\n",
    "            x, targets = prep_fn(example, vocab)\n",
    "            output, alphas = model(x)\n",
    "            # output shape (sequence length, nr of products): a score for each product at each time step\n",
    "            # alphas are the alphas used in the Narm part\n",
    "\n",
    "            if len(batch_out) == 0:\n",
    "                batch_out = output\n",
    "                batch_targets = targets\n",
    "            else:\n",
    "                batch_out = torch.cat((batch_out, output), dim=0)\n",
    "                batch_targets = torch.cat((batch_targets, targets), dim=0)\n",
    "                \n",
    "            batch_iter += 1\n",
    "                \n",
    "            if batch_iter == batch_size:\n",
    "                # enough users put through the network, time to calculate the loss and update the network\n",
    "                \n",
    "                # calculate loss\n",
    "                # important: len(batch_out) is not always the same\n",
    "                # it does always contain data on the same number of users\n",
    "                # batch_size, in other words, is how many users to look at before updating\n",
    "                loss = criterion(batch_out, batch_targets)\n",
    "                train_loss += loss.item()\n",
    "                \n",
    "                # backward pass\n",
    "                # erase previous gradients\n",
    "                model.zero_grad()\n",
    "\n",
    "                # compute gradients\n",
    "                loss.backward()\n",
    "\n",
    "                # update weights - take a small step in the opposite dir of the gradient\n",
    "                optimizer.step()\n",
    "                \n",
    "                # reset the iterator, batch_out and batch_targets\n",
    "                batch_iter = 0\n",
    "                batch_out = []\n",
    "                batch_targets = []\n",
    "                \n",
    "        if (epoch + 1) % print_every == 0:\n",
    "            print(\"Epoch %r: loss=%.4f, time=%.2fs\" % \n",
    "                 (epoch + 1, train_loss, time.time()-start))\n",
    "            losses.append(train_loss)       \n",
    "            train_loss = 0.\n",
    "            \n",
    "        if (epoch + 1) % eval_every == 0:\n",
    "            accuracy, _ = eval_fn(model, dev_data, batch_size=eval_batch_size,\n",
    "                                         batch_fn=batch_fn, prep_fn=prep_fn)\n",
    "            accuracies.append(accuracy)\n",
    "            print(\"epoch %r: dev acc=%.4f\" % (epoch + 1, accuracy))       \n",
    "\n",
    "            # save best model parameters\n",
    "            if accuracy > best_eval:\n",
    "                print(\"new highscore\")\n",
    "                best_eval = accuracy\n",
    "                best_iter = epoch + 1\n",
    "                path = \"{}{}.pt\".format(model.__class__.__name__,name_extension)\n",
    "                ckpt = {\n",
    "                  \"state_dict\": model.state_dict(),\n",
    "                  \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                  \"best_eval\": best_eval,\n",
    "                  \"best_iter\": best_iter\n",
    "                }\n",
    "                torch.save(ckpt, path)\n",
    "    \n",
    "    # Done training\n",
    "    # evaluate on train, dev, and test with best model\n",
    "    print(\"Loading best model\")\n",
    "    path = \"{}{}.pt\".format(model.__class__.__name__,name_extension)        \n",
    "    ckpt = torch.load(path)\n",
    "    model.load_state_dict(ckpt[\"state_dict\"])\n",
    "\n",
    "    train_acc, _ = eval_fn(\n",
    "        model, train_data, batch_size=eval_batch_size, \n",
    "        batch_fn=batch_fn, prep_fn=prep_fn)\n",
    "    dev_acc, _ = eval_fn(\n",
    "        model, dev_data, batch_size=eval_batch_size,\n",
    "        batch_fn=batch_fn, prep_fn=prep_fn)\n",
    "    test_acc, predictions = eval_fn(\n",
    "        model, test_data, batch_size=eval_batch_size, \n",
    "        batch_fn=batch_fn, prep_fn=prep_fn)\n",
    "\n",
    "    print(\"best model iter {:d}: \"\n",
    "          \"train acc={:.4f}, dev acc={:.4f}, test acc={:.4f}\".format(\n",
    "              best_iter, train_acc, dev_acc, test_acc))\n",
    "\n",
    "    return test_acc, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createSplits(data, k):\n",
    "    folds = {}\n",
    "    for i in range(k):\n",
    "        dev = data[math.ceil(i*len(data)/k) : math.ceil((i+1)*len(data)/k)]\n",
    "        train = [x for x in data if x not in dev]\n",
    "        folds[i] = train,dev\n",
    "    return folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if device == 'cuda':\n",
    "    torch.cuda.empty_cache()\n",
    "device = 'cpu'\n",
    "\n",
    "# item_embedding_dim, hidden_size, output_dim, num_layers, vocab, \n",
    "train_data = trainData\n",
    "dev_data = test_data = testData\n",
    "\n",
    "num_users = len(userList)\n",
    "num_products = len(productList)\n",
    "\n",
    "model = NarmPlus(math.ceil(num_products**0.25),math.ceil(num_users**0.25),10,1,1,v,dropout=0.2)\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "a, p = train_model(model, optimizer, eval_fn=recall, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2],\n",
      "        [ 3,  4,  5],\n",
      "        [ 6,  7,  8],\n",
      "        [ 9, 10, 11]])\n",
      "tensor([[1],\n",
      "        [3],\n",
      "        [8],\n",
      "        [9]])\n",
      "tensor([[-1,  0,  1],\n",
      "        [ 0,  1,  2],\n",
      "        [-2, -1,  0],\n",
      "        [ 0,  1,  2]])\n",
      "torch.Size([4, 3]) torch.Size([4, 1])\n",
      "[[0, 1, 2], [3, 4, 5], [6, 7, 8], [9, 10, 11]]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'ceil' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-073c5f7ef53e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'ceil' is not defined"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([i for i in range(12)]).reshape((4,3))\n",
    "print(a)\n",
    "b = torch.tensor([[1],[0],[2],[0]])\n",
    "b = torch.t(torch.tensor([[1,0,2,0]]))\n",
    "print(torch.gather(a, 1, b))\n",
    "print(a - torch.gather(a, 1, b))\n",
    "print(a.shape, b.shape)\n",
    "print(a.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top1loss(output, targets):\n",
    "    scores_for_targets = torch.gather(output, 1, targets)\n",
    "    loss = nn.Mean(nn.Sigmoid(output - scores_for_targets) +\n",
    "            nn.Sigmoid(output**2))\n",
    "    return loss\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
