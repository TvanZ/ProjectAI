{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NARM+\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing functions.\n",
    "import pandas\n",
    "import csv\n",
    "from collections import Counter, OrderedDict, defaultdict, namedtuple\n",
    "import gc\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import itertools\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# PyTorch can run on CPU or on Nvidia GPU (video card) using CUDA\n",
    "# This cell selects the GPU if one is available.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.synchronize() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.6.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gensim.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11832\n",
      "67172\n"
     ]
    }
   ],
   "source": [
    "# Create sessions\n",
    "sample = pandas.read_pickle(\"./data/processedData.pkl\")\n",
    "sample['SESSION'] = pandas.to_datetime(sample['TIMESTAMP'],unit='s').dt.date\n",
    "\n",
    "print(len(sample[\"USERID\"].unique()))\n",
    "print(len(sample[\"PRODUCTID\"].unique()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of sessions per user\n",
      "5.765128465179175\n",
      "Average number of clicks per session\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11.017386715142274"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Average number of sessions per user\")\n",
    "print(sample.groupby('USERID')['SESSION'].nunique().mean())\n",
    "\n",
    "print(\"Average number of clicks per session\")\n",
    "sample.groupby(['USERID', 'SESSION'])['ACTION'].count().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATASET CREATION\n",
    "\n",
    "# read CSV file\n",
    "# sample = pandas.read_csv(\"./data/UserBehavior.csv\", names=[\"USERID\", \"PRODUCTID\", \"CATEGORYID\", \"ACTION\", \"TIMESTAMP\"])\n",
    "\n",
    "# edit size and print size of userID\n",
    "# sample.set_index(\"USERID\")\n",
    "# sampleNew = sample[sample[\"USERID\"] < 10319]\n",
    "# print(len(sampleNew[\"USERID\"].unique()))\n",
    "# print(len(sampleNew[\"PRODUCTID\"].unique()))\n",
    "\n",
    "#sampleNew.to_pickle('./data/datasetName.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### DO NOT RUN THIS CELL IF YOU READ THE PICKLE processedData\n",
    "\n",
    "# ##Preprocessing the Data\n",
    "# #Remove products that occur less than 5 times\n",
    "# productsize = sample.groupby([\"PRODUCTID\"]).size() > 4\n",
    "# indexProducts = productsize.index[productsize]\n",
    "# productTotal = sample[sample[\"PRODUCTID\"].isin(indexProducts)]\n",
    "\n",
    "# #Remove sessions that have a length of 1 or less\n",
    "# #Remove users that have 2 or less sessions\n",
    "# sessionList = productTotal.groupby([\"USERID\",\"SESSION\"]).size() > 1\n",
    "# sessionBase = sessionList.to_frame(name = \"realSESSION\").reset_index()\n",
    "# enoughSessions = sessionBase.groupby([\"USERID\", \"realSESSION\"]).size() > 2\n",
    "# trueSessionBase = enoughSessions.to_frame(name = \"enoughSESSIONS\").reset_index()\n",
    "\n",
    "\n",
    "# #Merge DataSets such that original Dataset is restored with newly added filters\n",
    "# totalDataSet = productTotal.merge(trueSessionBase)\n",
    "# totalDataSet = totalDataSet[totalDataSet[\"enoughSESSIONS\"] == True]\n",
    "\n",
    "# #Drop useless columns\n",
    "# totalDataSet = totalDataSet.drop(\"enoughSESSIONS\", axis=1)\n",
    "# totalDataSet = totalDataSet.drop(\"realSESSION\", axis=1)\n",
    "# sample = totalDataSet\n",
    "\n",
    "# #Piece of mind stuff\n",
    "# userList = totalDataSet[\"USERID\"].unique()\n",
    "# productList = totalDataSet[\"PRODUCTID\"].unique()\n",
    "# print(len(totalDataSet2))\n",
    "# print(len(userList))\n",
    "# print(len(productList))\n",
    "\n",
    "\n",
    "# sample.to_pickle(\"./data/processedData.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "userList = sample[\"USERID\"].unique()\n",
    "productList = sample[\"PRODUCTID\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we first define a class that can map a product to an ID (p2i)\n",
    "# and back (i2p).\n",
    "\n",
    "class OrderedCounter(Counter, OrderedDict):\n",
    "    \"\"\"Counter that remembers the order elements are first seen\"\"\"\n",
    "    def __repr__(self):\n",
    "        return '%s(%r)' % (self.__class__.__name__,\n",
    "                      OrderedDict(self))\n",
    "    def __reduce__(self):\n",
    "        return self.__class__, (OrderedDict(self),)\n",
    "\n",
    "\n",
    "class Vocabulary:\n",
    "    \"\"\"A vocabulary, assigns IDs to tokens\"\"\"\n",
    "    def __init__(self):\n",
    "        self.freqs = OrderedCounter()\n",
    "        self.users = []\n",
    "        self.u2i = {}\n",
    "        self.i2u = []\n",
    "        self.p2i = {}\n",
    "        self.i2p = []\n",
    "        self.p2e = {}\n",
    "        self.u2e = {}\n",
    "\n",
    "    def count_product(self, t):\n",
    "        self.freqs[t] += 1\n",
    "    \n",
    "    def count_user(self, t):\n",
    "        self.users.append(t)\n",
    "\n",
    "    def add_product(self, t):\n",
    "        self.p2i[t] = str(len(self.p2i))\n",
    "        self.i2p.append(t) \n",
    "        \n",
    "    def add_user(self, t):\n",
    "        self.u2i[t] = str(len(self.u2i))\n",
    "        self.i2u.append(t)\n",
    "\n",
    "    def build(self, min_freq=0):\n",
    "#         self.add_product(\"<unk>\")  # reserve 0 for <unk> (unknown products (products only occuring in test set))\n",
    "#         self.add_user(\"<unk>\")\n",
    "        tok_freq = list(self.freqs.items())\n",
    "        tok_freq.sort(key=lambda x: x[1], reverse=True)\n",
    "        for tok, freq in tok_freq:\n",
    "            if freq >= min_freq:\n",
    "                self.add_product(tok)\n",
    "        for user in self.users:\n",
    "            self.add_user(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 67172\n"
     ]
    }
   ],
   "source": [
    "# This process should be deterministic and should have the same result \n",
    "# if run multiple times on the same data set.\n",
    "\n",
    "def build_voc(userList, productList):\n",
    "    v = Vocabulary()\n",
    "    for product in productList:\n",
    "        v.count_product(product)\n",
    "    for user in userList:\n",
    "        v.count_user(user)\n",
    "    v.build()\n",
    "    return v\n",
    "\n",
    "v = build_voc(userList, productList)\n",
    "print(\"Vocabulary size:\", len(v.p2i))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2268318, 2333346], [4365585, 230380], [2951368, 3108797], [2734026, 4152983, 266784, 266784, 1305059], [2087357, 3157558], [2087357, 1340922, 4954999], [3219016, 2028434, 3219016], [4954999, 818610, 271696]]\n"
     ]
    }
   ],
   "source": [
    "# Create nested list of sessions and items per user\n",
    "userBase = sample.groupby(['USERID', 'SESSION'])['PRODUCTID'].apply(list).groupby('USERID').apply(list)\n",
    "print(userBase[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['25413', '55820'], ['19169', '11952', '15103', '33969', '8333', '35062', '16380', '18524', '33446', '50208', '13150', '33449', '33446', '33164', '33164'], ['22040', '33164', '22040'], ['36412', '44564', '24160'], ['44564', '8622', '52184'], ['46819', '27654', '46819', '27654']]\n",
      "[['25413', '55820'], ['19169', '11952', '15103', '33969', '8333', '35062', '16380', '18524', '33446', '50208', '13150', '33449', '33446', '33164', '33164'], ['22040', '33164', '22040'], ['36412', '44564', '24160'], ['44564', '8622', '52184'], ['46819', '27654', '46819', '27654']]\n",
      "Example(userID='1', history=['0', '1', '2', '3', '4', '5', '6', '7', '8', '8', '9', '10', '11', '10', '12', '13'], inputs=['14', '15'], target=['15', '14'])\n",
      "\n",
      "Example(userID='1', history=['0', '1', '2', '3', '4', '5', '6', '7', '8', '8', '9', '10', '11', '10', '12', '13', '14', '15', '14'], inputs=['13', '16'], target=['16', '17'])\n"
     ]
    }
   ],
   "source": [
    "# More efficient create examples function\n",
    "# A simple way to define a class is using namedtuple.\n",
    "Example = namedtuple(\"Example\", [\"userID\", \"history\", \"inputs\", \"target\"])\n",
    "\n",
    "allSessions = []\n",
    "allUsers = []\n",
    "\n",
    "def f(userid, sessions, train):\n",
    "    #print(sessions)\n",
    "    sessions = [[v.p2i.get(t,0) for t in ses] for ses in sessions if len(ses) > 1]\n",
    "    if userid == 11905:\n",
    "        print(sessions)\n",
    "    if train:\n",
    "        object_train = Example(userID = str(userid), history = \n",
    "                               [item for sublist in sessions[:-2] for item in sublist], \n",
    "                               inputs = sessions[-2][:-1], target = sessions[-2][1:])\n",
    "        return object_train\n",
    "    else:\n",
    "        # store info for the pretrained embeddings\n",
    "        allSessions.extend(sessions)\n",
    "        userDoc = [t for ses in sessions for t in ses]\n",
    "        allUsers.append(TaggedDocument(userDoc, [str(userid)]))\n",
    "        return Example(userID = str(userid), history = \n",
    "                       [item for sublist in sessions[:-1] for item in sublist], \n",
    "                       inputs = sessions[-1][:-1], \n",
    "                       target = sessions[-1][1:])\n",
    "\n",
    "def createExamples(userBase):\n",
    "    ''' Create training and testing set '''\n",
    "    userBase = pandas.DataFrame(userBase)\n",
    "    userBase.reset_index(level = 0, inplace = True)\n",
    "    trainData = [x for x in \n",
    "                 userBase.apply(lambda x: f(x['USERID'], x['PRODUCTID'], True), axis = 1).tolist() \n",
    "                 if x is not None]\n",
    "    testData = [x for x in \n",
    "                userBase.apply(lambda x: f(x['USERID'], x['PRODUCTID'], False), axis = 1).tolist() \n",
    "                if x is not None]\n",
    "    return trainData, testData\n",
    "\n",
    "trainData, testData = createExamples(userBase)\n",
    "print(trainData[0])\n",
    "print('')\n",
    "print(testData[0])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# product_embeddings = Word2Vec(allSessions, size=16, window=5, min_count=1)\n",
    "# print(product_embeddings.wv['1'])\n",
    "# user_embeddings = Doc2Vec(allUsers, vector_size=8, window=5, min_count=1)\n",
    "# print(user_embeddings.wv['1'])\n",
    "# v.u2e = user_embeddings.wv\n",
    "# v.p2e = product_embeddings.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HELPER FUNCTIONS\n",
    "\n",
    "# function to yield one example at a time\n",
    "def get_examples(data, shuffle=True, **kwargs):\n",
    "    \"\"\"Shuffle data set and return 1 example at a time (until nothing left)\"\"\"\n",
    "    if shuffle:\n",
    "#         print(\"Shuffling training data\")\n",
    "        random.shuffle(data)  # shuffle training data each epoch\n",
    "    for example in data:\n",
    "        yield example\n",
    "        \n",
    "def get_minibatch(data, batch_size=25, shuffle=True):\n",
    "    \"\"\"Return minibatches, optional shuffling\"\"\"\n",
    "\n",
    "    if shuffle:\n",
    "#         print(\"Shuffling training data\")\n",
    "        random.shuffle(data)  # shuffle training data each epoch\n",
    "\n",
    "    batch = []\n",
    "\n",
    "    # yield minibatches\n",
    "    for example in data:\n",
    "        batch.append(example)\n",
    "\n",
    "        if len(batch) == batch_size:\n",
    "            yield batch\n",
    "            batch = []\n",
    "\n",
    "        # in case there is something left\n",
    "    if len(batch) > 0:\n",
    "        yield batch\n",
    "    \n",
    "# function to prepare an example for usage by the model\n",
    "def prepare_example(example, vocab):\n",
    "    \"\"\"\n",
    "    Turn an example into tensors of inputs and target.\n",
    "    \"\"\"\n",
    "    u = vocab.u2i.get(example.userID,0)\n",
    "    v = torch.LongTensor([u])\n",
    "    v = v.to(device)\n",
    "    \n",
    "    w = torch.LongTensor([int(t) for t in example.history])\n",
    "    w = w.to(device)\n",
    "    \n",
    "    x = torch.LongTensor([int(t) for t in example.inputs])[:,None]\n",
    "    x = x.to(device)\n",
    "\n",
    "    y = torch.LongTensor([int(t) for t in example.target])[:,None]\n",
    "    y = y.to(device)\n",
    "\n",
    "    return (v, w, (x, 0)), y\n",
    "\n",
    "def prepare_example_pre_trained(example, vocab):\n",
    "    \"\"\"\n",
    "    Turn an example into tensors of inputs and target.\n",
    "    \"\"\"\n",
    "    u = vocab.u2e[example.userID]\n",
    "    v = torch.FloatTensor([u])\n",
    "    v = v.to(device)\n",
    "    \n",
    "    w = torch.FloatTensor([vocab.p2e[t] for t in example.history])\n",
    "    w = w.to(device)\n",
    "    \n",
    "    x = torch.FloatTensor([[vocab.p2e[t] for t in example.inputs]])\n",
    "    x = x.to(device)\n",
    "\n",
    "    y = torch.LongTensor([int(t)-1 for t in example.target])[:,None]\n",
    "    y = y.to(device)\n",
    "\n",
    "    return (v, w, (x, 0)), y\n",
    "\n",
    "def pad(tokens, length, pad_value=-1):\n",
    "    \"\"\"add padding 0s to a sequence to that it has the desired length\"\"\"\n",
    "    return tokens + [pad_value] * (length - len(tokens))\n",
    "\n",
    "def prepare_minibatch(mb, vocab):\n",
    "    \"\"\"\n",
    "    Minibatch is a list of examples.\n",
    "    This function converts products to IDs and returns\n",
    "    torch tensors to be used as input/targets.\n",
    "    \"\"\"\n",
    "    batch_size = len(mb)\n",
    "    \n",
    "    u = [vocab.u2i.get(example.userID,0) for example in mb]\n",
    "    v = torch.LongTensor(u)\n",
    "    v = v.to(device)\n",
    "    # shape v (batch size, user id)\n",
    "    \n",
    "    # vocab returns 0 if the word is not there\n",
    "    maxlen = max([len(ex.history) for ex in mb])\n",
    "    w = [[pad([int(t) for t in ex.history], maxlen)] for ex in mb]\n",
    "    w = torch.LongTensor(w)\n",
    "    w = w.to(device)\n",
    "    # shape w (batch size, max history length)\n",
    "\n",
    "    # vocab returns 0 if the word is not there\n",
    "    maxlen = max([len(ex.inputs) for ex in mb])\n",
    "    x = [pad([int(t) for t in ex.inputs], maxlen) for ex in mb]\n",
    "    x = torch.LongTensor(x)\n",
    "    x = x.to(device)\n",
    "    xlengths = torch.LongTensor([len(ex.inputs) for ex in mb])\n",
    "    # shape x (batch size, max current session length)\n",
    "\n",
    "    y = [pad([int(t)-1 for t in ex.target], maxlen) for ex in mb]\n",
    "    y = torch.LongTensor(y)\n",
    "    y = y.to(device)\n",
    "    \n",
    "    gc.collect()\n",
    "\n",
    "    return (v,w,(x,xlengths)), y\n",
    "\n",
    "def prepare_minibatch_pre_trained(mb, vocab):\n",
    "    \"\"\"\n",
    "    Minibatch is a list of examples.\n",
    "    This function converts products to IDs and returns\n",
    "    torch tensors to be used as input/targets.\n",
    "    \"\"\"\n",
    "    batch_size = len(mb)\n",
    "    embedding_dim = len(vocab.p2e['1'])\n",
    "    \n",
    "    u = [[vocab.u2e[example.userID]] for example in mb]\n",
    "    v = torch.FloatTensor(u)\n",
    "    v = v.to(device)\n",
    "    # shape v (batch size, 1, embedding size)\n",
    "    \n",
    "    # vocab returns 0 if the word is not there\n",
    "    maxlen = max([len(ex.history) for ex in mb])\n",
    "    w = [pad(ex.history, maxlen, pad_value='0') for ex in mb]\n",
    "    w = [[vocab.p2e[t] for t in ex] for ex in w]\n",
    "    w = torch.FloatTensor(w)\n",
    "    w = w.to(device)\n",
    "    # shape w (batch size, max history length, embedding size)\n",
    "\n",
    "    # vocab returns 0 if the word is not there\n",
    "    maxlen = max([len(ex.inputs) for ex in mb])\n",
    "    x = [pad(ex.inputs, maxlen, pad_value='0') for ex in mb]\n",
    "    x = [[vocab.p2e[t] for t in ex] for ex in x]\n",
    "    x = torch.FloatTensor(x)\n",
    "    x = x.to(device)\n",
    "    xlengths = torch.LongTensor([len(ex.inputs) for ex in mb])\n",
    "    # shape x (batch size, max current session length, embedding size)\n",
    "\n",
    "    y = [pad([int(t)-1 for t in ex.target], maxlen) for ex in mb]\n",
    "    y = torch.LongTensor(y)\n",
    "    y = y.to(device)\n",
    "        \n",
    "    gc.collect()\n",
    "    \n",
    "    return (v,w,(x,xlengths)), y\n",
    "\n",
    "# simple evaluation function\n",
    "def simple_evaluate(model, data, prep_fn=prepare_example, **kwargs):\n",
    "    \"\"\"Precision of a model on given data set.\"\"\"\n",
    "    model.eval()  # disable dropout\n",
    "    targets = []\n",
    "    predictions = []\n",
    "    \n",
    "    vocab = model.vocab\n",
    "    for example in data:\n",
    "        # convert the example input and targets to PyTorch tensors\n",
    "        targets.extend([int(t) for t in example.target])\n",
    "        x, target = prep_fn(example, vocab)\n",
    "\n",
    "        # forward pass\n",
    "        # get the output from the neural network for input x\n",
    "        with torch.no_grad():\n",
    "            output, alphas = model(x)\n",
    "        # output shape: (sequence length, score for each item)\n",
    "        prediction = torch.argmax(output, dim=1).tolist()\n",
    "        predictions.extend(prediction)\n",
    "            \n",
    "    precision = sum([1 if p==t else 0 for p,t in zip(predictions,targets)])/len(targets)\n",
    "\n",
    "    return precision, None\n",
    "\n",
    "def recall(model, data, prep_fn=prepare_example, batch_fn=prepare_minibatch, at=5, batch_size=25, **kwargs):\n",
    "    model.eval() # disable dropout\n",
    "    targets = []\n",
    "    predictions = []\n",
    "    recall = 0\n",
    "    \n",
    "    vocab = model.vocab\n",
    "    for batch in batch_fn(train_data, batch_size=batch_size):\n",
    "        # convert the example input and targets to PyTorch tensors\n",
    "#         targets.extend([int(t) for t in example.target])\n",
    "        x, target = prep_fn(batch, vocab)\n",
    "        # forward pass\n",
    "        # get the output from the neural network for input x\n",
    "        with torch.no_grad():\n",
    "            output, alphas = model(x)\n",
    "        # output shape: (batch size, sequence length, score for each item)\n",
    "        prediction = torch.argsort(output, dim=2, descending=True)[:,:,:at].tolist()\n",
    "        print(prediction)\n",
    "        print(target.tolist())\n",
    "        predictions.extend(prediction)\n",
    "        \n",
    "#     print(predictions[:10],targets[:10])\n",
    "#     recall = sum([1 if t in p else 0 for t,p in zip(targets,predictions)])/len(targets)\n",
    "    \n",
    "    return recall, None\n",
    "\n",
    "def mrr(model, data, prep_fn=prepare_example, batch_fn=prepare_minibatch, at=5, **kwargs):\n",
    "    model.eval() # disable dropout\n",
    "    targets = []\n",
    "    predictions = []\n",
    "    \n",
    "    vocab = model.vocab\n",
    "    for example in data:\n",
    "        # convert the example input and targets to PyTorch tensors\n",
    "        targets.extend([int(t)-1 for t in example.target])\n",
    "        x, target = prep_fn(example, vocab)\n",
    "\n",
    "        # forward pass\n",
    "        # get the output from the neural network for input x\n",
    "        with torch.no_grad():\n",
    "            output, alphas = model(x)\n",
    "        # output shape: (sequence length, nr of products)\n",
    "        prediction = torch.argsort(output, dim=1, descending=True)[:,:at].tolist()\n",
    "        predictions.extend(prediction)\n",
    "        \n",
    "    mrr = sum([1/(p.index(t) + 1) if t in p else 0 for t,p in zip(targets,predictions)])/len(targets)\n",
    "    \n",
    "    return mrr, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom NN\n",
    "\n",
    "#Item embedding & User Embedding equal size\n",
    "#\n",
    "\n",
    "class NarmPlus(nn.Module):\n",
    "    def __init__(self, \n",
    "                 item_embedding_dim, user_embedding_dim, hidden_size, output_dim, num_layers, \n",
    "                 vocab, pre_trained=False, batch_size=10,\n",
    "                 activation_fn=nn.RReLU(), dropout=0.2):\n",
    "        super(NarmPlus, self).__init__()\n",
    "        # Store parameters\n",
    "        self.item_embedding_dim = item_embedding_dim\n",
    "        self.user_embedding_dim = user_embedding_dim\n",
    "        self.hidden_size = hidden_size # hidden size is also user embedding dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.pre_trained = pre_trained\n",
    "        self.batch_size = batch_size\n",
    "        # Shape of hidden_state: (num_layers * num_directions, batch, hidden_size)\n",
    "        self.hidden_state_dim = (num_layers, batch_size, hidden_size)\n",
    "        self.hidden_state_size = num_layers * hidden_size\n",
    "        self.vocab = vocab\n",
    "        num_users = len(vocab.u2i)\n",
    "        num_items = len(vocab.p2i)\n",
    "        \n",
    "        # General part\n",
    "        self.ActivationFn = activation_fn\n",
    "        self.Softmax = nn.Softmax(dim=1)\n",
    "        self.loss = self.top1loss\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # History part\n",
    "        self.UserEmbedding = nn.Embedding(num_users, user_embedding_dim)\n",
    "        self.ItemEmbedding = nn.Embedding(num_items, item_embedding_dim)\n",
    "        self.LatentItemHistory = nn.Linear(item_embedding_dim, user_embedding_dim)\n",
    "        self.ProfileToHidden = nn.Linear(user_embedding_dim, self.hidden_state_size)\n",
    "        \n",
    "        # NARM Part\n",
    "        # Input to the GRU is the item embedding: input_size = embedding_size\n",
    "        # Hidden size is something we can experiment with\n",
    "        self.Local = nn.GRU(item_embedding_dim, hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.Global = nn.GRU(item_embedding_dim, hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.Decoder = nn.Bilinear(item_embedding_dim, 2*hidden_size, output_dim)\n",
    "        \n",
    "        # Inner working of NARM attention part\n",
    "        # Latent space for alpha: what value to pick?\n",
    "        # I assume no bias, based on the paper\n",
    "        latent_space = hidden_size\n",
    "        self.A1 = nn.Linear(hidden_size,latent_space,bias=False)\n",
    "        self.A2 = nn.Linear(hidden_size,latent_space,bias=False)\n",
    "        self.v = nn.Linear(latent_space,1,bias=False)\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        user, history, (inputs, input_lengths) = x\n",
    "        \n",
    "        if self.pre_trained:\n",
    "            # user shape (1, embedding size)\n",
    "            # history shape (history length, embedding size)\n",
    "            # inputs shape (1, sequence length, embedding size)\n",
    "            print(f'Inputs shape: {inputs.shape}')\n",
    "            print(f'History shape: {history.shape}')\n",
    "            print(f'User shape: {user.shape}')\n",
    "            print(f'Input lengths shape: {input_lengths.shape}')\n",
    "            print(f'Input lengths: {input_lengths}')\n",
    "            \n",
    "#             dense = self.LatentItemHistory(history)\n",
    "#             dense = self.ActivationFn(dense)\n",
    "            dense = history\n",
    "#             print(f'Dense shape: {dense.shape}')\n",
    "            # dense shape (history_length, hidden_state_size)\n",
    "            if isinstance(input_lengths, int):\n",
    "                alpha1 = torch.softmax(torch.matmul(user,torch.transpose(dense, 0, 1)),1)\n",
    "                profile = torch.sum(torch.mul(alpha1,torch.transpose(dense,0,1)),1)\n",
    "                h_0 = self.dropout(torch.reshape(profile, self.hidden_state_dim))\n",
    "#             print(f'Alpha1 shape: {alpha1.shape}')\n",
    "            # alpha shape (history_length)\n",
    "            \n",
    "#             print(f'Profile shape: {profile.shape}')\n",
    "            # profile shape (hidden_state_size)\n",
    "            # reshape to correct hidden state dimensions\n",
    "#             h_0 = torch.reshape(self.ActivationFn(self.ProfileToHidden(profile)),self.hidden_state_dim)\n",
    "            \n",
    "#             print(f'h_0 shape: {h_0.shape}')\n",
    "            else:\n",
    "                alpha1 = self.Softmax(torch.matmul(dense,torch.transpose(user, 1, 2)))\n",
    "                print(f'Alpha shape: {alpha1.shape}')\n",
    "                profile = torch.sum(torch.mul(alpha1,dense),1)\n",
    "                print(f'Profile shape: {profile.shape}')\n",
    "                # profile shape (batch size, embedding size)\n",
    "                h_0 = profile[None,:,:]\n",
    "                print(f'h_0 shape: {h_0.shape}')\n",
    "                # h_0 needs to be of shape: num layers, batch size, embedding size)\n",
    "\n",
    "    \n",
    "#             out_local, _ = self.Local(inputs, h_0)\n",
    "#             print(f'Out_local shape: {out_local.shape}')\n",
    "            \n",
    "            if not isinstance(input_lengths, int):\n",
    "                inputs = nn.utils.rnn.pack_padded_sequence(inputs, input_lengths, batch_first=True, enforce_sorted=False)\n",
    "            out_global, _ = self.Global(inputs, h_0)\n",
    "            if not isinstance(input_lengths, int):\n",
    "                out_global, l = nn.utils.rnn.pad_packed_sequence(out_global, batch_first=True)\n",
    "            print(f'Unpacked Out_global shape: {out_global.shape}')\n",
    "            print(l)\n",
    "            # out shape (batch_size, seq_length, hidden_size), containing hidden state output for every step\n",
    "            # Shape of c_global and c_local should be: (sequence, hidden_size)\n",
    "            c_global = out_global\n",
    "            print(f'c_global shape: {c_global.shape}')\n",
    "            # c_global shape (seq_length, hidden_size)\n",
    "            c_local, alphas = self.calculate_c_local(out_global)\n",
    "            print(f'c_local shape: {c_local.shape}')\n",
    "\n",
    "            # Shape of c_global and c_local should be: (sequence, hidden_size)\n",
    "            c = self.dropout(torch.cat((c_global, c_local), dim=2))\n",
    "            # shape c (batch size, sequence length, embedding dim * 2)\n",
    "            print(c.shape)\n",
    "            # Decoder takes as inputs: embeddings for each item and c\n",
    "            embeds = torch.FloatTensor([[self.vocab.p2e[str(i)] for i in range(len(self.vocab.p2i))]]).to(device)\n",
    "            # shape embeds (1, number of products, embedding dim)\n",
    "            print(embeds.shape)\n",
    "#             embeds = self.ItemEmbedding(torch.LongTensor([i for i in range(len(self.vocab.p2i))]).to(device))\n",
    "            batch_size = c.shape[0]\n",
    "            sequence_length = c.shape[1]\n",
    "            nr_of_products = embeds.shape[1]\n",
    "#             out = c.new_empty(batch_size, sequence_length, nr_of_products)\n",
    "#             for b in range(batch_size):\n",
    "#                 for t in range(sequence_length):\n",
    "#                     for p in range(nr_of_products):\n",
    "# #                         tmp_c = c[b,t,:]\n",
    "# #                         print(tmp_c.shape)\n",
    "# #                         tmp_p = embeds[0,p,:]\n",
    "# #                         print(tmp_p.shape)\n",
    "#                         out[b,t,p] = self.Decoder(embeds[0,p,:].unsqueeze(0),c[b,t,:].unsqueeze(0)).squeeze()  \n",
    "#                     gc.collect()\n",
    "                \n",
    "            # Make embeds and c the same shape\n",
    "            embeds = embeds.repeat(c.shape[0],c.shape[1],1,1).contiguous()\n",
    "            print(embeds.shape)\n",
    "            c = c[:,:,None,:].repeat(1,1,embeds.shape[2],1).contiguous()\n",
    "            print(c.shape)\n",
    "            print(f'c shape: {c.shape}')\n",
    "            print(f'All item embeds shape: {embeds.shape}')\n",
    "            out = self.Decoder(embeds, c)[:,:,:,0]\n",
    "            print(f'Output shape: {out.shape}')\n",
    "            output = self.Softmax(out)\n",
    "            return output, alphas\n",
    "        else:    \n",
    "            # user shape (1)\n",
    "            # history shape (history_length)\n",
    "            # inputs shape (seq_length,1)\n",
    "            item_embeds_h = self.dropout(self.ItemEmbedding(history))\n",
    "    #         print(f'Item embedding shape (history): {item_embeds_h.shape}')\n",
    "            # item_embeds_h shape (history_length, embedding_size)\n",
    "            user_embed = self.dropout(self.UserEmbedding(user))\n",
    "    #         print(f'User embedding shape: {user_embed.shape}')\n",
    "            # user_embed shape (1, embedding_size) <- shouldn't this be (1, hidden_state_size)?\n",
    "            dense = self.LatentItemHistory(item_embeds_h)\n",
    "            dense = self.ActivationFn(dense)\n",
    "    #         print(f'Dense shape: {dense.shape}')\n",
    "            # dense shape (history_length, hidden_state_size)\n",
    "            alpha1 = self.Softmax(torch.matmul(user_embed,torch.transpose(dense, 0, 1)))\n",
    "    #         print(f'Alpha1 shape: {alpha1.shape}')\n",
    "            # alpha shape (history_length)\n",
    "            profile = torch.sum(torch.mul(alpha1,torch.transpose(dense,0,1)),1)\n",
    "    #         print(f'Profile shape: {profile.shape}')\n",
    "            # profile shape (hidden_state_size)\n",
    "            # reshape to correct hidden state dimensions\n",
    "            h_0 = torch.reshape(self.ActivationFn(self.ProfileToHidden(profile)),self.hidden_state_dim)\n",
    "    #         print(f'h_0 shape: {h_0.shape}')\n",
    "\n",
    "            # Embed the items of the current session\n",
    "            item_embeds_c = self.dropout(torch.transpose(self.ItemEmbedding(inputs),0,1))\n",
    "    #         print(f'Item embedding shape (current): {item_embeds_c.shape}')\n",
    "            # shape (seq_length, embedding_size)\n",
    "            # add a batch dimension to the front, necessary for GRU\n",
    "    #         item_embeds_c = item_embeds_c[None,:,:] \n",
    "    #         print(f'Item embedding shape (current): {item_embeds_c.shape}')\n",
    "\n",
    "#             out_local, _ = self.Local(item_embeds_c,h_0)\n",
    "    #         print(f'Out_local shape: {out_local.shape}')\n",
    "            out_global, _ = self.Global(item_embeds_c,h_0)\n",
    "    #         print(f'Out_global shape: {out_global.shape}')\n",
    "            # out shape (batch_size, seq_length, hidden_size), containing hidden state output for every step\n",
    "            # Shape of c_global and c_local should be: (sequence, hidden_size)\n",
    "            c_global = out_global[0,:,:]\n",
    "    #         print(f'c_global shape: {c_global.shape}')\n",
    "            # c_global shape (seq_length, hidden_size)\n",
    "            c_local, alphas = self.calculate_c_local(out_global)\n",
    "    #         print(f'c_local shape: {c_local.shape}')\n",
    "\n",
    "            # Shape of c_global and c_local should be: (sequence, hidden_size)\n",
    "            c = self.dropout(torch.cat((c_global, c_local), dim=2))\n",
    "            print(c.shape)\n",
    "            # Decoder takes as inputs: embeddings for each item and c\n",
    "            embeds = self.ItemEmbedding(torch.LongTensor([i for i in range(len(self.vocab.p2i))]).to(device))\n",
    "            print(embeds.shape)\n",
    "            # Make embeds and c the same shape\n",
    "            embeds = embeds.repeat(c.shape[0],1,1).contiguous()\n",
    "            c = c[:,None,:].repeat(1,embeds.shape[1],1).contiguous()\n",
    "    #         print(f'c shape: {c.shape}')\n",
    "    #         print(f'All item embeds shape: {embeds.shape}')\n",
    "            out = self.Decoder(embeds, c)[:,:,0]\n",
    "    #         print(f'Output shape: {out.shape}')\n",
    "            output = self.Softmax(out)\n",
    "            return output, alphas\n",
    "    \n",
    "    def calculate_c_local(self,H):\n",
    "        # H: hidden states returned from the GRU\n",
    "        # H shape (batch, sequence, hidden size)\n",
    "        # Initialise c_local with the output hidden states: every hidden state has a similarity of \n",
    "        # 1 with itself, so the entire hidden state is taken into account\n",
    "        c_local_base = H.clone().detach().requires_grad_(True)\n",
    "        c_local = H.new_empty(H.shape)\n",
    "  \n",
    "        alphas = torch.ones((H.shape[0],H.shape[1],H.shape[1]),dtype=torch.float32).to(device)\n",
    "        for b in range(H.shape[0]):\n",
    "            cs = [torch.zeros((1,H.shape[2]), dtype=torch.float32).to(device)]\n",
    "            for t in range(H.shape[1]):\n",
    "                # If it is the first hidden state, then there are no previous hidden states to calculate the \n",
    "                # alpha from and the current hidden state has already been saved in c_local.\n",
    "                if t == 0:\n",
    "                    continue\n",
    "\n",
    "                # Technically we do not need to store the alphas, but maybe we want to do something with these values\n",
    "                alphas_t = torch.zeros(H.shape[1], dtype=torch.float32).to(device)\n",
    "                A1 = self.A1(H[b,t,:])\n",
    "                for j in range(t):\n",
    "                    A2 = self.A2(H[b,j,:])\n",
    "                    # next three lines could be done in one line\n",
    "                    alphas_t[j] = self.v(self.ActivationFn(A1 + A2))\n",
    "    #                 ct_j = torch.mul(alphas_t[j],H[j])\n",
    "    #                 c_local[t] = torch.add(c_local[t],ct_j)\n",
    "                ct = torch.sum(alphas_t[:t].unsqueeze(1) * H[b,:t,:],dim=0).unsqueeze(0)\n",
    "                cs.append(ct)\n",
    "                alphas[b][t] = alphas_t\n",
    "            c_local[b] = torch.cat(cs,dim=0) + H[b]\n",
    "            del(cs)\n",
    "        return c_local, alphas\n",
    "    \n",
    "    def top1loss(self, output, targets):\n",
    "        print(output.shape, targets.shape)\n",
    "        output = output.view(1,-1,output.shape[2]).squeeze()\n",
    "        targets = targets.view(-1,1)\n",
    "        print(output.shape, targets.shape)\n",
    "        \n",
    "        # create a mask by filtering out all tokens that ARE NOT the padding token\n",
    "        print(targets)\n",
    "        mask = (targets > -1)\n",
    "        masklist = mask.view(-1).tolist()\n",
    "        inx = [r for r in range(len(masklist)) if masklist[r] == 1]\n",
    "#         print(mask)\n",
    "#         print(targets[mask])\n",
    "#         print(output)\n",
    "#         print(output[inx])\n",
    "        targets = targets[mask].unsqueeze(1)\n",
    "        output = output[inx]\n",
    "#         # count how many tokens we have\n",
    "#         nb_tokens = int(torch.sum(mask).data[0])\n",
    "#         # pick the values for the label and zero out the rest with the mask\n",
    "#         output = output[range(Y_hat.shape[0]), Y] * mask\n",
    "        \n",
    "        scores_for_targets = torch.gather(output, 1, targets)\n",
    "        loss = torch.mean(torch.sigmoid(output - scores_for_targets) +\n",
    "                torch.sigmoid(output**2))\n",
    "        return loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to train a model\n",
    "name_extension = ''\n",
    "def train_model(model, optimizer, num_epochs=200, \n",
    "                print_every=1, eval_every=1,\n",
    "                batch_fn=get_examples, \n",
    "                prep_fn=prepare_example,\n",
    "                eval_fn=simple_evaluate,\n",
    "                batch_size=10, eval_batch_size=None,\n",
    "                pre_trained=False,\n",
    "                predict=False\n",
    "               ):\n",
    "    \"\"\"Train a model.\"\"\"  \n",
    "    train_loss = 0.\n",
    "    start = time.time()\n",
    "    best_eval = 0.\n",
    "    best_iter = 0\n",
    "    eval_iter = 0\n",
    "    criterion=model.loss\n",
    "\n",
    "    # store train loss and validation accuracy during training\n",
    "    # so we can plot them afterwards\n",
    "    losses = []\n",
    "    accuracies = []  \n",
    "\n",
    "    if eval_batch_size is None:\n",
    "        eval_batch_size = batch_size\n",
    "    \n",
    "    vocab = model.vocab\n",
    "    \n",
    "    product_embeddings = Word2Vec(allSessions, size=model.hidden_state_size, window=5, min_count=1)\n",
    "#     print(product_embeddings.wv['1'])\n",
    "    user_embeddings = Doc2Vec(allUsers, vector_size=model.hidden_state_size, window=5, min_count=1)\n",
    "#     print(user_embeddings.wv['1'])\n",
    "    vocab.u2e = user_embeddings.wv\n",
    "    vocab.u2e.add(['0'], [np.zeros(model.hidden_state_size)])\n",
    "    vocab.p2e = product_embeddings.wv\n",
    "    vocab.p2e.add(['0'], [np.zeros(model.hidden_state_size)])\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        for example in batch_fn(train_data, batch_size=batch_size): # goes through the entire training data once, a.k.a. an epoch\n",
    "            gc.collect()\n",
    "            # forward pass, make sure the model is in train modus\n",
    "            model.train()\n",
    "            x, targets = prep_fn(example, vocab)\n",
    "            print(targets)\n",
    "#             print(example.userID)\n",
    "            output, alphas = model(x)\n",
    "            # output shape (sequence length, nr of products): a score for each product at each time step\n",
    "            # alphas are the alphas used in the Narm part\n",
    "\n",
    "            eval_iter += 1\n",
    "                \n",
    "            loss = criterion(output, targets)\n",
    "            train_loss += float(loss.item())\n",
    "\n",
    "            # backward pass\n",
    "            # erase previous gradients\n",
    "            model.zero_grad()\n",
    "\n",
    "            # compute gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # update weights - take a small step in the opposite dir of the gradient\n",
    "            optimizer.step()\n",
    "            \n",
    "            break\n",
    "            \n",
    "            if eval_iter % 100 == 0:\n",
    "                accuracy, _ = eval_fn(model, dev_data, batch_size=eval_batch_size,\n",
    "                                         batch_fn=batch_fn, prep_fn=prep_fn)\n",
    "                accuracies.append(accuracy)\n",
    "                print(\"epoch %r: dev acc=%.4f\" % (epoch + 1, accuracy))       \n",
    "\n",
    "                # save best model parameters\n",
    "                if accuracy > best_eval:\n",
    "                    print(\"new highscore\")\n",
    "                    best_eval = accuracy\n",
    "                    best_iter = epoch + 1\n",
    "                    path = \"{}{}.pt\".format(model.__class__.__name__,name_extension)\n",
    "                    ckpt = {\n",
    "                      \"state_dict\": model.state_dict(),\n",
    "                      \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                      \"best_eval\": best_eval,\n",
    "                      \"best_iter\": best_iter\n",
    "                    }\n",
    "                    torch.save(ckpt, path)\n",
    "         \n",
    "        break\n",
    "        \n",
    "        if (epoch + 1) % print_every == 0:\n",
    "            print(\"Epoch %r: loss=%.4f, time=%.2fs\" % \n",
    "                 (epoch + 1, train_loss, time.time()-start))\n",
    "            losses.append(train_loss)       \n",
    "            train_loss = 0.\n",
    "            \n",
    "        if (epoch + 1) % eval_every == 0:\n",
    "            accuracy, _ = eval_fn(model, dev_data, batch_size=eval_batch_size,\n",
    "                                         batch_fn=batch_fn, prep_fn=prep_fn)\n",
    "            accuracies.append(accuracy)\n",
    "            print(\"epoch %r: dev acc=%.4f\" % (epoch + 1, accuracy))       \n",
    "\n",
    "            # save best model parameters\n",
    "            if accuracy > best_eval:\n",
    "                print(\"new highscore\")\n",
    "                best_eval = accuracy\n",
    "                best_iter = epoch + 1\n",
    "                path = \"{}{}.pt\".format(model.__class__.__name__,name_extension)\n",
    "                ckpt = {\n",
    "                  \"state_dict\": model.state_dict(),\n",
    "                  \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                  \"best_eval\": best_eval,\n",
    "                  \"best_iter\": best_iter\n",
    "                }\n",
    "                torch.save(ckpt, path)\n",
    "    \n",
    "    # Done training\n",
    "    # evaluate on train, dev, and test with best model\n",
    "    print(\"Loading best model\")\n",
    "    path = \"{}{}.pt\".format(model.__class__.__name__,name_extension)        \n",
    "    ckpt = torch.load(path)\n",
    "    model.load_state_dict(ckpt[\"state_dict\"])\n",
    "\n",
    "    train_acc, _ = eval_fn(\n",
    "        model, train_data, batch_size=eval_batch_size, \n",
    "        batch_fn=batch_fn, prep_fn=prep_fn)\n",
    "    dev_acc, _ = eval_fn(\n",
    "        model, dev_data, batch_size=eval_batch_size,\n",
    "        batch_fn=batch_fn, prep_fn=prep_fn)\n",
    "    test_acc, predictions = eval_fn(\n",
    "        model, test_data, batch_size=eval_batch_size, \n",
    "        batch_fn=batch_fn, prep_fn=prep_fn)\n",
    "\n",
    "    print(\"best model iter {:d}: \"\n",
    "          \"train acc={:.4f}, dev acc={:.4f}, test acc={:.4f}\".format(\n",
    "              best_iter, train_acc, dev_acc, test_acc))\n",
    "\n",
    "    return test_acc, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createSplits(data, k):\n",
    "    folds = {}\n",
    "    for i in range(k):\n",
    "        dev = data[math.ceil(i*len(data)/k) : math.ceil((i+1)*len(data)/k)]\n",
    "        train = [x for x in data if x not in dev]\n",
    "        folds[i] = train,dev\n",
    "    return folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2902, 38878, 14828,  6834, 44964,   587, 58693,   587,    -1,    -1,\n",
      "            -1,    -1],\n",
      "        [21673, 32519, 32520, 32521, 32522, 17936,    -1,    -1,    -1,    -1,\n",
      "            -1,    -1],\n",
      "        [51765, 46554, 55729, 62367, 31223,  5902,  1896,  6156, 25988, 27341,\n",
      "         52532,  4333]])\n",
      "Inputs shape: torch.Size([3, 12, 50])\n",
      "History shape: torch.Size([3, 25, 50])\n",
      "User shape: torch.Size([3, 1, 50])\n",
      "Input lengths shape: torch.Size([3])\n",
      "Input lengths: tensor([ 8,  6, 12])\n",
      "Alpha shape: torch.Size([3, 25, 1])\n",
      "Profile shape: torch.Size([3, 50])\n",
      "h_0 shape: torch.Size([1, 3, 50])\n",
      "Unpacked Out_global shape: torch.Size([3, 12, 50])\n",
      "tensor([ 8,  6, 12])\n",
      "c_global shape: torch.Size([3, 12, 50])\n",
      "c_local shape: torch.Size([3, 12, 50])\n",
      "torch.Size([3, 12, 100])\n",
      "torch.Size([1, 67172, 50])\n",
      "torch.Size([3, 12, 67172, 50])\n",
      "torch.Size([3, 12, 67172, 100])\n",
      "c shape: torch.Size([3, 12, 67172, 100])\n",
      "All item embeds shape: torch.Size([3, 12, 67172, 50])\n",
      "Output shape: torch.Size([3, 12, 67172])\n",
      "torch.Size([3, 12, 67172]) torch.Size([3, 12])\n",
      "torch.Size([36, 67172]) torch.Size([36, 1])\n",
      "tensor([[ 2902],\n",
      "        [38878],\n",
      "        [14828],\n",
      "        [ 6834],\n",
      "        [44964],\n",
      "        [  587],\n",
      "        [58693],\n",
      "        [  587],\n",
      "        [   -1],\n",
      "        [   -1],\n",
      "        [   -1],\n",
      "        [   -1],\n",
      "        [21673],\n",
      "        [32519],\n",
      "        [32520],\n",
      "        [32521],\n",
      "        [32522],\n",
      "        [17936],\n",
      "        [   -1],\n",
      "        [   -1],\n",
      "        [   -1],\n",
      "        [   -1],\n",
      "        [   -1],\n",
      "        [   -1],\n",
      "        [51765],\n",
      "        [46554],\n",
      "        [55729],\n",
      "        [62367],\n",
      "        [31223],\n",
      "        [ 5902],\n",
      "        [ 1896],\n",
      "        [ 6156],\n",
      "        [25988],\n",
      "        [27341],\n",
      "        [52532],\n",
      "        [ 4333]])\n",
      "Loading best model\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'example' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-e3141812c607>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m                    \u001b[0mprep_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepare_minibatch_pre_trained\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                    \u001b[0mbatch_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_minibatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                    batch_size=3)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-34-66121f11c48f>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, num_epochs, print_every, eval_every, batch_fn, prep_fn, eval_fn, batch_size, eval_batch_size, pre_trained, predict)\u001b[0m\n\u001b[1;32m    124\u001b[0m     train_acc, _ = eval_fn(\n\u001b[1;32m    125\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_batch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m         batch_fn=batch_fn, prep_fn=prep_fn)\n\u001b[0m\u001b[1;32m    127\u001b[0m     dev_acc, _ = eval_fn(\n\u001b[1;32m    128\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_batch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-69-ae595f6c6fcd>\u001b[0m in \u001b[0;36mrecall\u001b[0;34m(model, data, prep_fn, batch_fn, at, batch_size, **kwargs)\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;31m# convert the example input and targets to PyTorch tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;31m#         targets.extend([int(t) for t in example.target])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprep_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m         \u001b[0;31m# forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0;31m# get the output from the neural network for input x\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'example' is not defined"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "device='cpu'\n",
    "\n",
    "train_data = trainData\n",
    "test_data = testData\n",
    "dev_data = testData[:100]\n",
    "\n",
    "num_users = len(userList)\n",
    "num_products = len(productList)\n",
    "\n",
    "# item_embedding_dim, user_embedding_dim, hidden_size, output_dim, num_layers, vocab, \n",
    "# model = NarmPlus(math.ceil(num_products**0.25),math.ceil(num_users**0.25),10,1,1,v,dropout=0.2)\n",
    "model = NarmPlus(50,50,50,1,1,v,dropout=0.2,pre_trained=True, batch_size=3)\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "a, p = train_model(model, optimizer, eval_fn=recall, \n",
    "                   num_epochs=1, \n",
    "                   prep_fn=prepare_minibatch_pre_trained,\n",
    "                   batch_fn=get_minibatch,\n",
    "                   batch_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alph = torch.FloatTensor(\n",
    "    [\n",
    "        [[1],[0]],\n",
    "        [[10],[-10]],\n",
    "        [[5],[-5]],\n",
    "        [[0],[1]],\n",
    "        [[2],[3]]\n",
    "    ]\n",
    ")\n",
    "\n",
    "prod = torch.FloatTensor(\n",
    "    [\n",
    "        [[1,1,1],[2,2,2]],\n",
    "        [[1,1,1],[2,2,2]],\n",
    "        [[1,1,1],[2,2,2]],\n",
    "        [[1,1,1],[2,2,2]],\n",
    "        [[1,1,1],[2,2,2]]\n",
    "    ]\n",
    ")\n",
    "\n",
    "usr = torch.FloatTensor(\n",
    "    [\n",
    "        [[1,2,3]],\n",
    "        [[4,5,6]],\n",
    "        [[7,8,9]],\n",
    "        [[10,11,12]],\n",
    "        [[13,14,15]]\n",
    "    ]\n",
    ")\n",
    "print(usr.shape)\n",
    "print(prod.shape)\n",
    "\n",
    "a = torch.matmul(prod,torch.transpose(usr,1,2))\n",
    "print(a)\n",
    "ass = torch.softmax(a,1)\n",
    "print(ass)\n",
    "\n",
    "torch.t(torch.sum(torch.mul(ass,prod),1)).shape\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 3]\n",
      "tensor([[[ 99.],\n",
      "         [ 99.]],\n",
      "\n",
      "        [[ 10.],\n",
      "         [-10.]],\n",
      "\n",
      "        [[  5.],\n",
      "         [ -5.]],\n",
      "\n",
      "        [[ 99.],\n",
      "         [ 99.]],\n",
      "\n",
      "        [[  2.],\n",
      "         [  3.]]])\n",
      "torch.Size([5, 2, 1])\n"
     ]
    }
   ],
   "source": [
    "alph = torch.FloatTensor(\n",
    "    [\n",
    "        [[1],[0]],\n",
    "        [[10],[-10]],\n",
    "        [[5],[-5]],\n",
    "        [[0],[1]],\n",
    "        [[2],[3]]\n",
    "    ]\n",
    ")\n",
    "inx = [1,0,0,1,0]\n",
    "selinx = [r for r in range(alph.shape[0]) if inx[r] == 1]\n",
    "print(selinx)\n",
    "alph[selinx] = 99\n",
    "print(alph)\n",
    "print(alph.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
