{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NARM+\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing functions.\n",
    "import pandas\n",
    "import csv\n",
    "from collections import Counter, OrderedDict, defaultdict, namedtuple\n",
    "import itertools\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# PyTorch can run on CPU or on Nvidia GPU (video card) using CUDA\n",
    "# This cell selects the GPU if one is available.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.synchronize() \n",
    "device = 'cpu'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>USERID</th>\n",
       "      <th>PRODUCTID</th>\n",
       "      <th>CATEGORYID</th>\n",
       "      <th>ACTION</th>\n",
       "      <th>TIMESTAMP</th>\n",
       "      <th>SESSION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2268318</td>\n",
       "      <td>2520377</td>\n",
       "      <td>pv</td>\n",
       "      <td>1511544070</td>\n",
       "      <td>2017-11-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2333346</td>\n",
       "      <td>2520771</td>\n",
       "      <td>pv</td>\n",
       "      <td>1511561733</td>\n",
       "      <td>2017-11-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2576651</td>\n",
       "      <td>149192</td>\n",
       "      <td>pv</td>\n",
       "      <td>1511572885</td>\n",
       "      <td>2017-11-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3830808</td>\n",
       "      <td>4181361</td>\n",
       "      <td>pv</td>\n",
       "      <td>1511593493</td>\n",
       "      <td>2017-11-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>4365585</td>\n",
       "      <td>2520377</td>\n",
       "      <td>pv</td>\n",
       "      <td>1511596146</td>\n",
       "      <td>2017-11-25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   USERID  PRODUCTID  CATEGORYID ACTION   TIMESTAMP     SESSION\n",
       "0       1    2268318     2520377     pv  1511544070  2017-11-24\n",
       "1       1    2333346     2520771     pv  1511561733  2017-11-24\n",
       "2       1    2576651      149192     pv  1511572885  2017-11-25\n",
       "3       1    3830808     4181361     pv  1511593493  2017-11-25\n",
       "4       1    4365585     2520377     pv  1511596146  2017-11-25"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create sessions\n",
    "sample = pandas.read_pickle(\"./data/smallTaoBao.pkl\")\n",
    "sample['SESSION'] = pandas.to_datetime(sample['TIMESTAMP'],unit='s').dt.date\n",
    "sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of sessions per user\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7.2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Average number of sessions per user\")\n",
    "sample.groupby('USERID')['SESSION'].nunique().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of clicks per session\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14.126388888888888"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Average number of clicks per session\")\n",
    "sample.groupby(['USERID', 'SESSION'])['ACTION'].count().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>USERID</th>\n",
       "      <th>PRODUCTID</th>\n",
       "      <th>CATEGORYID</th>\n",
       "      <th>ACTION</th>\n",
       "      <th>TIMESTAMP</th>\n",
       "      <th>SESSION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2268318</td>\n",
       "      <td>2520377</td>\n",
       "      <td>pv</td>\n",
       "      <td>1511544070</td>\n",
       "      <td>2017-11-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2333346</td>\n",
       "      <td>2520771</td>\n",
       "      <td>pv</td>\n",
       "      <td>1511561733</td>\n",
       "      <td>2017-11-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2576651</td>\n",
       "      <td>149192</td>\n",
       "      <td>pv</td>\n",
       "      <td>1511572885</td>\n",
       "      <td>2017-11-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3830808</td>\n",
       "      <td>4181361</td>\n",
       "      <td>pv</td>\n",
       "      <td>1511593493</td>\n",
       "      <td>2017-11-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>4365585</td>\n",
       "      <td>2520377</td>\n",
       "      <td>pv</td>\n",
       "      <td>1511596146</td>\n",
       "      <td>2017-11-25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   USERID  PRODUCTID  CATEGORYID ACTION   TIMESTAMP     SESSION\n",
       "0       1    2268318     2520377     pv  1511544070  2017-11-24\n",
       "1       1    2333346     2520771     pv  1511561733  2017-11-24\n",
       "2       1    2576651      149192     pv  1511572885  2017-11-25\n",
       "3       1    3830808     4181361     pv  1511593493  2017-11-25\n",
       "4       1    4365585     2520377     pv  1511596146  2017-11-25"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user1 = sample.loc[sample['USERID'] == 1]\n",
    "user1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2268318, 2333346], [2576651, 3830808, 4365585, 4606018, 230380], [3827899, 3745169, 1531036], [2266567, 2951368, 3108797, 1338525, 2286574], [5002615, 2734026, 5002615, 3239041, 4615417, 4152983, 266784, 46259, 266784, 4092065, 1305059], [2791761, 3239041, 46259, 4973305, 2087357, 3157558], [2087357, 4170517, 1340922, 3911125, 4170517, 3682069, 4954999, 79715, 4666650], [1323189, 4198227, 4954999], [2041056, 3219016, 2104483, 2028434, 3219016, 2278603, 929177], [4954999, 818610, 271696, 568695]]\n"
     ]
    }
   ],
   "source": [
    "# Create nested list of sessions and items per user\n",
    "userBase = sample.groupby(['USERID', 'SESSION'])['PRODUCTID'].apply(list).groupby('USERID').apply(list)\n",
    "print(userBase[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example(userID=1, history=[2268318, 2333346, 2576651, 3830808, 4365585, 4606018, 230380, 3827899, 3745169, 1531036, 2266567, 2951368, 3108797, 1338525, 2286574, 5002615, 2734026, 5002615, 3239041, 4615417, 4152983, 266784, 46259, 266784, 4092065, 1305059, 2791761, 3239041, 46259, 4973305, 2087357, 3157558, 2087357, 4170517, 1340922, 3911125, 4170517, 3682069, 4954999, 79715, 4666650, 1323189, 4198227, 4954999], inputs=[2041056, 3219016, 2104483, 2028434, 3219016, 2278603], target=[3219016, 2104483, 2028434, 3219016, 2278603, 929177])\n",
      "\n",
      "Example(userID=1, history=[2268318, 2333346, 2576651, 3830808, 4365585, 4606018, 230380, 3827899, 3745169, 1531036, 2266567, 2951368, 3108797, 1338525, 2286574, 5002615, 2734026, 5002615, 3239041, 4615417, 4152983, 266784, 46259, 266784, 4092065, 1305059, 2791761, 3239041, 46259, 4973305, 2087357, 3157558, 2087357, 4170517, 1340922, 3911125, 4170517, 3682069, 4954999, 79715, 4666650, 1323189, 4198227, 4954999, 2041056, 3219016, 2104483, 2028434, 3219016, 2278603, 929177], inputs=[4954999, 818610, 271696], target=[818610, 271696, 568695])\n"
     ]
    }
   ],
   "source": [
    "# More efficient create examples function\n",
    "# A simple way to define a class is using namedtuple.\n",
    "Example = namedtuple(\"Example\", [\"userID\", \"history\", \"inputs\", \"target\"])\n",
    "\n",
    "def f(userid, sessions, train):\n",
    "    if train:\n",
    "        object_train = Example(userID = userid, history = [item for sublist in sessions[:-2] for item in sublist], inputs = sessions[-2][:-1], target = sessions[-2][1:])\n",
    "        return object_train\n",
    "    else:\n",
    "        return Example(userID = userid, history = [item for sublist in sessions[:-1] for item in sublist], inputs = sessions[-1][:-1], target = sessions[-1][1:])\n",
    "\n",
    "def createExamples(userBase):\n",
    "    ''' Create training and testing set '''\n",
    "    userBase = pandas.DataFrame(userBase)\n",
    "    userBase.reset_index(level = 0, inplace = True)\n",
    "    trainData = userBase.apply(lambda x: f(x['USERID'], x['PRODUCTID'], True), axis = 1).tolist()\n",
    "    testData = userBase.apply(lambda x: f(x['USERID'], x['PRODUCTID'], False), axis = 1).tolist()\n",
    "    return trainData, testData\n",
    "\n",
    "trainData, testData = createExamples(userBase)\n",
    "print(trainData[0])\n",
    "print('')\n",
    "print(testData[0])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we first define a class that can map a product to an ID (p2i)\n",
    "# and back (i2p).\n",
    "\n",
    "class OrderedCounter(Counter, OrderedDict):\n",
    "    \"\"\"Counter that remembers the order elements are first seen\"\"\"\n",
    "    def __repr__(self):\n",
    "        return '%s(%r)' % (self.__class__.__name__,\n",
    "                      OrderedDict(self))\n",
    "    def __reduce__(self):\n",
    "        return self.__class__, (OrderedDict(self),)\n",
    "\n",
    "\n",
    "class Vocabulary:\n",
    "    \"\"\"A vocabulary, assigns IDs to tokens\"\"\"\n",
    "    def __init__(self):\n",
    "        self.freqs = OrderedCounter()\n",
    "        self.users = []\n",
    "        self.u2i = {}\n",
    "        self.i2u = []\n",
    "        self.p2i = {}\n",
    "        self.i2p = []\n",
    "\n",
    "    def count_product(self, t):\n",
    "        self.freqs[t] += 1\n",
    "    \n",
    "    def count_user(self, t):\n",
    "        self.users.append(t)\n",
    "\n",
    "    def add_product(self, t):\n",
    "        self.p2i[t] = len(self.p2i)\n",
    "        self.i2p.append(t) \n",
    "        \n",
    "    def add_user(self, t):\n",
    "        self.u2i[t] = len(self.u2i)\n",
    "        self.i2u.append(t)\n",
    "\n",
    "    def build(self, min_freq=0):\n",
    "        self.add_product(\"<unk>\")  # reserve 0 for <unk> (unknown products (products only occuring in test set))\n",
    "        self.add_user(\"<unk>\")\n",
    "        tok_freq = list(self.freqs.items())\n",
    "        tok_freq.sort(key=lambda x: x[1], reverse=True)\n",
    "        for tok, freq in tok_freq:\n",
    "            if freq >= min_freq:\n",
    "                self.add_product(tok)\n",
    "        for user in self.users:\n",
    "            self.add_user(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 6702\n"
     ]
    }
   ],
   "source": [
    "# This process should be deterministic and should have the same result \n",
    "# if run multiple times on the same data set.\n",
    "\n",
    "def build_voc(data_sets):\n",
    "    v = Vocabulary()\n",
    "    for data_set in data_sets:\n",
    "        for ex in data_set:\n",
    "            for product in ex.history + ex.inputs:\n",
    "                v.count_product(product)\n",
    "            v.count_user(ex.userID)\n",
    "        v.build()\n",
    "    return v\n",
    "\n",
    "v = build_voc([trainData])\n",
    "print(\"Vocabulary size:\", len(v.p2i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HELPER FUNCTIONS\n",
    "\n",
    "# function to yield one example at a time\n",
    "def get_examples(data, shuffle=True, **kwargs):\n",
    "    \"\"\"Shuffle data set and return 1 example at a time (until nothing left)\"\"\"\n",
    "    if shuffle:\n",
    "#         print(\"Shuffling training data\")\n",
    "        random.shuffle(data)  # shuffle training data each epoch\n",
    "    for example in data:\n",
    "        yield example\n",
    "    \n",
    "# function to prepare an example for usage by the model\n",
    "def prepare_example(example, vocab):\n",
    "    \"\"\"\n",
    "    Turn an example into tensors of inputs and target.\n",
    "    \"\"\"\n",
    "    u = vocab.u2i.get(example.userID,0)\n",
    "    v = torch.LongTensor([u])\n",
    "    v = v.to(device)\n",
    "    \n",
    "    w = torch.LongTensor([vocab.p2i.get(t, 0) for t in example.history])\n",
    "    w = w.to(device)\n",
    "    \n",
    "    x = torch.LongTensor([vocab.p2i.get(t, 0) for t in example.inputs])[:,None]\n",
    "    x = x.to(device)\n",
    "\n",
    "    y = torch.LongTensor([vocab.p2i.get(t, 0) for t in example.target])[:,None]\n",
    "    y = y.to(device)\n",
    "\n",
    "    return (v, w, x), y\n",
    "\n",
    "# simple evaluation function\n",
    "def simple_evaluate(model, data, prep_fn=prepare_example, **kwargs):\n",
    "    \"\"\"Explained Variance Score of a model on given data set.\"\"\"\n",
    "    model.eval()  # disable dropout (explained later)\n",
    "    targets = []\n",
    "    predictions = []\n",
    "\n",
    "    for example in data:\n",
    "\n",
    "        # convert the example input and label to PyTorch tensors\n",
    "        targets.append(example.target)\n",
    "        x, target = prepare_example(example)\n",
    "\n",
    "        # forward pass\n",
    "        # get the output from the neural network for input x\n",
    "        with torch.no_grad():\n",
    "            output = model(x)\n",
    "        # output shape: (batch, output_size)\n",
    "        prediction = output[0].tolist()\n",
    "        predictions.append(prediction)\n",
    "            \n",
    "    score = explained_variance_score(targets, predictions, multioutput='variance_weighted')\n",
    "\n",
    "    return score, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom NN\n",
    "\n",
    "class NarmPlus(nn.Module):\n",
    "    def __init__(self, \n",
    "                 embedding_dim, hidden_size, output_dim, num_layers, \n",
    "                 vocab, \n",
    "                 activation_fn=nn.RReLU()):\n",
    "        super(NarmPlus, self).__init__()\n",
    "        # Store parameters\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_size = hidden_size # hidden size is also user embedding dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_layers = num_layers\n",
    "        # Shape of hidden_state: (num_layers * num_directions, batch, hidden_size)\n",
    "        self.hidden_state_dim = (num_layers, 1, hidden_size)\n",
    "        self.hidden_state_size = num_layers * hidden_size\n",
    "        self.vocab = vocab\n",
    "        num_users = len(vocab.u2i)\n",
    "        num_items = len(vocab.p2i)\n",
    "        \n",
    "        # General part\n",
    "        self.ActivationFn = activation_fn\n",
    "        self.Softmax = nn.Softmax(dim=1)\n",
    "        self.loss = self.top1\n",
    "        \n",
    "        # History part\n",
    "        self.UserEmbedding = nn.Embedding(num_users, hidden_size)\n",
    "        self.ItemEmbedding = nn.Embedding(num_items, embedding_dim)\n",
    "        self.LatentItemHistory = nn.Linear(embedding_dim, self.hidden_state_size)\n",
    "        \n",
    "        # NARM Part\n",
    "        # Input to the GRU is the item embedding: input_size = embedding_size\n",
    "        # Hidden size is something we can experiment with\n",
    "        self.Local = nn.GRU(embedding_dim, hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.Global = nn.GRU(embedding_dim, hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.Decoder = nn.Bilinear(embedding_dim, 2*hidden_size, output_dim)\n",
    "        \n",
    "        # Inner working of NARM attention part\n",
    "        # Latent space for alpha: what value to pick?\n",
    "        # I assume no bias, based on the paper\n",
    "        latent_space = hidden_size\n",
    "        self.A1 = nn.Linear(hidden_size,latent_space,bias=False)\n",
    "        self.A2 = nn.Linear(hidden_size,latent_space,bias=False)\n",
    "        self.v = nn.Linear(latent_space,1,bias=False)\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        user, history, inputs = x\n",
    "        # user shape (1)\n",
    "        # history shape (history_length)\n",
    "        # inputs shape (seq_length,1)\n",
    "        item_embeds_h = self.ItemEmbedding(history)\n",
    "        print(f'Item embedding shape (history): {item_embeds_h.shape}')\n",
    "        # item_embeds_h shape (history_length, embedding_size)\n",
    "        user_embed = self.UserEmbedding(user)\n",
    "        print(f'User embedding shape: {user_embed.shape}')\n",
    "        # user_embed shape (1, embedding_size) <- shouldn't this be (1, hidden_state_size)?\n",
    "        dense = self.LatentItemHistory(item_embeds_h)\n",
    "        dense = self.ActivationFn(dense)\n",
    "        print(f'Dense shape: {dense.shape}')\n",
    "        # dense shape (history_length, hidden_state_size)\n",
    "        alpha1 = self.Softmax(torch.matmul(user_embed,torch.transpose(dense, 0, 1)))\n",
    "        print(f'Alpha1 shape: {alpha1.shape}')\n",
    "        # alpha shape (history_length)\n",
    "        profile = torch.sum(torch.mul(alpha1,torch.transpose(dense,0,1)),1)\n",
    "        print(f'Profile shape: {profile.shape}')\n",
    "        # profile shape (hidden_state_size)\n",
    "        # reshape to correct hidden state dimensions\n",
    "        h_0 = torch.reshape(profile,self.hidden_state_dim)\n",
    "        print(f'h_0 shape: {h_0.shape}')\n",
    "        \n",
    "        # Embed the items of the current session\n",
    "        item_embeds_c = self.ItemEmbedding(inputs).squeeze()\n",
    "        print(f'Item embedding shape (current): {item_embeds_c.shape}')\n",
    "        # shape (seq_length, embedding_size)\n",
    "        # add a batch dimension to the front, necessary for GRU\n",
    "        item_embeds_c = item_embeds_c[None,:,:] \n",
    "        print(f'Item embedding shape (current): {item_embeds_c.shape}')\n",
    "        \n",
    "        out_local, _ = self.Local(item_embeds_c,h_0)\n",
    "        print(f'Out_local shape: {out_local.shape}')\n",
    "        out_global, _ = self.Global(item_embeds_c,h_0)\n",
    "        print(f'Out_global shape: {out_global.shape}')\n",
    "        # out shape (batch_size, seq_length, hidden_size), containing hidden state output for every step\n",
    "        # Shape of c_global and c_local should be: (sequence, hidden_size)\n",
    "        c_global = out_global.squeeze()\n",
    "        print(f'c_global shape: {c_global.shape}')\n",
    "        # c_global shape (seq_length, hidden_size)\n",
    "        c_local, alphas = self.calculate_c_local(out_local)\n",
    "        print(f'c_local shape: {c_local.shape}')\n",
    "        \n",
    "        # Shape of c_global and c_local should be: (sequence, hidden_size)\n",
    "        c = torch.cat((c_global, c_local), dim=1)\n",
    "        # Decoder takes as inputs: embeddings for each item and c\n",
    "        embeds = self.ItemEmbedding(torch.LongTensor([i for i in range(len(self.vocab.p2i))]).to(device))\n",
    "        # Make embeds and c the same shape\n",
    "        embeds = embeds.repeat(c.shape[0],1,1).contiguous()\n",
    "        c = c[:,None,:].repeat(1,embeds.shape[1],1).contiguous()\n",
    "        print(f'c shape: {c.shape}')\n",
    "        print(f'All item embeds shape: {embeds.shape}')\n",
    "        out = self.Decoder(embeds, c).squeeze()\n",
    "        print(out.shape)\n",
    "        output = self.Softmax(out)\n",
    "        return output, alphas\n",
    "    \n",
    "    def calculate_c_local(self,H):\n",
    "        # H: hidden states returned from the GRU\n",
    "        # H shape (batch, sequence, hidden size)\n",
    "        H = H.squeeze()\n",
    "        # H shape (sequence, hidden size)\n",
    "        # Initialise c_local with the output hidden states: every hidden state has a similarity of \n",
    "        # 1 with itself, so the entire hidden state is taken into account\n",
    "        c_local = H.clone().detach().requires_grad_(True)\n",
    "        \n",
    "  \n",
    "        alphas = torch.ones((H.shape[0],H.shape[0]),dtype=torch.float64).to(device)\n",
    "        \n",
    "        for t in range(H.shape[0]):\n",
    "            # If it is the first hidden state, then there are no previous hidden states to calculate the \n",
    "            # alpha from and the current hidden state has already been saved in c_local.\n",
    "            if t == 0:\n",
    "                continue\n",
    "            \n",
    "            # Technically we do not need to store the alphas, but maybe we want to do something with these values\n",
    "            alphas_t = torch.zeros(H.shape[0], dtype=torch.float64).to(device)\n",
    "            A1 = self.A1(H[t])\n",
    "            for j in range(t):\n",
    "                A2 = self.A2(H[j])\n",
    "                # next three lines could be done in one line\n",
    "                alphas_t[j] = self.v(self.ActivationFn(torch.add(A1,A2)))\n",
    "                ct_j = torch.mul(alphas_t[j],H[j])\n",
    "                c_local[t] = torch.add(c_local[t],ct_j)\n",
    "            alphas[t] = alphas_t\n",
    "        return c_local, alphas\n",
    "    \n",
    "    def top1(self, yhat):\n",
    "        ''' Top1 loss, yhat is vector with softmax probabilities '''\n",
    "        # Not sure if you can just call backward to this, but I think it should work. Code from:\n",
    "        # https://github.com/mquad/hgru4rec/blob/master/src/hgru4rec.py\n",
    "        yhatT = torch.transpose(yhat, 0, 1)\n",
    "        loss = torch.mean(torch.mean(nn.sigmoid( - torch.diag(yhat) + yhatT) + nn.sigmoid(yhat ** 2), dim = 0) - nn.sigmoid(T.diag(yhat ** 2)))\n",
    "        return loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to train a model\n",
    "name_extension = ''\n",
    "def train_model(model, optimizer, num_iterations=100000, \n",
    "                print_every=100000, eval_every=100000,\n",
    "                batch_fn=get_examples, \n",
    "                prep_fn=prepare_example,\n",
    "                eval_fn=simple_evaluate,\n",
    "                batch_size=1, eval_batch_size=None,\n",
    "                predict=False\n",
    "               ):\n",
    "    \"\"\"Train a model.\"\"\"  \n",
    "    iter_i = 0\n",
    "    train_loss = 0.\n",
    "    print_num = 0\n",
    "    start = time.time()\n",
    "    best_eval = 0.\n",
    "    best_iter = 0\n",
    "    criterion=model.loss\n",
    "\n",
    "    # store train loss and validation accuracy during training\n",
    "    # so we can plot them afterwards\n",
    "    losses = []\n",
    "    accuracies = []  \n",
    "\n",
    "    if eval_batch_size is None:\n",
    "        eval_batch_size = batch_size\n",
    "\n",
    "    while True:  # when we run out of examples, shuffle and continue\n",
    "        for batch in batch_fn(train_data, batch_size=batch_size):\n",
    "\n",
    "            # forward pass\n",
    "            vocab = model.vocab\n",
    "            model.train()\n",
    "            x, targets = prep_fn(batch, vocab)\n",
    "            output, alphas = model(x)\n",
    "            # output shape: (batch, outputsize)\n",
    "\n",
    "            # B stands for batch size\n",
    "            B = targets.size(0)  # later we will use B examples per update\n",
    "\n",
    "            # compute Huber loss (our criterion)\n",
    "            loss = criterion(output, targets)\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            # backward pass\n",
    "            # erase previous gradients\n",
    "            model.zero_grad()\n",
    "\n",
    "            # compute gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # update weights - take a small step in the opposite dir of the gradient\n",
    "            optimizer.step()\n",
    "\n",
    "            print_num += 1\n",
    "            iter_i += 1\n",
    "\n",
    "            # print info\n",
    "            if iter_i % print_every == 0:\n",
    "#                 print(\"Iter %r: loss=%.4f, time=%.2fs\" % \n",
    "#                       (iter_i, train_loss, time.time()-start))\n",
    "                losses.append(train_loss)\n",
    "                print_num = 0        \n",
    "                train_loss = 0.\n",
    "\n",
    "            # evaluate\n",
    "            if iter_i % eval_every == 0:\n",
    "                accuracy, _ = eval_fn(model, dev_data, batch_size=eval_batch_size,\n",
    "                                         batch_fn=batch_fn, prep_fn=prep_fn)\n",
    "                accuracies.append(accuracy)\n",
    "#                 print(\"iter %r: dev acc=%.4f\" % (iter_i, accuracy))       \n",
    "\n",
    "                # save best model parameters\n",
    "                if accuracy > best_eval:\n",
    "#                     print(\"new highscore\")\n",
    "                    best_eval = accuracy\n",
    "                    best_iter = iter_i\n",
    "                    path = \"{}{}.pt\".format(model.__class__.__name__,name_extension)\n",
    "                    ckpt = {\n",
    "                      \"state_dict\": model.state_dict(),\n",
    "                      \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                      \"best_eval\": best_eval,\n",
    "                      \"best_iter\": best_iter\n",
    "                    }\n",
    "                    torch.save(ckpt, path)\n",
    "\n",
    "            # done training\n",
    "            if iter_i == num_iterations:\n",
    "#                 print(\"Done training\")\n",
    "\n",
    "                # evaluate on train, dev, and test with best model\n",
    "#                 print(\"Loading best model\")\n",
    "                path = \"{}{}.pt\".format(model.__class__.__name__,name_extension)        \n",
    "                ckpt = torch.load(path)\n",
    "                model.load_state_dict(ckpt[\"state_dict\"])\n",
    "\n",
    "                train_acc, _ = eval_fn(\n",
    "                    model, train_data, batch_size=eval_batch_size, \n",
    "                    batch_fn=batch_fn, prep_fn=prep_fn)\n",
    "                dev_acc, _ = eval_fn(\n",
    "                    model, dev_data, batch_size=eval_batch_size,\n",
    "                    batch_fn=batch_fn, prep_fn=prep_fn)\n",
    "                test_acc, predictions = eval_fn(\n",
    "                    model, test_data, batch_size=eval_batch_size, \n",
    "                    batch_fn=batch_fn, prep_fn=prep_fn)\n",
    "                \n",
    "                if predictions and predict:\n",
    "                    with open(f\"predictions-{name_extension}.csv\", \"w\", newline=\"\") as out:\n",
    "                        wr = csv.writer(out)\n",
    "                        wr.writerow(['gwb_code_8'] + new_index_columns[3:])\n",
    "                        wr.writerows(predictions)\n",
    "\n",
    "#                 print(\"best model iter {:d}: \"\n",
    "#                       \"train acc={:.4f}, dev acc={:.4f}, test acc={:.4f}\".format(\n",
    "#                           best_iter, train_acc, dev_acc, test_acc))\n",
    "\n",
    "                return test_acc, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createSplits(data, k):\n",
    "    folds = {}\n",
    "    for i in range(k):\n",
    "        dev = data[math.ceil(i*len(data)/k) : math.ceil((i+1)*len(data)/k)]\n",
    "        train = [x for x in data if x not in dev]\n",
    "        folds[i] = train,dev\n",
    "    return folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item embedding shape (history): torch.Size([185, 64])\n",
      "User embedding shape: torch.Size([1, 64])\n",
      "Dense shape: torch.Size([185, 64])\n",
      "Alpha1 shape: torch.Size([1, 185])\n",
      "Profile shape: torch.Size([64])\n",
      "h_0 shape: torch.Size([1, 1, 64])\n",
      "Item embedding shape (current): torch.Size([6, 64])\n",
      "Item embedding shape (current): torch.Size([1, 6, 64])\n",
      "Out_local shape: torch.Size([1, 6, 64])\n",
      "Out_global shape: torch.Size([1, 6, 64])\n",
      "c_global shape: torch.Size([6, 64])\n",
      "c_local shape: torch.Size([6, 64])\n",
      "c shape: torch.Size([6, 6702, 128])\n",
      "All item embeds shape: torch.Size([6, 6702, 64])\n",
      "torch.Size([6, 6702])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "top1() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-ad777c0b06da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-60-d24e3adbbeff>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, num_iterations, print_every, eval_every, batch_fn, prep_fn, eval_fn, batch_size, eval_batch_size, predict)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;31m# compute Huber loss (our criterion)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: top1() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "# item_embedding_dim, hidden_size, output_dim, num_layers, vocab, \n",
    "train_data = trainData\n",
    "\n",
    "model = NarmPlus(64,64,1,1,v)\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "a, p = train_model(model,optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
