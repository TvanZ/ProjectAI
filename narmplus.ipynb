{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NARM+\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing functions.\n",
    "import pandas\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "from collections import namedtuple\n",
    "import itertools\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas\n",
    "import random\n",
    "from sklearn.metrics import explained_variance_score\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>USERID</th>\n",
       "      <th>PRODUCTID</th>\n",
       "      <th>CATEGORYID</th>\n",
       "      <th>ACTION</th>\n",
       "      <th>TIMESTAMP</th>\n",
       "      <th>SESSION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2268318</td>\n",
       "      <td>2520377</td>\n",
       "      <td>pv</td>\n",
       "      <td>1511544070</td>\n",
       "      <td>2017-11-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2333346</td>\n",
       "      <td>2520771</td>\n",
       "      <td>pv</td>\n",
       "      <td>1511561733</td>\n",
       "      <td>2017-11-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2576651</td>\n",
       "      <td>149192</td>\n",
       "      <td>pv</td>\n",
       "      <td>1511572885</td>\n",
       "      <td>2017-11-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3830808</td>\n",
       "      <td>4181361</td>\n",
       "      <td>pv</td>\n",
       "      <td>1511593493</td>\n",
       "      <td>2017-11-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>4365585</td>\n",
       "      <td>2520377</td>\n",
       "      <td>pv</td>\n",
       "      <td>1511596146</td>\n",
       "      <td>2017-11-25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   USERID  PRODUCTID  CATEGORYID ACTION   TIMESTAMP     SESSION\n",
       "0       1    2268318     2520377     pv  1511544070  2017-11-24\n",
       "1       1    2333346     2520771     pv  1511561733  2017-11-24\n",
       "2       1    2576651      149192     pv  1511572885  2017-11-25\n",
       "3       1    3830808     4181361     pv  1511593493  2017-11-25\n",
       "4       1    4365585     2520377     pv  1511596146  2017-11-25"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create sessions\n",
    "sample = pandas.read_pickle(\"./data/smallTaoBao.pkl\")\n",
    "sample['SESSION'] = pandas.to_datetime(unpickled_smallTaoBao['TIMESTAMP'],unit='s').dt.date\n",
    "sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of sessions per user\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7.2"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Average number of sessions per user\")\n",
    "unpickled_smallTaoBao.groupby('USERID')['SESSION'].nunique().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of clicks per session\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14.126388888888888"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Average number of clicks per session\")\n",
    "sample.groupby(['USERID', 'SESSION'])['ACTION'].count().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>USERID</th>\n",
       "      <th>PRODUCTID</th>\n",
       "      <th>CATEGORYID</th>\n",
       "      <th>ACTION</th>\n",
       "      <th>TIMESTAMP</th>\n",
       "      <th>SESSION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2268318</td>\n",
       "      <td>2520377</td>\n",
       "      <td>pv</td>\n",
       "      <td>1511544070</td>\n",
       "      <td>2017-11-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2333346</td>\n",
       "      <td>2520771</td>\n",
       "      <td>pv</td>\n",
       "      <td>1511561733</td>\n",
       "      <td>2017-11-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2576651</td>\n",
       "      <td>149192</td>\n",
       "      <td>pv</td>\n",
       "      <td>1511572885</td>\n",
       "      <td>2017-11-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3830808</td>\n",
       "      <td>4181361</td>\n",
       "      <td>pv</td>\n",
       "      <td>1511593493</td>\n",
       "      <td>2017-11-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>4365585</td>\n",
       "      <td>2520377</td>\n",
       "      <td>pv</td>\n",
       "      <td>1511596146</td>\n",
       "      <td>2017-11-25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   USERID  PRODUCTID  CATEGORYID ACTION   TIMESTAMP     SESSION\n",
       "0       1    2268318     2520377     pv  1511544070  2017-11-24\n",
       "1       1    2333346     2520771     pv  1511561733  2017-11-24\n",
       "2       1    2576651      149192     pv  1511572885  2017-11-25\n",
       "3       1    3830808     4181361     pv  1511593493  2017-11-25\n",
       "4       1    4365585     2520377     pv  1511596146  2017-11-25"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user1 = sample.loc[unpickled_smallTaoBao['USERID'] == 1]\n",
    "user1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2268318, 2333346], [2576651, 3830808, 4365585, 4606018, 230380], [3827899, 3745169, 1531036], [2266567, 2951368, 3108797, 1338525, 2286574], [5002615, 2734026, 5002615, 3239041, 4615417, 4152983, 266784, 46259, 266784, 4092065, 1305059], [2791761, 3239041, 46259, 4973305, 2087357, 3157558], [2087357, 4170517, 1340922, 3911125, 4170517, 3682069, 4954999, 79715, 4666650], [1323189, 4198227, 4954999], [2041056, 3219016, 2104483, 2028434, 3219016, 2278603, 929177], [4954999, 818610, 271696, 568695]]\n"
     ]
    }
   ],
   "source": [
    "# Create nested list of sessions and items per user\n",
    "userBase = sample.groupby(['USERID', 'SESSION'])['PRODUCTID'].apply(list).groupby('USERID').apply(list)\n",
    "print(userBase[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example(userID=1, history=[[2268318, 2333346], [2576651, 3830808, 4365585, 4606018, 230380], [3827899, 3745169, 1531036], [2266567, 2951368, 3108797, 1338525, 2286574], [5002615, 2734026, 5002615, 3239041, 4615417, 4152983, 266784, 46259, 266784, 4092065, 1305059], [2791761, 3239041, 46259, 4973305, 2087357, 3157558], [2087357, 4170517, 1340922, 3911125, 4170517, 3682069, 4954999, 79715, 4666650], [1323189, 4198227, 4954999]], inputs=[2041056, 3219016, 2104483, 2028434, 3219016, 2278603, 929177], target=[3219016, 2104483, 2028434, 3219016, 2278603, 929177])\n",
      "\n",
      "Example(userID=1, history=[[2268318, 2333346], [2576651, 3830808, 4365585, 4606018, 230380], [3827899, 3745169, 1531036], [2266567, 2951368, 3108797, 1338525, 2286574], [5002615, 2734026, 5002615, 3239041, 4615417, 4152983, 266784, 46259, 266784, 4092065, 1305059], [2791761, 3239041, 46259, 4973305, 2087357, 3157558], [2087357, 4170517, 1340922, 3911125, 4170517, 3682069, 4954999, 79715, 4666650], [1323189, 4198227, 4954999], [2041056, 3219016, 2104483, 2028434, 3219016, 2278603, 929177]], inputs=[4954999, 818610, 271696, 568695], target=[818610, 271696, 568695])\n"
     ]
    }
   ],
   "source": [
    "# More efficient create examples function\n",
    "# A simple way to define a class is using namedtuple.\n",
    "Example = namedtuple(\"Example\", [\"userID\", \"history\", \"inputs\", \"target\"])\n",
    "\n",
    "def f(userid, sessions, train):\n",
    "    if train:\n",
    "        object_train = Example(userID = userid, history = sessions[:-2], inputs = sessions[-2], target = sessions[-2][1:])\n",
    "        return object_train\n",
    "    else:\n",
    "        return Example(userID = userid, history = sessions[:-1], inputs = sessions[-1], target = sessions[-1][1:])\n",
    "\n",
    "def createExamples(userBase):\n",
    "    ''' Create training and testing set '''\n",
    "    userBase = pandas.DataFrame(userBase)\n",
    "    userBase.reset_index(level = 0, inplace = True)\n",
    "    trainData = userBase.apply(lambda x: f(x['USERID'], x['PRODUCTID'], True), axis = 1).tolist()\n",
    "    testData = userBase.apply(lambda x: f(x['USERID'], x['PRODUCTID'], False), axis = 1).tolist()\n",
    "    return trainData, testData\n",
    "\n",
    "trainData, testData = createExamples(userBase)\n",
    "print(trainData[0])\n",
    "print('')\n",
    "print(testData[0])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will probably take very long for entire dataset, code above does the same\n",
    "# IDs = list(sample.USERID.unique())\n",
    "# sessions = sorted(list(unpickled_smallTaoBao.SESSION.unique()))\n",
    "# userBase = defaultdict(list)\n",
    "\n",
    "# for ID in IDs:\n",
    "#     userData = sample.loc[unpickled_smallTaoBao['USERID'] == ID]\n",
    "    \n",
    "#     #Verschil tussen historical data and current session.\n",
    "    \n",
    "#     for session in sessions:\n",
    "#         userSession = userData.loc[userData['SESSION'] == session]\n",
    "#         userData_PRODUCTLIST = userSession['PRODUCTID'].values.tolist()\n",
    "        \n",
    "#         if len(userData_PRODUCTLIST) > 0:\n",
    "#             userBase[ID].append(userData_PRODUCTLIST)\n",
    "    \n",
    "    \n",
    "\n",
    "# print(userBase[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example(userID=1, history=[[2268318, 2333346], [2576651, 3830808, 4365585, 4606018, 230380], [3827899, 3745169, 1531036], [2266567, 2951368, 3108797, 1338525, 2286574], [5002615, 2734026, 5002615, 3239041, 4615417, 4152983, 266784, 46259, 266784, 4092065, 1305059], [2791761, 3239041, 46259, 4973305, 2087357, 3157558], [2087357, 4170517, 1340922, 3911125, 4170517, 3682069, 4954999, 79715, 4666650], [1323189, 4198227, 4954999]], inputs=[2041056, 3219016, 2104483, 2028434, 3219016, 2278603], target=[3219016, 2104483, 2028434, 3219016, 2278603, 929177])\n",
      "Example(userID=1, history=[[2268318, 2333346], [2576651, 3830808, 4365585, 4606018, 230380], [3827899, 3745169, 1531036], [2266567, 2951368, 3108797, 1338525, 2286574], [5002615, 2734026, 5002615, 3239041, 4615417, 4152983, 266784, 46259, 266784, 4092065, 1305059], [2791761, 3239041, 46259, 4973305, 2087357, 3157558], [2087357, 4170517, 1340922, 3911125, 4170517, 3682069, 4954999, 79715, 4666650], [1323189, 4198227, 4954999], [2041056, 3219016, 2104483, 2028434, 3219016, 2278603, 929177]], inputs=[4954999, 818610, 271696], target=[818610, 271696, 568695])\n"
     ]
    }
   ],
   "source": [
    "# # DON'T RUN THIS YET, NEEDS REWORKING\n",
    "\n",
    "# # Create dataset of examples\n",
    "\n",
    "# # A simple way to define a class is using namedtuple.\n",
    "# Example = namedtuple(\"Example\", [\"userID\", \"history\", \"inputs\", \"target\"])\n",
    "\n",
    "# def createExamples(userBase, IDs):\n",
    "#     # convert raw data into examples and create dataset\n",
    "    \n",
    "#     #TODO: create train and testdata\n",
    "#     #Example = userID, history, inputs, targets\n",
    "    \n",
    "#     trainData = []\n",
    "#     testData = []\n",
    "    \n",
    "    \n",
    "    \n",
    "#     for ID in IDs:\n",
    "#         userData = userBase[ID]\n",
    "#         historyTrain = userData[:-2]\n",
    "#         inputsTrain = userData[len(userData) -2]\n",
    "#         trainData.append(Example(userID = ID, history = historyTrain, inputs=inputsTrain[:-1],target=inputsTrain[1:]))\n",
    "        \n",
    "#         historyTest = userData[:-1]\n",
    "#         inputsTest = userData[len(userData) - 1]\n",
    "#         testData.append(Example(userID = ID, history = historyTest, inputs=inputsTest[:-1], target=inputsTest[1:]))\n",
    "        \n",
    "#     return trainData, testData\n",
    "\n",
    "# trainData, testData = createExamples(userBase, IDs)\n",
    "# print(trainData[0])\n",
    "# print(testData[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HELPER FUNCTIONS\n",
    "\n",
    "# function to yield one example at a time\n",
    "def get_examples(data, shuffle=True, **kwargs):\n",
    "    \"\"\"Shuffle data set and return 1 example at a time (until nothing left)\"\"\"\n",
    "    if shuffle:\n",
    "#         print(\"Shuffling training data\")\n",
    "        random.shuffle(data)  # shuffle training data each epoch\n",
    "    for example in data:\n",
    "        yield example\n",
    "    \n",
    "# function to prepare an example for usage by the model\n",
    "def prepare_example(example):\n",
    "    \"\"\"\n",
    "    Turn an example into tensors of inputs and target.\n",
    "    \"\"\"\n",
    "    v = torch.FloatTensor([example.userID])\n",
    "    v = v.to(device)\n",
    "    \n",
    "    w = torch.FloatTensor([example.history])\n",
    "    w = w.to(device)\n",
    "    \n",
    "    x = torch.FloatTensor([example.inputs])\n",
    "    x = x.to(device)\n",
    "\n",
    "    y = torch.FloatTensor([example.target])\n",
    "    y = y.to(device)\n",
    "\n",
    "    return v, w, x, y\n",
    "\n",
    "# function to yield a (mini-)batch\n",
    "def get_minibatch(data, batch_size=25, shuffle=True):\n",
    "    \"\"\"Return minibatches, optional shuffling\"\"\" \n",
    "    if shuffle:\n",
    "#         print(\"Shuffling training data\")\n",
    "        random.shuffle(data)  # shuffle training data each epoch\n",
    "\n",
    "    batch = []\n",
    "\n",
    "    # yield minibatches\n",
    "    for example in data:\n",
    "        batch.append(example)\n",
    "\n",
    "        if len(batch) == batch_size:\n",
    "            yield batch\n",
    "            batch = []    \n",
    "        # in case there is something left\n",
    "        if len(batch) > 0:\n",
    "            yield batch\n",
    "\n",
    "# function to make the (mini-)batch ready for usage by the model\n",
    "def prepare_minibatch(mb):\n",
    "    \"\"\"\n",
    "    Minibatch is a list of examples.\n",
    "    This function converts returns\n",
    "    torch tensors to be used as input/targets.\n",
    "    \"\"\"\n",
    "    batch_size = len(mb)\n",
    "    x = [ex.inputs for ex in mb]\n",
    "    x = torch.FloatTensor(x)\n",
    "    x = x.to(device)\n",
    "\n",
    "    y = [ex.target for ex in mb]\n",
    "    y = torch.FloatTensor(y)\n",
    "    y = y.to(device)\n",
    "\n",
    "    return x, y\n",
    "\n",
    "# simple evaluation function\n",
    "def simple_evaluate(model, data, prep_fn=prepare_example, **kwargs):\n",
    "    \"\"\"Explained Variance Score of a model on given data set.\"\"\"\n",
    "    model.eval()  # disable dropout (explained later)\n",
    "    targets = []\n",
    "    predictions = []\n",
    "\n",
    "    for example in data:\n",
    "\n",
    "        # convert the example input and label to PyTorch tensors\n",
    "        targets.append(example.target)\n",
    "        x, target = prepare_example(example)\n",
    "\n",
    "        # forward pass\n",
    "        # get the output from the neural network for input x\n",
    "        with torch.no_grad():\n",
    "            output = model(x)\n",
    "        # output shape: (batch, output_size)\n",
    "        prediction = output[0].tolist()\n",
    "        predictions.append(prediction)\n",
    "            \n",
    "    score = explained_variance_score(targets, predictions, multioutput='variance_weighted')\n",
    "\n",
    "    return score, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-6-76cc306ed08f>, line 93)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-76cc306ed08f>\"\u001b[0;36m, line \u001b[0;32m93\u001b[0m\n\u001b[0;31m    alphas = torch.ones((H.shape[0],H.shape[0],dtype=torch.float64, device=device)\u001b[0m\n\u001b[0m                                                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Custom NN\n",
    "\n",
    "class NarmPlus(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_dim, embedding_dim, hidden_size, output_dim, num_layers, \n",
    "                 num_items, item_vocabulary, num_users, \n",
    "                 activation_fn=nn.RReLU()):\n",
    "        super(NarmPlus, self).__init__()\n",
    "        # Store parameters\n",
    "        self.input_dim = input_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_dim = output_dim\n",
    "        self.num_layers = num_layers\n",
    "        # Shape of hidden_state: (num_layers * num_directions, batch, hidden_size)\n",
    "        self.hidden_state_dim = (num_layers, 1, hidden_size)\n",
    "        self.hidden_state_size = num_layers * hidden_size\n",
    "        self.items = item_vocabulary\n",
    "        \n",
    "        # General part\n",
    "        self.ActivationFn = activation_fn\n",
    "        self.Softmax = nn.Softmax()\n",
    "        self.loss = top1\n",
    "        \n",
    "        # History part\n",
    "        self.UserEmbedding = nn.Embedding(num_users, embedding_dim)\n",
    "        self.ItemEmbedding = nn.Embedding(num_items, embedding_dim)\n",
    "        self.LatentItemHistory = nn.Linear(embedding_dim, self.hidden_state_size)\n",
    "        \n",
    "        # NARM Part\n",
    "        # Input to the GRU is the item embedding: input_size = embedding_size\n",
    "        # Hidden size is something we can experiment with\n",
    "        self.Local = nn.GRU(embedding_size, hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.Global = nn.GRU(embedding_size, hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.Decoder = nn.Bilinear(embedding_dim, hidden_size, output_dim)\n",
    "        \n",
    "        # Inner working of NARM attention part\n",
    "        # Latent space for alpha: what value to pick?\n",
    "        # I assume no bias, based on the paper\n",
    "        latent_space = hidden_size\n",
    "        self.A1 = nn.Linear(hidden_size,latent_space,bias=False)\n",
    "        self.A2 = nn.Linear(hidden_size,latent_space,bias=False)\n",
    "        self.v = nn.Linear(latent_space,1,bias=False)\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        user, history, inputs = x\n",
    "        # user shape (1)\n",
    "        # history shape (history_length)\n",
    "        # inputs shape (seq_length,1)\n",
    "        item_embeds_h = self.ItemEmbedding(history)\n",
    "        # item_embeds_h shape (history_length, embedding_size)\n",
    "        user_embed = self.UserEmbedding(user)\n",
    "        # user_embed shape (1, embedding_size)\n",
    "        dense = self.ActivationFn(self.LatentItemHistory(item_embeds_h))\n",
    "        # dense shape (history_length, hidden_state_size)\n",
    "        alpha1 = self.Softmax(torch.matmul(user_embed,torch.transpose(dense, 0, 1)))\n",
    "        # alpha shape (history_length)\n",
    "        profile = torch.sum(torch.mul(alpha1,torch.transpose(dense,0,1)),1)\n",
    "        # profile shape (hidden_state_size)\n",
    "        # reshape to correct hidden state dimensions\n",
    "        h_0 = torch.reshape(profile,*self.hidden_state_dim)\n",
    "        \n",
    "        # add a batch dimension to the front, necessary for GRU\n",
    "        inputs = inputs[None,:,:] \n",
    "        \n",
    "        out_local, _ = self.Local(inputs,h_0)\n",
    "        out_global, _ = self.Global(inputs,h_0)\n",
    "        # out shape (batch_size, seq_length, hidden_size), containing hidden state output for every step\n",
    "        # Shape of c_global and c_local should be: (sequence, hidden_size)\n",
    "        c_global = out_global.squeeze()\n",
    "        c_local = self.calculate_c_local(out_local)\n",
    "        \n",
    "        # Shape of c_global and c_local should be: (sequence, hidden_size)\n",
    "        c = torch.cat(c_global, c_local, dim=1)\n",
    "        # Decoder takes as inputs: embeddings for each item and c\n",
    "        embeds = self.ItemEmbedding(self.items)\n",
    "        out = self.Decoder(embeds, c)\n",
    "        output = self.Softmax(out)\n",
    "        return output\n",
    "    \n",
    "    def calculate_c_local(self,H):\n",
    "        # H: hidden states returned from the GRU\n",
    "        # H shape (batch, sequence, hidden size)\n",
    "        H = H.squeeze()\n",
    "        # H shape (sequence, hidden size)\n",
    "        s = H.shape\n",
    "        # Initialise c_local with the output hidden states: every hidden state has a similarity of \n",
    "        # 1 with itself, so the entire hidden state is taken into account\n",
    "        c_local = torch.tensor(H)\n",
    "        \n",
    "        \n",
    "        alphas = torch.ones((H.shape[0],H.shape[0]),dtype=torch.float64, device=device)\n",
    "        \n",
    "        for t in range(H.shape[0]):\n",
    "            # If it is the first hidden state, then there are no previous hidden states to calculate the \n",
    "            # alpha from and the current hidden state has already been saved in c_local.\n",
    "            if t == 0:\n",
    "                continue\n",
    "            \n",
    "            # Technically we do not need to store the alphas, but maybe we want to do something with these values\n",
    "            alphas_t = torch.zeroes(H.shape[0], dtype=torch.float64, device=device)\n",
    "            A1 = self.A1(H[t])\n",
    "            for j in range(t):\n",
    "                A2 = self.A2(H[j])\n",
    "                # next three lines could be done in one line\n",
    "                alphas_t[j] = self.v(self.ActivationFn(torch.add(A1,A2)))\n",
    "                ct_j = torch.mul(alphas_t[j],H[j])\n",
    "                c_local[t] = torch.add(c_local[t],ct_j)\n",
    "            alphas[t] = alphas_t\n",
    "        return c_local\n",
    "    \n",
    "    def top1(self, yhat):\n",
    "        ''' Top1 loss, yhat is vector with softmax probabilities '''\n",
    "        # Not sure if you can just call backward to this, but I think it should work. Code from:\n",
    "        # https://github.com/mquad/hgru4rec/blob/master/src/hgru4rec.py\n",
    "        yhatT = torch.transpose(yhat, 0, 1)\n",
    "        loss = torch.mean(torch.mean(nn.sigmoid( - torch.diag(yhat) + yhatT) + nn.sigmoid(yhat ** 2), dim = 0) - nn.sigmoid(T.diag(yhat ** 2)))\n",
    "        return loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to train a model\n",
    "name_extension = ''\n",
    "def train_model(model, optimizer, num_iterations=100000, \n",
    "                print_every=100000, eval_every=100000,\n",
    "                batch_fn=get_examples, \n",
    "                prep_fn=prepare_example,\n",
    "                eval_fn=simple_evaluate,\n",
    "                batch_size=1, eval_batch_size=None,\n",
    "                criterion=nn.SmoothL1Loss(),\n",
    "                predict=False\n",
    "               ):\n",
    "    \"\"\"Train a model.\"\"\"  \n",
    "    iter_i = 0\n",
    "    train_loss = 0.\n",
    "    print_num = 0\n",
    "    start = time.time()\n",
    "    best_eval = 0.\n",
    "    best_iter = 0\n",
    "\n",
    "    # store train loss and validation accuracy during training\n",
    "    # so we can plot them afterwards\n",
    "    losses = []\n",
    "    accuracies = []  \n",
    "\n",
    "    if eval_batch_size is None:\n",
    "        eval_batch_size = batch_size\n",
    "\n",
    "    while True:  # when we run out of examples, shuffle and continue\n",
    "        for batch in batch_fn(train_data, batch_size=batch_size):\n",
    "\n",
    "            # forward pass\n",
    "            model.train()\n",
    "            x, targets = prep_fn(batch)\n",
    "            output = model(x)\n",
    "            # output shape: (batch, outputsize)\n",
    "\n",
    "            # B stands for batch size\n",
    "            B = targets.size(0)  # later we will use B examples per update\n",
    "\n",
    "            # compute Huber loss (our criterion)\n",
    "            loss = criterion(output, targets)\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            # backward pass\n",
    "            # erase previous gradients\n",
    "            model.zero_grad()\n",
    "\n",
    "            # compute gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # update weights - take a small step in the opposite dir of the gradient\n",
    "            optimizer.step()\n",
    "\n",
    "            print_num += 1\n",
    "            iter_i += 1\n",
    "\n",
    "            # print info\n",
    "            if iter_i % print_every == 0:\n",
    "#                 print(\"Iter %r: loss=%.4f, time=%.2fs\" % \n",
    "#                       (iter_i, train_loss, time.time()-start))\n",
    "                losses.append(train_loss)\n",
    "                print_num = 0        \n",
    "                train_loss = 0.\n",
    "\n",
    "            # evaluate\n",
    "            if iter_i % eval_every == 0:\n",
    "                accuracy, _ = eval_fn(model, dev_data, batch_size=eval_batch_size,\n",
    "                                         batch_fn=batch_fn, prep_fn=prep_fn)\n",
    "                accuracies.append(accuracy)\n",
    "#                 print(\"iter %r: dev acc=%.4f\" % (iter_i, accuracy))       \n",
    "\n",
    "                # save best model parameters\n",
    "                if accuracy > best_eval:\n",
    "#                     print(\"new highscore\")\n",
    "                    best_eval = accuracy\n",
    "                    best_iter = iter_i\n",
    "                    path = \"{}{}.pt\".format(model.__class__.__name__,name_extension)\n",
    "                    ckpt = {\n",
    "                      \"state_dict\": model.state_dict(),\n",
    "                      \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                      \"best_eval\": best_eval,\n",
    "                      \"best_iter\": best_iter\n",
    "                    }\n",
    "                    torch.save(ckpt, path)\n",
    "\n",
    "            # done training\n",
    "            if iter_i == num_iterations:\n",
    "#                 print(\"Done training\")\n",
    "\n",
    "                # evaluate on train, dev, and test with best model\n",
    "#                 print(\"Loading best model\")\n",
    "                path = \"{}{}.pt\".format(model.__class__.__name__,name_extension)        \n",
    "                ckpt = torch.load(path)\n",
    "                model.load_state_dict(ckpt[\"state_dict\"])\n",
    "\n",
    "                train_acc, _ = eval_fn(\n",
    "                    model, train_data, batch_size=eval_batch_size, \n",
    "                    batch_fn=batch_fn, prep_fn=prep_fn)\n",
    "                dev_acc, _ = eval_fn(\n",
    "                    model, dev_data, batch_size=eval_batch_size,\n",
    "                    batch_fn=batch_fn, prep_fn=prep_fn)\n",
    "                test_acc, predictions = eval_fn(\n",
    "                    model, test_data, batch_size=eval_batch_size, \n",
    "                    batch_fn=batch_fn, prep_fn=prep_fn)\n",
    "                \n",
    "                if predictions and predict:\n",
    "                    with open(f\"predictions-{name_extension}.csv\", \"w\", newline=\"\") as out:\n",
    "                        wr = csv.writer(out)\n",
    "                        wr.writerow(['gwb_code_8'] + new_index_columns[3:])\n",
    "                        wr.writerows(predictions)\n",
    "\n",
    "#                 print(\"best model iter {:d}: \"\n",
    "#                       \"train acc={:.4f}, dev acc={:.4f}, test acc={:.4f}\".format(\n",
    "#                           best_iter, train_acc, dev_acc, test_acc))\n",
    "\n",
    "                return test_acc, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createSplits(data, k):\n",
    "    folds = {}\n",
    "    for i in range(k):\n",
    "        dev = data[math.ceil(i*len(data)/k) : math.ceil((i+1)*len(data)/k)]\n",
    "        train = [x for x in data if x not in dev]\n",
    "        folds[i] = train,dev\n",
    "    return folds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
