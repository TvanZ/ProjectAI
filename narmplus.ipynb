{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NARM+\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing functions.\n",
    "import pandas\n",
    "import csv\n",
    "from collections import Counter, OrderedDict, defaultdict, namedtuple\n",
    "import itertools\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# PyTorch can run on CPU or on Nvidia GPU (video card) using CUDA\n",
    "# This cell selects the GPU if one is available.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.synchronize() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>USERID</th>\n",
       "      <th>PRODUCTID</th>\n",
       "      <th>CATEGORYID</th>\n",
       "      <th>ACTION</th>\n",
       "      <th>TIMESTAMP</th>\n",
       "      <th>SESSION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2268318</td>\n",
       "      <td>2520377</td>\n",
       "      <td>pv</td>\n",
       "      <td>1511544070</td>\n",
       "      <td>2017-11-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2333346</td>\n",
       "      <td>2520771</td>\n",
       "      <td>pv</td>\n",
       "      <td>1511561733</td>\n",
       "      <td>2017-11-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2576651</td>\n",
       "      <td>149192</td>\n",
       "      <td>pv</td>\n",
       "      <td>1511572885</td>\n",
       "      <td>2017-11-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3830808</td>\n",
       "      <td>4181361</td>\n",
       "      <td>pv</td>\n",
       "      <td>1511593493</td>\n",
       "      <td>2017-11-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>4365585</td>\n",
       "      <td>2520377</td>\n",
       "      <td>pv</td>\n",
       "      <td>1511596146</td>\n",
       "      <td>2017-11-25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   USERID  PRODUCTID  CATEGORYID ACTION   TIMESTAMP     SESSION\n",
       "0       1    2268318     2520377     pv  1511544070  2017-11-24\n",
       "1       1    2333346     2520771     pv  1511561733  2017-11-24\n",
       "2       1    2576651      149192     pv  1511572885  2017-11-25\n",
       "3       1    3830808     4181361     pv  1511593493  2017-11-25\n",
       "4       1    4365585     2520377     pv  1511596146  2017-11-25"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create sessions\n",
    "sample = pandas.read_pickle(\"./data/smallTaoBao.pkl\")\n",
    "sample['SESSION'] = pandas.to_datetime(sample['TIMESTAMP'],unit='s').dt.date\n",
    "sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of sessions per user\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7.2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Average number of sessions per user\")\n",
    "sample.groupby('USERID')['SESSION'].nunique().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of clicks per session\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14.126388888888888"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Average number of clicks per session\")\n",
    "sample.groupby(['USERID', 'SESSION'])['ACTION'].count().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>USERID</th>\n",
       "      <th>PRODUCTID</th>\n",
       "      <th>CATEGORYID</th>\n",
       "      <th>ACTION</th>\n",
       "      <th>TIMESTAMP</th>\n",
       "      <th>SESSION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2268318</td>\n",
       "      <td>2520377</td>\n",
       "      <td>pv</td>\n",
       "      <td>1511544070</td>\n",
       "      <td>2017-11-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2333346</td>\n",
       "      <td>2520771</td>\n",
       "      <td>pv</td>\n",
       "      <td>1511561733</td>\n",
       "      <td>2017-11-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2576651</td>\n",
       "      <td>149192</td>\n",
       "      <td>pv</td>\n",
       "      <td>1511572885</td>\n",
       "      <td>2017-11-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3830808</td>\n",
       "      <td>4181361</td>\n",
       "      <td>pv</td>\n",
       "      <td>1511593493</td>\n",
       "      <td>2017-11-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>4365585</td>\n",
       "      <td>2520377</td>\n",
       "      <td>pv</td>\n",
       "      <td>1511596146</td>\n",
       "      <td>2017-11-25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   USERID  PRODUCTID  CATEGORYID ACTION   TIMESTAMP     SESSION\n",
       "0       1    2268318     2520377     pv  1511544070  2017-11-24\n",
       "1       1    2333346     2520771     pv  1511561733  2017-11-24\n",
       "2       1    2576651      149192     pv  1511572885  2017-11-25\n",
       "3       1    3830808     4181361     pv  1511593493  2017-11-25\n",
       "4       1    4365585     2520377     pv  1511596146  2017-11-25"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user1 = sample.loc[sample['USERID'] == 1]\n",
    "user1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2268318, 2333346], [2576651, 3830808, 4365585, 4606018, 230380], [3827899, 3745169, 1531036], [2266567, 2951368, 3108797, 1338525, 2286574], [5002615, 2734026, 5002615, 3239041, 4615417, 4152983, 266784, 46259, 266784, 4092065, 1305059], [2791761, 3239041, 46259, 4973305, 2087357, 3157558], [2087357, 4170517, 1340922, 3911125, 4170517, 3682069, 4954999, 79715, 4666650], [1323189, 4198227, 4954999], [2041056, 3219016, 2104483, 2028434, 3219016, 2278603, 929177], [4954999, 818610, 271696, 568695]]\n"
     ]
    }
   ],
   "source": [
    "# Create nested list of sessions and items per user\n",
    "userBase = sample.groupby(['USERID', 'SESSION'])['PRODUCTID'].apply(list).groupby('USERID').apply(list)\n",
    "print(userBase[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not enough sessions\n",
      "Not enough sessions\n",
      "Not enough sessions\n",
      "Not enough sessions\n",
      "Example(userID=1, history=[2268318, 2333346, 2576651, 3830808, 4365585, 4606018, 230380, 3827899, 3745169, 1531036, 2266567, 2951368, 3108797, 1338525, 2286574, 5002615, 2734026, 5002615, 3239041, 4615417, 4152983, 266784, 46259, 266784, 4092065, 1305059, 2791761, 3239041, 46259, 4973305, 2087357, 3157558, 2087357, 4170517, 1340922, 3911125, 4170517, 3682069, 4954999, 79715, 4666650, 1323189, 4198227, 4954999], inputs=[2041056, 3219016, 2104483, 2028434, 3219016, 2278603], target=[3219016, 2104483, 2028434, 3219016, 2278603, 929177])\n",
      "\n",
      "Example(userID=1, history=[2268318, 2333346, 2576651, 3830808, 4365585, 4606018, 230380, 3827899, 3745169, 1531036, 2266567, 2951368, 3108797, 1338525, 2286574, 5002615, 2734026, 5002615, 3239041, 4615417, 4152983, 266784, 46259, 266784, 4092065, 1305059, 2791761, 3239041, 46259, 4973305, 2087357, 3157558, 2087357, 4170517, 1340922, 3911125, 4170517, 3682069, 4954999, 79715, 4666650, 1323189, 4198227, 4954999, 2041056, 3219016, 2104483, 2028434, 3219016, 2278603, 929177], inputs=[4954999, 818610, 271696], target=[818610, 271696, 568695])\n"
     ]
    }
   ],
   "source": [
    "# More efficient create examples function\n",
    "# A simple way to define a class is using namedtuple.\n",
    "Example = namedtuple(\"Example\", [\"userID\", \"history\", \"inputs\", \"target\"])\n",
    "\n",
    "def f(userid, sessions, train):\n",
    "    non_empty_sessions = [ses for ses in sessions if len(ses) >= 2]\n",
    "    if len(non_empty_sessions) < 3:\n",
    "        print('Not enough sessions')\n",
    "        return\n",
    "    if train:\n",
    "        object_train = Example(userID = userid, history = \n",
    "                               [item for sublist in non_empty_sessions[:-2] for item in sublist], \n",
    "                               inputs = non_empty_sessions[-2][:-1], target = non_empty_sessions[-2][1:])\n",
    "        return object_train\n",
    "    else:\n",
    "        return Example(userID = userid, history = \n",
    "                       [item for sublist in non_empty_sessions[:-1] for item in sublist], \n",
    "                       inputs = non_empty_sessions[-1][:-1], \n",
    "                       target = non_empty_sessions[-1][1:])\n",
    "\n",
    "def createExamples(userBase):\n",
    "    ''' Create training and testing set '''\n",
    "    userBase = pandas.DataFrame(userBase)\n",
    "    userBase.reset_index(level = 0, inplace = True)\n",
    "    trainData = [x for x in \n",
    "                 userBase.apply(lambda x: f(x['USERID'], x['PRODUCTID'], True), axis = 1).tolist() \n",
    "                 if x is not None]\n",
    "    testData = [x for x in \n",
    "                userBase.apply(lambda x: f(x['USERID'], x['PRODUCTID'], False), axis = 1).tolist() \n",
    "                if x is not None]\n",
    "    return trainData, testData\n",
    "\n",
    "trainData, testData = createExamples(userBase)\n",
    "print(trainData[0])\n",
    "print('')\n",
    "print(testData[0])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we first define a class that can map a product to an ID (p2i)\n",
    "# and back (i2p).\n",
    "\n",
    "class OrderedCounter(Counter, OrderedDict):\n",
    "    \"\"\"Counter that remembers the order elements are first seen\"\"\"\n",
    "    def __repr__(self):\n",
    "        return '%s(%r)' % (self.__class__.__name__,\n",
    "                      OrderedDict(self))\n",
    "    def __reduce__(self):\n",
    "        return self.__class__, (OrderedDict(self),)\n",
    "\n",
    "\n",
    "class Vocabulary:\n",
    "    \"\"\"A vocabulary, assigns IDs to tokens\"\"\"\n",
    "    def __init__(self):\n",
    "        self.freqs = OrderedCounter()\n",
    "        self.users = []\n",
    "        self.u2i = {}\n",
    "        self.i2u = []\n",
    "        self.p2i = {}\n",
    "        self.i2p = []\n",
    "\n",
    "    def count_product(self, t):\n",
    "        self.freqs[t] += 1\n",
    "    \n",
    "    def count_user(self, t):\n",
    "        self.users.append(t)\n",
    "\n",
    "    def add_product(self, t):\n",
    "        self.p2i[t] = len(self.p2i)\n",
    "        self.i2p.append(t) \n",
    "        \n",
    "    def add_user(self, t):\n",
    "        self.u2i[t] = len(self.u2i)\n",
    "        self.i2u.append(t)\n",
    "\n",
    "    def build(self, min_freq=0):\n",
    "        self.add_product(\"<unk>\")  # reserve 0 for <unk> (unknown products (products only occuring in test set))\n",
    "        self.add_user(\"<unk>\")\n",
    "        tok_freq = list(self.freqs.items())\n",
    "        tok_freq.sort(key=lambda x: x[1], reverse=True)\n",
    "        for tok, freq in tok_freq:\n",
    "            if freq >= min_freq:\n",
    "                self.add_product(tok)\n",
    "        for user in self.users:\n",
    "            self.add_user(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 7496\n"
     ]
    }
   ],
   "source": [
    "# This process should be deterministic and should have the same result \n",
    "# if run multiple times on the same data set.\n",
    "\n",
    "def build_voc(data_sets):\n",
    "    v = Vocabulary()\n",
    "    for data_set in data_sets:\n",
    "        for ex in data_set:\n",
    "            for product in ex.history + ex.inputs + ex.target:\n",
    "                v.count_product(product)\n",
    "            v.count_user(ex.userID)\n",
    "        v.build()\n",
    "    return v\n",
    "\n",
    "v = build_voc([testData])\n",
    "print(\"Vocabulary size:\", len(v.p2i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HELPER FUNCTIONS\n",
    "\n",
    "# function to yield one example at a time\n",
    "def get_examples(data, shuffle=True, **kwargs):\n",
    "    \"\"\"Shuffle data set and return 1 example at a time (until nothing left)\"\"\"\n",
    "    if shuffle:\n",
    "#         print(\"Shuffling training data\")\n",
    "        random.shuffle(data)  # shuffle training data each epoch\n",
    "    for example in data:\n",
    "        yield example\n",
    "    \n",
    "# function to prepare an example for usage by the model\n",
    "def prepare_example(example, vocab):\n",
    "    \"\"\"\n",
    "    Turn an example into tensors of inputs and target.\n",
    "    \"\"\"\n",
    "    u = vocab.u2i.get(example.userID,0)\n",
    "    v = torch.LongTensor([u])\n",
    "    v = v.to(device)\n",
    "    \n",
    "    w = torch.LongTensor([vocab.p2i.get(t, 0) for t in example.history])\n",
    "    w = w.to(device)\n",
    "    \n",
    "    x = torch.LongTensor([vocab.p2i.get(t, 0) for t in example.inputs])[:,None]\n",
    "    x = x.to(device)\n",
    "\n",
    "    y = torch.LongTensor([vocab.p2i.get(t, 0) for t in example.target])[:,None]\n",
    "    y = y.to(device)\n",
    "\n",
    "    return (v, w, x), y\n",
    "\n",
    "# simple evaluation function\n",
    "def simple_evaluate(model, data, prep_fn=prepare_example, **kwargs):\n",
    "    \"\"\"Precision of a model on given data set.\"\"\"\n",
    "    model.eval()  # disable dropout\n",
    "    targets = []\n",
    "    predictions = []\n",
    "    \n",
    "    vocab = model.vocab\n",
    "    for example in data:\n",
    "        # convert the example input and targets to PyTorch tensors\n",
    "        targets.extend([vocab.p2i.get(t,0) for t in example.target])\n",
    "        x, target = prepare_example(example, vocab)\n",
    "\n",
    "        # forward pass\n",
    "        # get the output from the neural network for input x\n",
    "        with torch.no_grad():\n",
    "            output, alphas = model(x)\n",
    "        # output shape: (sequence length, score for each item)\n",
    "        prediction = torch.argmax(output, dim=1).tolist()\n",
    "        predictions.extend(prediction)\n",
    "            \n",
    "    precision = sum([1 if p==t else 0 for p,t in zip(predictions,targets)])/len(targets)\n",
    "\n",
    "    return precision, None\n",
    "\n",
    "def recall(model, data, prep_fn=prepare_example, at=5, **kwargs):\n",
    "    model.eval() # disable dropout\n",
    "    targets = []\n",
    "    predictions = []\n",
    "    \n",
    "    vocab = model.vocab\n",
    "    for example in data:\n",
    "        # convert the example input and targets to PyTorch tensors\n",
    "        targets.extend([vocab.p2i.get(t,0) for t in example.target])\n",
    "        x, target = prepare_example(example, vocab)\n",
    "\n",
    "        # forward pass\n",
    "        # get the output from the neural network for input x\n",
    "        with torch.no_grad():\n",
    "            output, alphas = model(x)\n",
    "        # output shape: (sequence length, score for each item)\n",
    "        prediction = torch.argsort(output, dim=1, descending=True)[:,:at].tolist()\n",
    "        predictions.extend(prediction)\n",
    "        \n",
    "    recall = sum([1 if t in p else 0 for t,p in zip(targets,predictions)])/len(targets)\n",
    "    \n",
    "    return recall, None\n",
    "\n",
    "def mrr(model, data, prep_fn=prepare_example, at=5, **kwargs):\n",
    "    model.eval() # disable dropout\n",
    "    targets = []\n",
    "    predictions = []\n",
    "    \n",
    "    vocab = model.vocab\n",
    "    for example in data:\n",
    "        # convert the example input and targets to PyTorch tensors\n",
    "        targets.extend([vocab.p2i.get(t,0) for t in example.target])\n",
    "        x, target = prepare_example(example, vocab)\n",
    "\n",
    "        # forward pass\n",
    "        # get the output from the neural network for input x\n",
    "        with torch.no_grad():\n",
    "            output, alphas = model(x)\n",
    "        # output shape: (sequence length, nr of products)\n",
    "        prediction = torch.argsort(output, dim=1, descending=True)[:,:at].tolist()\n",
    "        predictions.extend(prediction)\n",
    "        \n",
    "    mrr = sum([1/(p.index(t) + 1) if (t != 0 and t in p) else 0 for t,p in zip(targets,predictions)])/len(targets)\n",
    "    \n",
    "    return mrr, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom NN\n",
    "\n",
    "class NarmPlus(nn.Module):\n",
    "    def __init__(self, \n",
    "                 item_embedding_dim, user_embedding_dim, hidden_size, output_dim, num_layers, \n",
    "                 vocab, \n",
    "                 activation_fn=nn.RReLU(), dropout=0.2):\n",
    "        super(NarmPlus, self).__init__()\n",
    "        # Store parameters\n",
    "        self.item_embedding_dim = item_embedding_dim\n",
    "        self.user_embedding_dim = user_embedding_dim\n",
    "        self.hidden_size = hidden_size # hidden size is also user embedding dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_layers = num_layers\n",
    "        # Shape of hidden_state: (num_layers * num_directions, batch, hidden_size)\n",
    "        self.hidden_state_dim = (num_layers, 1, hidden_size)\n",
    "        self.hidden_state_size = num_layers * hidden_size\n",
    "        self.vocab = vocab\n",
    "        num_users = len(vocab.u2i)\n",
    "        num_items = len(vocab.p2i)\n",
    "        \n",
    "        # General part\n",
    "        self.ActivationFn = activation_fn\n",
    "        self.Softmax = nn.Softmax(dim=1)\n",
    "        self.loss = self.top1loss\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # History part\n",
    "        self.UserEmbedding = nn.Embedding(num_users, user_embedding_dim)\n",
    "        self.ItemEmbedding = nn.Embedding(num_items, item_embedding_dim)\n",
    "        self.LatentItemHistory = nn.Linear(item_embedding_dim, user_embedding_dim)\n",
    "        self.ProfileToHidden = nn.Linear(user_embedding_dim, self.hidden_state_size)\n",
    "        \n",
    "        # NARM Part\n",
    "        # Input to the GRU is the item embedding: input_size = embedding_size\n",
    "        # Hidden size is something we can experiment with\n",
    "        self.Local = nn.GRU(item_embedding_dim, hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.Global = nn.GRU(item_embedding_dim, hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.Decoder = nn.Bilinear(item_embedding_dim, 2*hidden_size, output_dim)\n",
    "        \n",
    "        # Inner working of NARM attention part\n",
    "        # Latent space for alpha: what value to pick?\n",
    "        # I assume no bias, based on the paper\n",
    "        latent_space = hidden_size\n",
    "        self.A1 = nn.Linear(hidden_size,latent_space,bias=False)\n",
    "        self.A2 = nn.Linear(hidden_size,latent_space,bias=False)\n",
    "        self.v = nn.Linear(latent_space,1,bias=False)\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        user, history, inputs = x\n",
    "        # user shape (1)\n",
    "        # history shape (history_length)\n",
    "        # inputs shape (seq_length,1)\n",
    "        item_embeds_h = self.dropout(self.ItemEmbedding(history))\n",
    "#         print(f'Item embedding shape (history): {item_embeds_h.shape}')\n",
    "        # item_embeds_h shape (history_length, embedding_size)\n",
    "        user_embed = self.dropout(self.UserEmbedding(user))\n",
    "#         print(f'User embedding shape: {user_embed.shape}')\n",
    "        # user_embed shape (1, embedding_size) <- shouldn't this be (1, hidden_state_size)?\n",
    "        dense = self.LatentItemHistory(item_embeds_h)\n",
    "        dense = self.ActivationFn(dense)\n",
    "#         print(f'Dense shape: {dense.shape}')\n",
    "        # dense shape (history_length, hidden_state_size)\n",
    "        alpha1 = self.Softmax(torch.matmul(user_embed,torch.transpose(dense, 0, 1)))\n",
    "#         print(f'Alpha1 shape: {alpha1.shape}')\n",
    "        # alpha shape (history_length)\n",
    "        profile = torch.sum(torch.mul(alpha1,torch.transpose(dense,0,1)),1)\n",
    "#         print(f'Profile shape: {profile.shape}')\n",
    "        # profile shape (hidden_state_size)\n",
    "        # reshape to correct hidden state dimensions\n",
    "        h_0 = torch.reshape(self.ProfileToHidden(profile),self.hidden_state_dim)\n",
    "#         print(f'h_0 shape: {h_0.shape}')\n",
    "        \n",
    "        # Embed the items of the current session\n",
    "        item_embeds_c = self.dropout(torch.transpose(self.ItemEmbedding(inputs),0,1))\n",
    "#         print(f'Item embedding shape (current): {item_embeds_c.shape}')\n",
    "        # shape (seq_length, embedding_size)\n",
    "        # add a batch dimension to the front, necessary for GRU\n",
    "#         item_embeds_c = item_embeds_c[None,:,:] \n",
    "#         print(f'Item embedding shape (current): {item_embeds_c.shape}')\n",
    "        \n",
    "        out_local, _ = self.Local(item_embeds_c,h_0)\n",
    "#         print(f'Out_local shape: {out_local.shape}')\n",
    "        out_global, _ = self.Global(item_embeds_c,h_0)\n",
    "#         print(f'Out_global shape: {out_global.shape}')\n",
    "        # out shape (batch_size, seq_length, hidden_size), containing hidden state output for every step\n",
    "        # Shape of c_global and c_local should be: (sequence, hidden_size)\n",
    "        c_global = out_global[0,:,:]\n",
    "#         print(f'c_global shape: {c_global.shape}')\n",
    "        # c_global shape (seq_length, hidden_size)\n",
    "        c_local, alphas = self.calculate_c_local(out_local)\n",
    "#         print(f'c_local shape: {c_local.shape}')\n",
    "        \n",
    "        # Shape of c_global and c_local should be: (sequence, hidden_size)\n",
    "        c = self.dropout(torch.cat((c_global, c_local), dim=1))\n",
    "        # Decoder takes as inputs: embeddings for each item and c\n",
    "        embeds = self.ItemEmbedding(torch.LongTensor([i for i in range(len(self.vocab.p2i))]).to(device))\n",
    "        # Make embeds and c the same shape\n",
    "        embeds = embeds.repeat(c.shape[0],1,1).contiguous()\n",
    "        c = c[:,None,:].repeat(1,embeds.shape[1],1).contiguous()\n",
    "#         print(f'c shape: {c.shape}')\n",
    "#         print(f'All item embeds shape: {embeds.shape}')\n",
    "        out = self.Decoder(embeds, c)[:,:,0]\n",
    "#         print(f'Output shape: {out.shape}')\n",
    "        output = self.Softmax(out)\n",
    "        return output, alphas\n",
    "    \n",
    "    def calculate_c_local(self,H):\n",
    "        # H: hidden states returned from the GRU\n",
    "        # H shape (batch, sequence, hidden size)\n",
    "        H = H[0,:,:]\n",
    "        # H shape (sequence, hidden size)\n",
    "        # Initialise c_local with the output hidden states: every hidden state has a similarity of \n",
    "        # 1 with itself, so the entire hidden state is taken into account\n",
    "        c_local_base = H.clone().detach().requires_grad_(True)\n",
    "        \n",
    "  \n",
    "        alphas = torch.ones((H.shape[0],H.shape[0]),dtype=torch.float32).to(device)\n",
    "        cs = [torch.zeros((1,H.shape[1]), dtype=torch.float32).to(device)]\n",
    "        for t in range(H.shape[0]):\n",
    "            # If it is the first hidden state, then there are no previous hidden states to calculate the \n",
    "            # alpha from and the current hidden state has already been saved in c_local.\n",
    "            if t == 0:\n",
    "                continue\n",
    "            \n",
    "            # Technically we do not need to store the alphas, but maybe we want to do something with these values\n",
    "            alphas_t = torch.zeros(H.shape[0], dtype=torch.float32).to(device)\n",
    "            A1 = self.A1(H[t])\n",
    "            for j in range(t):\n",
    "                A2 = self.A2(H[j])\n",
    "                # next three lines could be done in one line\n",
    "                alphas_t[j] = self.v(self.ActivationFn(A1 + A2))\n",
    "#                 ct_j = torch.mul(alphas_t[j],H[j])\n",
    "#                 c_local[t] = torch.add(c_local[t],ct_j)\n",
    "            ct = torch.sum(alphas_t[:t].unsqueeze(1) * H[:t,:],dim=0).unsqueeze(0)\n",
    "            cs.append(ct)\n",
    "            alphas[t] = alphas_t\n",
    "        c_local = torch.cat(cs,dim=0) + c_local_base\n",
    "        return c_local, alphas\n",
    "    \n",
    "    def top1(self, yhat):\n",
    "        ''' Top1 loss, yhat is vector with softmax probabilities '''\n",
    "        # Not sure if you can just call backward to this, but I think it should work. Code from:\n",
    "        # https://github.com/mquad/hgru4rec/blob/master/src/hgru4rec.py\n",
    "        yhatT = torch.transpose(yhat, 0, 1)\n",
    "        loss = torch.mean(torch.mean(nn.sigmoid( - torch.diag(yhat) + yhatT) + nn.sigmoid(yhat ** 2), dim = 0) \n",
    "                          - nn.sigmoid(T.diag(yhat ** 2)))\n",
    "        return loss\n",
    "    \n",
    "    def top1loss(self, output, targets):\n",
    "        scores_for_targets = torch.gather(output, 1, targets)\n",
    "        loss = torch.mean(torch.sigmoid(output - scores_for_targets) +\n",
    "                torch.sigmoid(output**2))\n",
    "        return loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to train a model\n",
    "name_extension = ''\n",
    "def train_model(model, optimizer, num_epochs=200, \n",
    "                print_every=5, eval_every=5,\n",
    "                batch_fn=get_examples, \n",
    "                prep_fn=prepare_example,\n",
    "                eval_fn=simple_evaluate,\n",
    "                batch_size=5, eval_batch_size=None,\n",
    "                predict=False\n",
    "               ):\n",
    "    \"\"\"Train a model.\"\"\"  \n",
    "    train_loss = 0.\n",
    "    start = time.time()\n",
    "    best_eval = 0.\n",
    "    best_iter = 0\n",
    "    criterion=model.loss\n",
    "\n",
    "    # store train loss and validation accuracy during training\n",
    "    # so we can plot them afterwards\n",
    "    losses = []\n",
    "    accuracies = []  \n",
    "\n",
    "    if eval_batch_size is None:\n",
    "        eval_batch_size = batch_size\n",
    "    \n",
    "    vocab = model.vocab\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        batch_iter = 0\n",
    "        batch_out = []\n",
    "        batch_targets = []\n",
    "        for example in batch_fn(train_data): # goes through the entire training data once, a.k.a. an epoch\n",
    "\n",
    "            # forward pass, make sure the model is in train modus\n",
    "            model.train()\n",
    "            x, targets = prep_fn(example, vocab)\n",
    "            output, alphas = model(x)\n",
    "            # output shape (sequence length, nr of products): a score for each product at each time step\n",
    "            # alphas are the alphas used in the Narm part\n",
    "\n",
    "            if len(batch_out) == 0:\n",
    "                batch_out = output\n",
    "                batch_targets = targets\n",
    "            else:\n",
    "                batch_out = torch.cat((batch_out, output), dim=0)\n",
    "                batch_targets = torch.cat((batch_targets, targets), dim=0)\n",
    "                \n",
    "            batch_iter += 1\n",
    "                \n",
    "            if batch_iter == batch_size:\n",
    "                # enough users put through the network, time to calculate the loss and update the network\n",
    "                \n",
    "                # calculate loss\n",
    "                # important: len(batch_out) is not always the same\n",
    "                # it does always contain data on the same number of users\n",
    "                # batch_size, in other words, is how many users to look at before updating\n",
    "                loss = criterion(batch_out, batch_targets)\n",
    "                train_loss += loss.item()\n",
    "                \n",
    "                # backward pass\n",
    "                # erase previous gradients\n",
    "                model.zero_grad()\n",
    "\n",
    "                # compute gradients\n",
    "                loss.backward()\n",
    "\n",
    "                # update weights - take a small step in the opposite dir of the gradient\n",
    "                optimizer.step()\n",
    "                \n",
    "                # reset the iterator, batch_out and batch_targets\n",
    "                batch_iter = 0\n",
    "                batch_out = []\n",
    "                batch_targets = []\n",
    "                \n",
    "        if (epoch + 1) % print_every == 0:\n",
    "            print(\"Iter %r: loss=%.4f, time=%.2fs\" % \n",
    "                 (epoch + 1, train_loss, time.time()-start))\n",
    "            losses.append(train_loss)       \n",
    "            train_loss = 0.\n",
    "            \n",
    "        if (epoch + 1) % eval_every == 0:\n",
    "            accuracy, _ = eval_fn(model, dev_data, batch_size=eval_batch_size,\n",
    "                                         batch_fn=batch_fn, prep_fn=prep_fn)\n",
    "            accuracies.append(accuracy)\n",
    "            print(\"iter %r: dev acc=%.4f\" % (epoch + 1, accuracy))       \n",
    "\n",
    "            # save best model parameters\n",
    "            if accuracy > best_eval:\n",
    "                print(\"new highscore\")\n",
    "                best_eval = accuracy\n",
    "                best_iter = epoch + 1\n",
    "                path = \"{}{}.pt\".format(model.__class__.__name__,name_extension)\n",
    "                ckpt = {\n",
    "                  \"state_dict\": model.state_dict(),\n",
    "                  \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                  \"best_eval\": best_eval,\n",
    "                  \"best_iter\": best_iter\n",
    "                }\n",
    "                torch.save(ckpt, path)\n",
    "    \n",
    "    # Done training\n",
    "    # evaluate on train, dev, and test with best model\n",
    "    print(\"Loading best model\")\n",
    "    path = \"{}{}.pt\".format(model.__class__.__name__,name_extension)        \n",
    "    ckpt = torch.load(path)\n",
    "    model.load_state_dict(ckpt[\"state_dict\"])\n",
    "\n",
    "    train_acc, _ = eval_fn(\n",
    "        model, train_data, batch_size=eval_batch_size, \n",
    "        batch_fn=batch_fn, prep_fn=prep_fn)\n",
    "    dev_acc, _ = eval_fn(\n",
    "        model, dev_data, batch_size=eval_batch_size,\n",
    "        batch_fn=batch_fn, prep_fn=prep_fn)\n",
    "    test_acc, predictions = eval_fn(\n",
    "        model, test_data, batch_size=eval_batch_size, \n",
    "        batch_fn=batch_fn, prep_fn=prep_fn)\n",
    "\n",
    "    print(\"best model iter {:d}: \"\n",
    "          \"train acc={:.4f}, dev acc={:.4f}, test acc={:.4f}\".format(\n",
    "              best_iter, train_acc, dev_acc, test_acc))\n",
    "\n",
    "    return test_acc, predictions\n",
    "            \n",
    "\n",
    "#     while True:  # when we run out of examples, shuffle and continue\n",
    "#         for batch in batch_fn(train_data, batch_size=batch_size):\n",
    "\n",
    "#             # forward pass\n",
    "#             vocab = model.vocab\n",
    "#             model.train()\n",
    "#             x, targets = prep_fn(batch, vocab)\n",
    "#             print(f'Targets shape: {targets.shape}')\n",
    "#             output, alphas = model(x)\n",
    "#             # output shape: (batch, outputsize)\n",
    "\n",
    "#             # B stands for batch size\n",
    "#             B = targets.size(0)  # later we will use B examples per update\n",
    "\n",
    "#             # compute top1 loss (our criterion)\n",
    "#             loss = criterion(output, targets)\n",
    "#             train_loss += loss.item()\n",
    "\n",
    "#             # backward pass\n",
    "#             # erase previous gradients\n",
    "#             model.zero_grad()\n",
    "\n",
    "#             # compute gradients\n",
    "#             loss.backward()\n",
    "\n",
    "#             # update weights - take a small step in the opposite dir of the gradient\n",
    "#             optimizer.step()\n",
    "\n",
    "#             print_num += 1\n",
    "#             iter_epoch += 1\n",
    "\n",
    "#             # print info\n",
    "#             if iter_epoch % print_every == 0:\n",
    "# #                 print(\"Iter %r: loss=%.4f, time=%.2fs\" % \n",
    "# #                       (iter_epoch, train_loss, time.time()-start))\n",
    "#                 losses.append(train_loss)\n",
    "#                 print_num = 0        \n",
    "#                 train_loss = 0.\n",
    "\n",
    "#             # evaluate\n",
    "#             if iter_epoch % eval_every == 0:\n",
    "#                 accuracy, _ = eval_fn(model, dev_data, batch_size=eval_batch_size,\n",
    "#                                          batch_fn=batch_fn, prep_fn=prep_fn)\n",
    "#                 accuracies.append(accuracy)\n",
    "# #                 print(\"iter %r: dev acc=%.4f\" % (iter_epoch, accuracy))       \n",
    "\n",
    "#                 # save best model parameters\n",
    "#                 if accuracy > best_eval:\n",
    "# #                     print(\"new highscore\")\n",
    "#                     best_eval = accuracy\n",
    "#                     best_iter = iter_epoch\n",
    "#                     path = \"{}{}.pt\".format(model.__class__.__name__,name_extension)\n",
    "#                     ckpt = {\n",
    "#                       \"state_dict\": model.state_dict(),\n",
    "#                       \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "#                       \"best_eval\": best_eval,\n",
    "#                       \"best_iter\": best_iter\n",
    "#                     }\n",
    "#                     torch.save(ckpt, path)\n",
    "\n",
    "#             # done training\n",
    "#             if iter_epoch == num_epochs:\n",
    "# #                 print(\"Done training\")\n",
    "\n",
    "#                 # evaluate on train, dev, and test with best model\n",
    "# #                 print(\"Loading best model\")\n",
    "#                 path = \"{}{}.pt\".format(model.__class__.__name__,name_extension)        \n",
    "#                 ckpt = torch.load(path)\n",
    "#                 model.load_state_dict(ckpt[\"state_dict\"])\n",
    "\n",
    "#                 train_acc, _ = eval_fn(\n",
    "#                     model, train_data, batch_size=eval_batch_size, \n",
    "#                     batch_fn=batch_fn, prep_fn=prep_fn)\n",
    "#                 dev_acc, _ = eval_fn(\n",
    "#                     model, dev_data, batch_size=eval_batch_size,\n",
    "#                     batch_fn=batch_fn, prep_fn=prep_fn)\n",
    "#                 test_acc, predictions = eval_fn(\n",
    "#                     model, test_data, batch_size=eval_batch_size, \n",
    "#                     batch_fn=batch_fn, prep_fn=prep_fn)\n",
    "                \n",
    "#                 if predictions and predict:\n",
    "#                     with open(f\"predictions-{name_extension}.csv\", \"w\", newline=\"\") as out:\n",
    "#                         wr = csv.writer(out)\n",
    "#                         wr.writerow(['gwb_code_8'] + new_index_columns[3:])\n",
    "#                         wr.writerows(predictions)\n",
    "\n",
    "# #                 print(\"best model iter {:d}: \"\n",
    "# #                       \"train acc={:.4f}, dev acc={:.4f}, test acc={:.4f}\".format(\n",
    "# #                           best_iter, train_acc, dev_acc, test_acc))\n",
    "\n",
    "#                 return test_acc, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createSplits(data, k):\n",
    "    folds = {}\n",
    "    for i in range(k):\n",
    "        dev = data[math.ceil(i*len(data)/k) : math.ceil((i+1)*len(data)/k)]\n",
    "        train = [x for x in data if x not in dev]\n",
    "        folds[i] = train,dev\n",
    "    return folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 5: loss=94.9998, time=62.77s\n",
      "iter 5: dev acc=0.0009\n",
      "new highscore\n",
      "Iter 10: loss=94.9973, time=127.97s\n",
      "iter 10: dev acc=0.0000\n",
      "Iter 15: loss=94.9681, time=193.56s\n",
      "iter 15: dev acc=0.0000\n",
      "Iter 20: loss=94.9473, time=257.19s\n",
      "iter 20: dev acc=0.0000\n",
      "Iter 25: loss=94.9276, time=320.29s\n",
      "iter 25: dev acc=0.0000\n",
      "Iter 30: loss=94.8088, time=383.26s\n",
      "iter 30: dev acc=0.0000\n",
      "Iter 35: loss=94.7233, time=445.97s\n",
      "iter 35: dev acc=0.0000\n",
      "Iter 40: loss=94.5907, time=508.18s\n",
      "iter 40: dev acc=0.0000\n",
      "Iter 45: loss=94.4403, time=569.79s\n",
      "iter 45: dev acc=0.0000\n",
      "Iter 50: loss=94.3362, time=634.49s\n",
      "iter 50: dev acc=0.0000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-60d65a95843c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrecall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-b59e23bb2d24>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, num_epochs, print_every, eval_every, batch_fn, prep_fn, eval_fn, batch_size, eval_batch_size, predict)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0;31m# compute gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m                 \u001b[0;31m# update weights - take a small step in the opposite dir of the gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/datamining/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/datamining/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if device == 'cuda':\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# item_embedding_dim, hidden_size, output_dim, num_layers, vocab, \n",
    "train_data = trainData\n",
    "dev_data = test_data = testData\n",
    "\n",
    "model = NarmPlus(10,4,4,1,1,v,dropout=0)\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "a, p = train_model(model,optimizer, eval_fn=recall, num_epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([i for i in range(12)]).reshape((4,3))\n",
    "print(a)\n",
    "b = torch.tensor([[1],[0],[2],[0]])\n",
    "b = torch.t(torch.tensor([[1,0,2,0]]))\n",
    "print(torch.gather(a, 1, b))\n",
    "print(a - torch.gather(a, 1, b))\n",
    "print(a.shape, b.shape)\n",
    "print(a.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top1loss(output, targets):\n",
    "    scores_for_targets = torch.gather(output, 1, targets)\n",
    "    loss = nn.Mean(nn.Sigmoid(output - scores_for_targets) +\n",
    "            nn.Sigmoid(output**2))\n",
    "    return loss\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
