{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NARM+\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing functions.\n",
    "import pandas\n",
    "import csv\n",
    "from collections import Counter, OrderedDict, defaultdict, namedtuple\n",
    "import gc\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import itertools\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# PyTorch can run on CPU or on Nvidia GPU (video card) using CUDA\n",
    "# This cell selects the GPU if one is available.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.synchronize() \n",
    "    \n",
    "# requirements:\n",
    "# gensim version 3.6.0\n",
    "# torch version 1.1.0\n",
    "# numpy version newest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7501"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.backends.cudnn.version()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11832\n",
      "67172\n"
     ]
    }
   ],
   "source": [
    "# Create sessions\n",
    "sample = pandas.read_pickle(\"./data/processedData.pkl\")\n",
    "sample['SESSION'] = pandas.to_datetime(sample['TIMESTAMP'],unit='s').dt.date\n",
    "\n",
    "print(len(sample[\"USERID\"].unique()))\n",
    "print(len(sample[\"PRODUCTID\"].unique()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of sessions per user\n",
      "5.765128465179175\n",
      "Average number of clicks per session\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11.017386715142274"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Average number of sessions per user\")\n",
    "print(sample.groupby('USERID')['SESSION'].nunique().mean())\n",
    "\n",
    "print(\"Average number of clicks per session\")\n",
    "sample.groupby(['USERID', 'SESSION'])['ACTION'].count().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "userList = sample[\"USERID\"].unique()\n",
    "productList = sample[\"PRODUCTID\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we first define a class that can map a product to an ID (p2i)\n",
    "# and back (i2p).\n",
    "\n",
    "class OrderedCounter(Counter, OrderedDict):\n",
    "    \"\"\"Counter that remembers the order elements are first seen\"\"\"\n",
    "    def __repr__(self):\n",
    "        return '%s(%r)' % (self.__class__.__name__,\n",
    "                      OrderedDict(self))\n",
    "    def __reduce__(self):\n",
    "        return self.__class__, (OrderedDict(self),)\n",
    "\n",
    "\n",
    "class Vocabulary:\n",
    "    \"\"\"A vocabulary, assigns IDs to tokens\"\"\"\n",
    "    def __init__(self):\n",
    "        self.freqs = OrderedCounter()\n",
    "        self.users = []\n",
    "        self.u2i = {}\n",
    "        self.i2u = []\n",
    "        self.p2i = {}\n",
    "        self.i2p = []\n",
    "        self.p2e = {}\n",
    "        self.u2e = {}\n",
    "\n",
    "    def count_product(self, t):\n",
    "        self.freqs[t] += 1\n",
    "    \n",
    "    def count_user(self, t):\n",
    "        self.users.append(t)\n",
    "\n",
    "    def add_product(self, t):\n",
    "        self.p2i[t] = str(len(self.p2i))\n",
    "        self.i2p.append(t) \n",
    "        \n",
    "    def add_user(self, t):\n",
    "        self.u2i[t] = str(len(self.u2i))\n",
    "        self.i2u.append(t)\n",
    "\n",
    "    def build(self, min_freq=0):\n",
    "#         self.add_product(\"<unk>\")  # reserve 0 for <unk> (unknown products (products only occuring in test set))\n",
    "#         self.add_user(\"<unk>\")\n",
    "        tok_freq = list(self.freqs.items())\n",
    "        tok_freq.sort(key=lambda x: x[1], reverse=True)\n",
    "        for tok, freq in tok_freq:\n",
    "            if freq >= min_freq:\n",
    "                self.add_product(tok)\n",
    "        for user in self.users:\n",
    "            self.add_user(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 67172\n"
     ]
    }
   ],
   "source": [
    "# This process should be deterministic and should have the same result \n",
    "# if run multiple times on the same data set.\n",
    "\n",
    "def build_voc(userList, productList):\n",
    "    v = Vocabulary()\n",
    "    for product in productList:\n",
    "        v.count_product(product)\n",
    "    for user in userList:\n",
    "        v.count_user(user)\n",
    "    v.build()\n",
    "    return v\n",
    "\n",
    "v = build_voc(userList, productList)\n",
    "print(\"Vocabulary size:\", len(v.p2i))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2268318, 2333346], [4365585, 230380], [2951368, 3108797], [2734026, 4152983, 266784, 266784, 1305059], [2087357, 3157558], [2087357, 1340922, 4954999], [3219016, 2028434, 3219016], [4954999, 818610, 271696]]\n"
     ]
    }
   ],
   "source": [
    "# Create nested list of sessions and items per user\n",
    "userBase = sample.groupby(['USERID', 'SESSION'])['PRODUCTID'].apply(list).groupby('USERID').apply(list)\n",
    "print(userBase[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example(userID='1', history=['0', '1', '2', '3', '4', '5', '6', '7', '8', '8', '9', '10', '11', '10', '12', '13'], inputs=['14', '15'], target=['14'])\n",
      "\n",
      "Example(userID='1', history=['0', '1', '2', '3', '4', '5', '6', '7', '8', '8', '9', '10', '11', '10', '12', '13', '14', '15', '14'], inputs=['13', '16'], target=['17'])\n"
     ]
    }
   ],
   "source": [
    "# More efficient create examples function\n",
    "# A simple way to define a class is using namedtuple.\n",
    "Example = namedtuple(\"Example\", [\"userID\", \"history\", \"inputs\", \"target\"])\n",
    "\n",
    "allSessions = []\n",
    "allUsers = []\n",
    "\n",
    "def f(userid, sessions, train):\n",
    "    #print(sessions)\n",
    "    sessions = [[v.p2i.get(t,0) for t in ses] for ses in sessions if len(ses) > 1]\n",
    "    if train:\n",
    "        train_session = sessions[-2]\n",
    "        train_sessions, train_labels = process_seqs(train_session)\n",
    "        objects_train = []\n",
    "        for session, label in zip(train_sessions, train_labels):\n",
    "            object_train = Example(userID = str(userid), history = \n",
    "                                   [item for sublist in sessions[:-2] for item in sublist], \n",
    "                                   inputs = session, target = [label])\n",
    "            objects_train.append(object_train)\n",
    "        return objects_train\n",
    "    else:\n",
    "        # store info for the pretrained embeddings\n",
    "        allSessions.extend(sessions)\n",
    "        userDoc = [t for ses in sessions for t in ses]\n",
    "        allUsers.append(TaggedDocument(userDoc, [str(userid)]))\n",
    "        \n",
    "        test_session = sessions[-1]\n",
    "        test_sessions, test_labels = process_seqs(test_session)\n",
    "        objects_test = []\n",
    "        for session, label in zip(test_sessions, test_labels):\n",
    "            object_test= Example(userID = str(userid), history = \n",
    "                           [item for sublist in sessions[:-1] for item in sublist], \n",
    "                           inputs = session, \n",
    "                           target = [label])\n",
    "            objects_test.append(object_test)\n",
    "        return objects_test\n",
    "    \n",
    "def process_seqs(seq):\n",
    "    out_seqs = []\n",
    "    labs = []\n",
    "    for i in range(1, len(seq)):\n",
    "        tar = seq[-i]\n",
    "        labs += [tar]\n",
    "        out_seqs += [seq[:-i]]\n",
    "\n",
    "    return out_seqs, labs\n",
    "\n",
    "def createExamples(userBase):\n",
    "    ''' Create training and testing set '''\n",
    "    userBase = pandas.DataFrame(userBase)\n",
    "    userBase.reset_index(level = 0, inplace = True)\n",
    "    trainData = [x for l in \n",
    "                 userBase.apply(lambda x: f(x['USERID'], x['PRODUCTID'], True), axis = 1).tolist() for x in l\n",
    "                 if x is not None]\n",
    "    testData = [x for l in \n",
    "                userBase.apply(lambda x: f(x['USERID'], x['PRODUCTID'], False), axis = 1).tolist() for x in l\n",
    "                if x is not None]\n",
    "    return trainData, testData\n",
    "\n",
    "trainData, testData = createExamples(userBase)\n",
    "print(trainData[0])\n",
    "print('')\n",
    "print(testData[0])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HELPER FUNCTIONS\n",
    "        \n",
    "def get_minibatch(data, batch_size=25, shuffle=True):\n",
    "    \"\"\"Return minibatches, optional shuffling\"\"\"\n",
    "\n",
    "    if shuffle:\n",
    "#         print(\"Shuffling training data\")\n",
    "        random.shuffle(data)  # shuffle training data each epoch\n",
    "\n",
    "    batch = []\n",
    "\n",
    "    # yield minibatches\n",
    "    for example in data:\n",
    "        batch.append(example)\n",
    "\n",
    "        if len(batch) == batch_size:\n",
    "            yield batch\n",
    "            batch = []\n",
    "\n",
    "        # in case there is something left\n",
    "    if len(batch) > 0:\n",
    "        yield batch\n",
    "\n",
    "def pad(tokens, length, pad_value=-1):\n",
    "    \"\"\"add padding 0s to a sequence to that it has the desired length\"\"\"\n",
    "    return tokens + [pad_value] * (length - len(tokens))\n",
    "\n",
    "def prepare_minibatch(mb, vocab):\n",
    "    \"\"\"\n",
    "    Minibatch is a list of examples.\n",
    "    This function converts products to IDs and returns\n",
    "    torch tensors to be used as input/targets.\n",
    "    \"\"\"\n",
    "    batch_size = len(mb)\n",
    "    \n",
    "    u = [vocab.u2i.get(example.userID,0) for example in mb]\n",
    "    v = torch.LongTensor(u)\n",
    "    v = v.to(device)\n",
    "    # shape v (batch size, user id)\n",
    "    \n",
    "    # vocab returns 0 if the word is not there\n",
    "    maxlen = max([len(ex.history) for ex in mb])\n",
    "    w = [[pad([int(t) for t in ex.history], maxlen)] for ex in mb]\n",
    "    w = torch.LongTensor(w)\n",
    "    w = w.to(device)\n",
    "    # shape w (batch size, max history length)\n",
    "\n",
    "    # vocab returns 0 if the word is not there\n",
    "    maxlen = max([len(ex.inputs) for ex in mb])\n",
    "    x = [pad([int(t) for t in ex.inputs], maxlen) for ex in mb]\n",
    "    x = torch.LongTensor(x)\n",
    "    x = x.to(device)\n",
    "    xlengths = torch.LongTensor([len(ex.inputs) for ex in mb])\n",
    "    # shape x (batch size, max current session length)\n",
    "\n",
    "    y = [[int(t)-1 for t in ex.target] for ex in mb]\n",
    "    y = torch.LongTensor(y)\n",
    "    y = y.to(device)\n",
    "    \n",
    "    gc.collect()\n",
    "\n",
    "    return (v,w,(x,xlengths)), y\n",
    "\n",
    "def prepare_minibatch_pre_trained(mb, vocab):\n",
    "    \"\"\"\n",
    "    Minibatch is a list of examples.\n",
    "    This function converts products to IDs and returns\n",
    "    torch tensors to be used as input/targets.\n",
    "    \"\"\"\n",
    "    batch_size = len(mb)\n",
    "    embedding_dim = len(vocab.p2e['1'])\n",
    "    \n",
    "    u = [[vocab.u2e[example.userID]] for example in mb]\n",
    "    v = torch.FloatTensor(u)\n",
    "    v = v.to(device)\n",
    "    # shape v (batch size, 1, embedding size)\n",
    "    \n",
    "    # vocab returns 0 if the word is not there\n",
    "    maxlen = max([len(ex.history) for ex in mb])\n",
    "    w = [pad(ex.history, maxlen, pad_value='0') for ex in mb]\n",
    "    w = [[vocab.p2e[t] for t in ex] for ex in w]\n",
    "    w = torch.FloatTensor(w)\n",
    "    w = w.to(device)\n",
    "    # shape w (batch size, max history length, embedding size)\n",
    "\n",
    "    # vocab returns 0 if the word is not there\n",
    "    maxlen = max([len(ex.inputs) for ex in mb])\n",
    "    x = [pad(ex.inputs, maxlen, pad_value='0') for ex in mb]\n",
    "    x = [[vocab.p2e[t] for t in ex] for ex in x]\n",
    "    x = torch.FloatTensor(x)\n",
    "    x = x.to(device)\n",
    "    xlengths = torch.LongTensor([len(ex.inputs) for ex in mb])\n",
    "    # shape x (batch size, max current session length, embedding size)\n",
    "\n",
    "    y = [[int(t)-1 for t in ex.target] for ex in mb]\n",
    "    y = torch.LongTensor(y)\n",
    "    y = y.to(device)\n",
    "        \n",
    "    gc.collect()\n",
    "    \n",
    "    return (v,w,(x,xlengths)), y\n",
    "\n",
    "def recall(model, data, prep_fn=prepare_minibatch, batch_fn=get_minibatch, at=20, batch_size=25, **kwargs):\n",
    "    model.eval() # disable dropout\n",
    "    targets = []\n",
    "    predictions = []\n",
    "    recall = 0\n",
    "    \n",
    "    vocab = model.vocab\n",
    "    for batch in batch_fn(data, batch_size=batch_size):\n",
    "        # convert the example input and targets to PyTorch tensors\n",
    "        x, target = prep_fn(batch, vocab)\n",
    "        # forward pass\n",
    "        # get the output from the neural network for input x\n",
    "        with torch.no_grad():\n",
    "            output, alphas = model(x)\n",
    "        # output shape: (batch size, sequence length, nr of products)\n",
    "        prediction = torch.argsort(output, dim=1, descending=True)[:,:at].tolist()\n",
    "    \n",
    "        batch_targets = [i for l in target.tolist() for i in l]\n",
    "        \n",
    "        targets.extend(batch_targets)\n",
    "        predictions.extend(prediction)\n",
    "        gc.collect()\n",
    "        \n",
    "    print(predictions[:10],targets[:10])\n",
    "    recall = sum(\n",
    "        [1 if t in p else 0 for t,p in zip(targets,predictions)]\n",
    "    )/len(targets)\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    return recall, None\n",
    "\n",
    "def mrr(model, data, prep_fn=prepare_minibatch, batch_fn=get_minibatch, at=5, **kwargs):\n",
    "    model.eval() # disable dropout\n",
    "    targets = []\n",
    "    predictions = []\n",
    "    mrr = 0\n",
    "    \n",
    "    vocab = model.vocab\n",
    "    for batch in batch_fn(data, batch_size=batch_size):\n",
    "        # convert the example input and targets to PyTorch tensors\n",
    "        x, target = prep_fn(batch, vocab)\n",
    "        # forward pass\n",
    "        # get the output from the neural network for input x\n",
    "        with torch.no_grad():\n",
    "            output, alphas = model(x)\n",
    "        # output shape: (batch size, sequence length, nr of products)\n",
    "        prediction = torch.argsort(output, dim=1, descending=True)[:,:at].tolist()\n",
    "    \n",
    "        batch_targets = [i for l in target.tolist() for i in l]\n",
    "        \n",
    "        targets.extend(batch_targets)\n",
    "        predictions.extend(predictions)\n",
    "        gc.collect()\n",
    "        \n",
    "    print(predictions[:10],targets[:10])\n",
    "        \n",
    "    mrr = sum(\n",
    "        [1/(p.index(t) + 1) if t in p else 0 for t,p in zip(targets,predictions)]\n",
    "    )/len(targets)\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    return mrr, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom NN\n",
    "\n",
    "#Item embedding & User Embedding equal size\n",
    "#\n",
    "\n",
    "class NarmPlus(nn.Module):\n",
    "    def __init__(self, \n",
    "                 item_embedding_dim, user_embedding_dim, hidden_size, output_dim, num_layers, \n",
    "                 vocab, pre_trained=True, batch_size=10,\n",
    "                 activation_fn=nn.RReLU(), dropout=0.2):\n",
    "        super(NarmPlus, self).__init__()\n",
    "        # Store parameters\n",
    "        self.item_embedding_dim = item_embedding_dim\n",
    "        self.user_embedding_dim = user_embedding_dim\n",
    "        self.hidden_size = hidden_size # hidden size is also user embedding dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.pre_trained = pre_trained\n",
    "        self.batch_size = batch_size\n",
    "        # Shape of hidden_state: (num_layers * num_directions, batch, hidden_size)\n",
    "        self.hidden_state_dim = (num_layers, batch_size, hidden_size)\n",
    "        self.hidden_state_size = num_layers * hidden_size\n",
    "        self.vocab = vocab\n",
    "        num_users = len(vocab.u2i)\n",
    "        num_items = len(vocab.p2i)\n",
    "        \n",
    "        # General part\n",
    "        self.ActivationFn = activation_fn\n",
    "        self.Softmax = nn.Softmax(dim=1)\n",
    "        self.loss = self.top1loss\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # History part\n",
    "        # embeddings are done seperately\n",
    "        self.LatentItemHistory = nn.Linear(item_embedding_dim, user_embedding_dim)\n",
    "        self.ProfileToHidden = nn.Linear(user_embedding_dim, self.hidden_state_size)\n",
    "        \n",
    "        # NARM Part\n",
    "        # Input to the GRU is the item embedding: input_size = embedding_size\n",
    "        # Hidden size is something we can experiment with\n",
    "        self.Global = nn.GRU(item_embedding_dim, hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.Decoder = nn.Bilinear(item_embedding_dim, 2*hidden_size, output_dim)\n",
    "        \n",
    "        # Inner working of NARM attention part\n",
    "        # Latent space for alpha: what value to pick?\n",
    "        # I assume no bias, based on the paper\n",
    "        latent_space = hidden_size\n",
    "        self.A1 = nn.Linear(hidden_size,latent_space,bias=False)\n",
    "        self.A2 = nn.Linear(hidden_size,latent_space,bias=False)\n",
    "        self.v = nn.Linear(latent_space,1,bias=False)\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        user, history, (inputs, input_lengths) = x\n",
    "        # user shape (batch size, embedding size)\n",
    "        # history shape (batch size, history length, embedding size)\n",
    "        # inputs shape (batch size, sequence length, embedding size)\n",
    "        if self.item_embedding_dim == self.user_embedding_dim:\n",
    "            dense = history\n",
    "        else:\n",
    "            dense = self.ActivationFn(self.LatentItemHistory(history))\n",
    "        # dense shape (batch size, history_length, hidden_state_size)\n",
    "        alpha1 = self.Softmax(torch.matmul(dense,torch.transpose(user, 1, 2)))\n",
    "        # alpha1 shape (batch size, history length, 1)\n",
    "        profile = torch.sum(torch.mul(alpha1,dense),1)\n",
    "        # profile shape (batch size, embedding size)\n",
    "        if self.user_embedding_dim == self.hidden_size:\n",
    "            h_0 = profile[None,:,:]\n",
    "        else:\n",
    "            h_0 = self.ActivationFn(self.ProfileToHidden(profile[None,:,:]))\n",
    "        # h_0 needs to be of shape: (num layers, batch size, embedding size)\n",
    "\n",
    "        inputs = nn.utils.rnn.pack_padded_sequence(inputs, input_lengths, batch_first=True, enforce_sorted=False)\n",
    "        out_global, _ = self.Global(inputs, h_0)\n",
    "        out_global, lengths = nn.utils.rnn.pad_packed_sequence(out_global, batch_first=True)\n",
    "        # out shape (batch_size, seq_length, hidden_size), containing hidden state output for every step\n",
    "        l = (lengths - 1).unsqueeze(1).to(device)\n",
    "        l = torch.transpose(l.repeat(1,1,self.hidden_size),0,1)\n",
    "        c_global = torch.gather(out_global, 1, l)\n",
    "        # Shape of c_global and c_local should be: (batch size, 1, hidden_size)\n",
    "        c_global = out_global[:,-1,:].unsqueeze(1)\n",
    "        # c_global shape (batch size, 1, hidden_size)\n",
    "        c_local = self.calculate_c_local(out_global, c_global, lengths)\n",
    "        # Shape of c_global and c_local are: (batch size, 1, hidden_size)\n",
    "        c = self.dropout(torch.cat((c_global, c_local), dim=2))\n",
    "        # shape c (batch size, 1, hidden size * 2)\n",
    "        # Decoder takes as inputs: embeddings for each item and c\n",
    "        embeds = torch.FloatTensor([[self.vocab.p2e[str(i)] for i in range(len(self.vocab.p2i))]]).to(device)\n",
    "        # shape embeds (1, number of products, embedding dim)\n",
    "        batch_size = c.shape[0]\n",
    "        sequence_length = c.shape[1]\n",
    "        nr_of_products = embeds.shape[1]\n",
    "\n",
    "        # Make embeds and c the same shape\n",
    "        embeds = embeds.repeat(batch_size,1,1).contiguous()\n",
    "        c = c.repeat(1,nr_of_products,1).contiguous()\n",
    "        out = self.Decoder(embeds, c)[:,:,0]\n",
    "        # out shape (batch size, number of products)\n",
    "        output = self.Softmax(out)\n",
    "        gc.collect()\n",
    "        return output, None\n",
    "    \n",
    "    def calculate_c_local(self,hidden_states,c_global,lengths):\n",
    "        c_local = c_global.new_empty(c_global.shape)\n",
    "        lengths = lengths.tolist()\n",
    "        batch_size = hidden_states.shape[0]\n",
    "        for b,length in enumerate(lengths):\n",
    "            alphas = torch.zeros(length, dtype=torch.float32).to(device)\n",
    "            ht = c_global[b,0,:]\n",
    "            A1 = self.A1(ht)\n",
    "            for j in range(length):\n",
    "                hj = hidden_states[b,j,:]\n",
    "                A2 = self.A2(hj)\n",
    "                alphas[j] = self.v(self.ActivationFn(A1 + A2))\n",
    "            ct = torch.sum(alphas.unsqueeze(1) * hidden_states[b,:length,:], dim=0).unsqueeze(0)\n",
    "            c_local[b] = ct\n",
    "        return c_local          \n",
    "    \n",
    "    def top1loss(self, output, targets):        \n",
    "        scores_for_targets = torch.gather(output, 1, targets)\n",
    "        loss = torch.mean(torch.sigmoid(output - scores_for_targets) +\n",
    "            torch.sigmoid(output**2))\n",
    "        gc.collect()\n",
    "        return loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to train a model\n",
    "name_extension = ''\n",
    "def train_model(model, optimizer, num_epochs=10, \n",
    "                print_every=1, eval_every=1,\n",
    "                batch_fn=get_minibatch, \n",
    "                prep_fn=prepare_minibatch_pre_trained,\n",
    "                eval_fn=recall,\n",
    "                batch_size=10, eval_batch_size=None,\n",
    "                pre_trained=True\n",
    "               ):\n",
    "    \"\"\"Train a model.\"\"\"  \n",
    "    train_loss = 0.\n",
    "    start = time.time()\n",
    "    best_eval = 0.\n",
    "    best_iter = 0\n",
    "    eval_iter = 0\n",
    "    criterion=model.loss\n",
    "\n",
    "    # store train loss and validation accuracy during training\n",
    "    # so we can plot them afterwards\n",
    "    losses = []\n",
    "    accuracies = []  \n",
    "\n",
    "    if eval_batch_size is None:\n",
    "        eval_batch_size = batch_size\n",
    "    \n",
    "    vocab = model.vocab\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        for example in batch_fn(train_data, batch_size=batch_size): \n",
    "            # goes through the entire training data once, a.k.a. an epoch\n",
    "            # do a garbage collect to make sure the memory is freed\n",
    "            gc.collect()\n",
    "            # forward pass, make sure the model is in train modus\n",
    "            model.train()\n",
    "            x, targets = prep_fn(example, vocab)\n",
    "\n",
    "            output, alphas = model(x)\n",
    "            # output shape (batch size, sequence length, nr of products): a score for each product at each time step\n",
    "            # alphas are the alphas generated in the Narm part\n",
    "\n",
    "            eval_iter += 1\n",
    "                \n",
    "            loss = criterion(output, targets)\n",
    "            train_loss += float(loss.item())\n",
    "\n",
    "            # backward pass\n",
    "            # erase previous gradients\n",
    "            model.zero_grad()\n",
    "\n",
    "            # compute gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # update weights - take a small step in the opposite dir of the gradient\n",
    "            optimizer.step()\n",
    "            \n",
    "            if eval_iter % 1000 == 0:\n",
    "                accuracy, _ = eval_fn(model, dev_data, batch_size=eval_batch_size,\n",
    "                                         batch_fn=batch_fn, prep_fn=prep_fn)\n",
    "                accuracies.append(accuracy)\n",
    "                print(\"epoch %r: dev acc=%.4f\" % (epoch + 1, accuracy))       \n",
    "\n",
    "                # save best model parameters\n",
    "                if accuracy > best_eval:\n",
    "                    print(\"new highscore\")\n",
    "                    best_eval = accuracy\n",
    "                    best_iter = epoch + 1\n",
    "                    path = \"{}{}.pt\".format(model.__class__.__name__,name_extension)\n",
    "                    ckpt = {\n",
    "                      \"state_dict\": model.state_dict(),\n",
    "                      \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                      \"best_eval\": best_eval,\n",
    "                      \"best_iter\": best_iter\n",
    "                    }\n",
    "                    torch.save(ckpt, path)\n",
    "\n",
    "        if (epoch + 1) % print_every == 0:\n",
    "            print(\"Epoch %r: loss=%.4f, time=%.2fs\" % \n",
    "                 (epoch + 1, train_loss, time.time()-start))\n",
    "            losses.append(train_loss)       \n",
    "            train_loss = 0.\n",
    "            \n",
    "        if (epoch + 1) % eval_every == 0:\n",
    "            accuracy, _ = eval_fn(model, dev_data, batch_size=eval_batch_size,\n",
    "                                         batch_fn=batch_fn, prep_fn=prep_fn)\n",
    "            accuracies.append(accuracy)\n",
    "            print(\"epoch %r: dev acc=%.4f\" % (epoch + 1, accuracy))       \n",
    "\n",
    "            # save best model parameters\n",
    "            if accuracy > best_eval:\n",
    "                print(\"new highscore\")\n",
    "                best_eval = accuracy\n",
    "                best_iter = epoch + 1\n",
    "                path = \"{}{}.pt\".format(model.__class__.__name__,name_extension)\n",
    "                ckpt = {\n",
    "                  \"state_dict\": model.state_dict(),\n",
    "                  \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                  \"best_eval\": best_eval,\n",
    "                  \"best_iter\": best_iter\n",
    "                }\n",
    "                torch.save(ckpt, path)\n",
    "    \n",
    "    # Done training\n",
    "    # evaluate on train, dev, and test with best model\n",
    "    print(\"Loading best model\")\n",
    "    path = \"{}{}.pt\".format(model.__class__.__name__,name_extension)        \n",
    "    ckpt = torch.load(path)\n",
    "    model.load_state_dict(ckpt[\"state_dict\"])\n",
    "\n",
    "    train_acc, _ = eval_fn(\n",
    "        model, train_data, batch_size=eval_batch_size, \n",
    "        batch_fn=batch_fn, prep_fn=prep_fn)\n",
    "    dev_acc, _ = eval_fn(\n",
    "        model, dev_data, batch_size=eval_batch_size,\n",
    "        batch_fn=batch_fn, prep_fn=prep_fn)\n",
    "    test_acc, predictions = eval_fn(\n",
    "        model, test_data, batch_size=eval_batch_size, \n",
    "        batch_fn=batch_fn, prep_fn=prep_fn)\n",
    "\n",
    "    print(\"best model iter {:d}: \"\n",
    "          \"train acc={:.4f}, dev acc={:.4f}, test acc={:.4f}\".format(\n",
    "              best_iter, train_acc, dev_acc, test_acc))\n",
    "\n",
    "    return test_acc, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createSplits(data, k):\n",
    "    folds = {}\n",
    "    for i in range(k):\n",
    "        dev = data[math.ceil(i*len(data)/k) : math.ceil((i+1)*len(data)/k)]\n",
    "        train = [x for x in data if x not in dev]\n",
    "        folds[i] = train,dev\n",
    "    return folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[601, 602, 600, 3096, 2172, 7431, 1905, 3246, 1484, 5316, 3345, 3099, 64993, 4504, 3133, 1503, 66452, 3279, 2374, 238], [601, 602, 600, 3096, 2172, 7431, 1905, 3246, 1484, 3345, 66452, 5316, 64993, 3099, 4504, 3133, 2374, 39777, 7761, 1503], [601, 3096, 2172, 1905, 3246, 7431, 600, 602, 1484, 5316, 3099, 3345, 1503, 4504, 3279, 238, 3133, 64993, 1553, 3510], [601, 602, 600, 3096, 2172, 7431, 1905, 3246, 3345, 1484, 66452, 5316, 64993, 3099, 4504, 3133, 39777, 2374, 1503, 7761], [2172, 3096, 5316, 3246, 7431, 1503, 1905, 3345, 1484, 238, 3099, 3133, 2374, 10864, 233, 735, 3279, 4504, 3844, 7507], [601, 602, 600, 7431, 3096, 66452, 2172, 1905, 3246, 3345, 64993, 61410, 1484, 5316, 39777, 61414, 4504, 3099, 3133, 2374], [601, 602, 600, 3096, 2172, 7431, 1905, 3246, 1484, 3345, 5316, 66452, 64993, 3099, 39777, 4504, 3133, 2374, 1503, 7761], [2172, 3096, 5316, 3246, 7431, 1503, 1905, 3345, 3133, 1484, 3099, 238, 2374, 735, 10864, 4504, 3279, 4704, 7507, 921], [601, 602, 600, 3096, 2172, 1905, 7431, 3246, 1484, 5316, 3345, 3099, 64993, 4504, 1503, 3133, 3279, 238, 2374, 1553], [601, 602, 600, 3096, 2172, 1905, 7431, 3246, 1484, 5316, 3345, 3099, 64993, 4504, 66452, 3133, 1503, 2374, 3279, 238]] [20814, 24748, 4620, 21434, 6029, 43808, 34718, 955, 1488, 38922]\n",
      "epoch 1: dev acc=0.0000\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "if device == 'cuda':\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# One of the problems: too much memory is needed to run it on GPU, so manually set device='cpu'\n",
    "# device='cpu'\n",
    "\n",
    "train_data = trainData\n",
    "test_data = testData\n",
    "dev_data = testData[:100]\n",
    "\n",
    "num_users = len(userList)\n",
    "num_products = len(productList)\n",
    "\n",
    "# item_embedding_dim, user_embedding_dim, hidden_size, output_dim, num_layers, vocab, \n",
    "model = NarmPlus(16,8,50,1,1,v,dropout=0.2,pre_trained=True, batch_size=5)\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "vocab = model.vocab\n",
    "product_embeddings = Word2Vec(allSessions, size=model.item_embedding_dim, window=5, min_count=1)\n",
    "user_embeddings = Doc2Vec(allUsers, vector_size=model.user_embedding_dim, window=5, min_count=1)\n",
    "vocab.u2e = user_embeddings.wv\n",
    "vocab.u2e.add(['0'], [np.zeros(model.user_embedding_dim)])\n",
    "vocab.p2e = product_embeddings.wv\n",
    "vocab.p2e.add(['0'], [np.zeros(model.item_embedding_dim)])\n",
    "\n",
    "a, p = train_model(model, optimizer, eval_fn=recall, \n",
    "                   num_epochs=10,\n",
    "                   batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([i for i in range(20)]).view(4,5)\n",
    "print(a)\n",
    "print(a.view(-1))\n",
    "\n",
    "# ix = torch.tensor([[[0]*5],[[1]*5],[[3]*5]])\n",
    "# print(ix.shape)\n",
    "# print(a.shape)\n",
    "# print(a)\n",
    "# torch.gather(a,1,ix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
