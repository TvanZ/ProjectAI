{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import csv\n",
    "import pickle\n",
    "import math\n",
    "import os\n",
    "import operator\n",
    "import pandas\n",
    "from __future__ import print_function\n",
    "\n",
    "from collections import Counter, OrderedDict, defaultdict, namedtuple\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import theano\n",
    "from theano import config\n",
    "import theano.tensor as T\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = pandas.read_pickle(\"./data/processedData.pkl\")\n",
    "userList = sample[\"USERID\"].unique()\n",
    "productList = sample[\"PRODUCTID\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userBase = sample.groupby(['USERID', 'SESSION'])['PRODUCTID'].apply(list).groupby('USERID').apply(list)\n",
    "print(userBase[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we first define a class that can map a product to an ID (p2i)\n",
    "# and back (i2p).\n",
    "\n",
    "class OrderedCounter(Counter, OrderedDict):\n",
    "    \"\"\"Counter that remembers the order elements are first seen\"\"\"\n",
    "    def __repr__(self):\n",
    "        return '%s(%r)' % (self.__class__.__name__,\n",
    "                      OrderedDict(self))\n",
    "    def __reduce__(self):\n",
    "        return self.__class__, (OrderedDict(self),)\n",
    "\n",
    "\n",
    "class Vocabulary:\n",
    "    \"\"\"A vocabulary, assigns IDs to tokens\"\"\"\n",
    "    def __init__(self):\n",
    "        self.freqs = OrderedCounter()\n",
    "        self.users = []\n",
    "        self.u2i = {}\n",
    "        self.i2u = []\n",
    "        self.p2i = {}\n",
    "        self.i2p = []\n",
    "        self.p2e = {}\n",
    "        self.u2e = {}\n",
    "\n",
    "    def count_product(self, t):\n",
    "        self.freqs[t] += 1\n",
    "    \n",
    "    def count_user(self, t):\n",
    "        self.users.append(t)\n",
    "\n",
    "    def add_product(self, t):\n",
    "        self.p2i[t] = str(len(self.p2i))\n",
    "        self.i2p.append(t) \n",
    "        \n",
    "    def add_user(self, t):\n",
    "        self.u2i[t] = str(len(self.u2i))\n",
    "        self.i2u.append(t)\n",
    "\n",
    "    def build(self, min_freq=0):\n",
    "#         self.add_product(\"<unk>\")  # reserve 0 for <unk> (unknown products (products only occuring in test set))\n",
    "#         self.add_user(\"<unk>\")\n",
    "        tok_freq = list(self.freqs.items())\n",
    "        tok_freq.sort(key=lambda x: x[1], reverse=True)\n",
    "        for tok, freq in tok_freq:\n",
    "            if freq >= min_freq:\n",
    "                self.add_product(tok)\n",
    "        for user in self.users:\n",
    "            self.add_user(user)\n",
    "            \n",
    "            \n",
    "# This process should be deterministic and should have the same result \n",
    "# if run multiple times on the same data set.\n",
    "\n",
    "def build_voc(userList, productList):\n",
    "    v = Vocabulary()\n",
    "    for product in productList:\n",
    "        v.count_product(product)\n",
    "    for user in userList:\n",
    "        v.count_user(user)\n",
    "    v.build()\n",
    "    return v\n",
    "\n",
    "v = build_voc(userList, productList)\n",
    "print(\"Vocabulary size:\", len(v.p2i))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More efficient create examples function\n",
    "# A simple way to define a class is using namedtuple.\n",
    "Example = namedtuple(\"Example\", [\"inputs\", \"target\"])\n",
    "\n",
    "\n",
    "def f(userid, sessions, train):\n",
    "    #print(sessions)\n",
    "    sessions = [[v.p2i.get(t,0) for t in ses] for ses in sessions if len(ses) > 1]\n",
    "#     if userid == 11905:\n",
    "#         print(sessions)\n",
    "    if train:\n",
    "        object_train = Example(inputs = sessions[-2], target = sessions[-2][1:])\n",
    "        return object_train\n",
    "    else:\n",
    "        return Example(\n",
    "                       inputs = sessions[-1], \n",
    "                       target = sessions[-1][1:])\n",
    "\n",
    "def createExamples(userBase):\n",
    "    ''' Create training and testing set '''\n",
    "    userBase = pandas.DataFrame(userBase)\n",
    "    userBase.reset_index(level = 0, inplace = True)\n",
    "    trainData = [x for x in \n",
    "                 userBase.apply(lambda x: f(x['USERID'], x['PRODUCTID'], True), axis = 1).tolist() \n",
    "                 if x is not None]\n",
    "    testData = [x for x in \n",
    "                userBase.apply(lambda x: f(x['USERID'], x['PRODUCTID'], False), axis = 1).tolist() \n",
    "                if x is not None]\n",
    "    return trainData, testData\n",
    "\n",
    "trainData, testData = createExamples(userBase)\n",
    "\n",
    "\n",
    "narmTrain = ([example.inputs for example in trainData],[example.target for example in trainData]) \n",
    "narmTest = ([example.inputs for example in testData],[example.target for example in testData])\n",
    "\n",
    "print(narmTrain[0][0])\n",
    "print('')\n",
    "print(narmTest[1][1])\n",
    "print(narmTest[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(narmTrain[1][25])\n",
    "print(narmTrain[0][25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_dict = {}\n",
    "item_ctr = 1\n",
    "train_seqs = []\n",
    "train_dates = []\n",
    "test_sess = narmTest[0]\n",
    "train_sess = narmTrain[0]\n",
    "\n",
    "# Convert training sessions to sequences and renumber items to start from 1\n",
    "date = 0\n",
    "for s in train_sess:\n",
    "    seq = len(s)\n",
    "    date += 1\n",
    "    outseq = []\n",
    "    for i in range(seq):\n",
    "        if i in item_dict:\n",
    "            outseq += [item_dict[i]]\n",
    "        else:\n",
    "            outseq += [item_ctr]\n",
    "            item_dict[i] = item_ctr\n",
    "            item_ctr += 1\n",
    "    if len(outseq) < 2:  # Doesn't occur\n",
    "        continue\n",
    "    train_seqs += [outseq]\n",
    "    train_dates += [date]\n",
    "\n",
    "test_seqs = []\n",
    "test_dates = []\n",
    "date = 0\n",
    "# Convert test sessions to sequences, ignoring items that do not appear in training set\n",
    "for s in test_sess:\n",
    "    seq = len(s)\n",
    "    outseq = []\n",
    "    date += 1\n",
    "    for i in range(seq):\n",
    "        if i in item_dict:\n",
    "            outseq += [item_dict[i]]\n",
    "    if len(outseq) < 2:\n",
    "        continue\n",
    "    test_seqs += [outseq]\n",
    "    test_dates += [date]\n",
    "\n",
    "print(item_ctr)\n",
    "\n",
    "\n",
    "\n",
    "def process_seqs(iseqs, idates):\n",
    "    out_seqs = []\n",
    "    out_dates = []\n",
    "    labs = []\n",
    "    for seq, date in zip(iseqs, idates):\n",
    "        for i in range(1, len(seq)):\n",
    "            tar = seq[-i]\n",
    "            labs += [tar]\n",
    "            out_seqs += [seq[:-i]]\n",
    "            out_dates += [date]\n",
    "\n",
    "    return out_seqs, out_dates, labs\n",
    "\n",
    "\n",
    "tr_seqs, tr_dates, tr_labs = process_seqs(train_seqs,train_dates)\n",
    "te_seqs, te_dates, te_labs = process_seqs(test_seqs,test_dates)\n",
    "\n",
    "trainData = (tr_seqs, tr_labs)\n",
    "testData = (te_seqs, te_labs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Atrain, Btrain = trainData\n",
    "\n",
    "print(test_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "def prepare_data(seqs, labels):\n",
    "    \"\"\"Create the matrices from the datasets.\n",
    "\n",
    "    This pad each sequence to the same lenght: the lenght of the\n",
    "    longuest sequence or maxlen.\n",
    "\n",
    "    if maxlen is set, we will cut all sequence to this maximum\n",
    "    lenght.\n",
    "\n",
    "    This swap the axis!\n",
    "    \"\"\"\n",
    "    # x: a list of sentences\n",
    "\n",
    "    lengths = [len(s) for s in seqs]\n",
    "    n_samples = len(seqs)\n",
    "    maxlen = np.max(lengths)\n",
    "\n",
    "    x = np.zeros((maxlen, n_samples)).astype('int64')\n",
    "    x_mask = np.ones((maxlen, n_samples)).astype(theano.config.floatX)\n",
    "    for idx, s in enumerate(seqs):\n",
    "        x[:lengths[idx], idx] = s\n",
    "\n",
    "    x_mask *= (1 - (x == 0))\n",
    "\n",
    "    return x, x_mask, labels\n",
    "\n",
    "\n",
    "def load_data(valid_portion=0.1, maxlen=19, sort_by_len=False):\n",
    "    '''Loads the dataset\n",
    "\n",
    "    :type path: String\n",
    "    :param path: The path to the dataset (here RSC2015)\n",
    "    :type n_items: int\n",
    "    :param n_items: The number of items.\n",
    "    :type valid_portion: float\n",
    "    :param valid_portion: The proportion of the full train set used for\n",
    "        the validation set.\n",
    "    :type maxlen: None or positive int\n",
    "    :param maxlen: the max sequence length we use in the train/valid set.\n",
    "    :type sort_by_len: bool\n",
    "    :name sort_by_len: Sort by the sequence lenght for the train,\n",
    "        valid and test set. This allow faster execution as it cause\n",
    "        less padding per minibatch. Another mechanism must be used to\n",
    "        shuffle the train set at each epoch.\n",
    "\n",
    "    '''\n",
    "\n",
    "    #############\n",
    "    # LOAD DATA #\n",
    "    #############\n",
    "\n",
    "    # Load the dataset\n",
    "    #path_train_data = './data/processedData.pkl'\n",
    "    #path_test_data = './data/processedData.pkl'\n",
    "\n",
    "    #f1 = open(path_train_data, 'rb')\n",
    "    train_set = trainData\n",
    "    #f1.close()\n",
    "\n",
    "    #f2 = open(path_test_data, 'rb')\n",
    "    test_set = testData\n",
    "    #f2.close()\n",
    "\n",
    "    if maxlen:\n",
    "        new_train_set_x = []\n",
    "        new_train_set_y = []\n",
    "        for x, y in zip(train_set[0], train_set[1]):\n",
    "            if len(x) < maxlen:\n",
    "                new_train_set_x.append(x)\n",
    "                new_train_set_y.append(y)\n",
    "            else:\n",
    "                new_train_set_x.append(x[:maxlen])\n",
    "                new_train_set_y.append(y)\n",
    "        train_set = (new_train_set_x, new_train_set_y)\n",
    "        del new_train_set_x, new_train_set_y\n",
    "\n",
    "        new_test_set_x = []\n",
    "        new_test_set_y = []\n",
    "        for xx, yy in zip(test_set[0], test_set[1]):\n",
    "            if len(xx) < maxlen:\n",
    "                new_test_set_x.append(xx)\n",
    "                new_test_set_y.append(yy)\n",
    "            else:\n",
    "                new_test_set_x.append(xx[:maxlen])\n",
    "                new_test_set_y.append(yy)\n",
    "        test_set = (new_test_set_x, new_test_set_y)\n",
    "        del new_test_set_x, new_test_set_y\n",
    "\n",
    "    # split training set into validation set\n",
    "    train_set_x, train_set_y = train_set\n",
    "    n_samples = len(train_set_x)\n",
    "    sidx = np.arange(n_samples, dtype='int32')\n",
    "    np.random.shuffle(sidx)\n",
    "    n_train = int(np.round(n_samples * (1. - valid_portion)))\n",
    "    valid_set_x = [train_set_x[s] for s in sidx[n_train:]]\n",
    "    valid_set_y = [train_set_y[s] for s in sidx[n_train:]]\n",
    "    train_set_x = [train_set_x[s] for s in sidx[:n_train]]\n",
    "    train_set_y = [train_set_y[s] for s in sidx[:n_train]]\n",
    "\n",
    "    train_set = (train_set_x, train_set_y)\n",
    "    valid_set = (valid_set_x, valid_set_y)\n",
    "\n",
    "    test_set_x, test_set_y = test_set\n",
    "    valid_set_x, valid_set_y = valid_set\n",
    "    train_set_x, train_set_y = train_set\n",
    "\n",
    "#     def len_argsort(seq):\n",
    "#         return sorted(range(len(seq)), key=lambda x: len(seq[x]))\n",
    "\n",
    "#     if sort_by_len:\n",
    "#         sorted_index = len_argsort(test_set_x)\n",
    "#         test_set_x = [test_set_x[i] for i in sorted_index]\n",
    "#         test_set_y = [test_set_y[i] for i in sorted_index]\n",
    "\n",
    "#         sorted_index = len_argsort(valid_set_x)\n",
    "#         valid_set_x = [valid_set_x[i] for i in sorted_index]\n",
    "#         valid_set_y = [valid_set_y[i] for i in sorted_index]\n",
    "\n",
    "    train = (train_set_x, train_set_y)\n",
    "    valid = (valid_set_x, valid_set_y)\n",
    "    test = (test_set_x, test_set_y)\n",
    "\n",
    "    return train, valid, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Build NARM model\n",
    "'''\n",
    "\n",
    "\n",
    "datasets = {'rsc2015': (load_data, prepare_data)}\n",
    "\n",
    "#datasets = {'rsc2015': (narmTrain, narmTest)}\n",
    "\n",
    "# Set the random number generators' seeds for consistency\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "\n",
    "def numpy_floatX(data):\n",
    "    return np.asarray(data, dtype=config.floatX)\n",
    "\n",
    "\n",
    "def get_minibatches_idx(n, minibatch_size, shuffle=False):\n",
    "    \"\"\"\n",
    "    Used to shuffle the dataset at each iteration.\n",
    "    \"\"\"\n",
    "\n",
    "    idx_list = np.arange(n, dtype=\"int32\")\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.shuffle(idx_list)\n",
    "\n",
    "    minibatches = []\n",
    "    minibatch_start = 0\n",
    "    for i in range(n // minibatch_size):\n",
    "        minibatches.append(idx_list[minibatch_start:\n",
    "                                    minibatch_start + minibatch_size])\n",
    "        minibatch_start += minibatch_size\n",
    "\n",
    "    if minibatch_start != n:\n",
    "        # Make a minibatch out of what is left\n",
    "        minibatches.append(idx_list[minibatch_start:])\n",
    "\n",
    "    return zip(range(len(minibatches)), minibatches)\n",
    "\n",
    "\n",
    "def get_dataset(name):\n",
    "    return datasets[name][0], datasets[name][1]\n",
    "\n",
    "\n",
    "def zipp(params, tparams):\n",
    "    \"\"\"\n",
    "    When we reload the model. Needed for the GPU stuff.\n",
    "    \"\"\"\n",
    "    for kk, vv in params.items():\n",
    "        tparams[kk].set_value(vv)\n",
    "\n",
    "\n",
    "def unzip(zipped):\n",
    "    \"\"\"\n",
    "    When we pickle the model. Needed for the GPU stuff.\n",
    "    \"\"\"\n",
    "    new_params = OrderedDict()\n",
    "    for kk, vv in zipped.items():\n",
    "        new_params[kk] = vv.get_value()\n",
    "    return new_params\n",
    "\n",
    "\n",
    "def dropout_layer(state_before, use_noise, trng, drop_p=0.5):\n",
    "    retain = 1. - drop_p\n",
    "    proj = T.switch(use_noise, (state_before * trng.binomial(state_before.shape,\n",
    "                                                             p=retain, n=1,\n",
    "                                                             dtype=state_before.dtype)), state_before * retain)\n",
    "    return proj\n",
    "\n",
    "\n",
    "def _p(pp, name):\n",
    "    return '%s_%s' % (pp, name)\n",
    "\n",
    "\n",
    "def init_params(options):\n",
    "    \"\"\"\n",
    "    Global (not GRU) parameter. For the embeding and the classifier.\n",
    "    \"\"\"\n",
    "    params = OrderedDict()\n",
    "    # embedding\n",
    "    params['Wemb'] = init_weights((options['n_items'], options['dim_proj']))\n",
    "    params = get_layer(options['encoder'])[0](options,\n",
    "                                              params,\n",
    "                                              prefix=options['encoder'])\n",
    "    # attention\n",
    "    params['W_encoder'] = init_weights((options['hidden_units'], options['hidden_units']))\n",
    "    params['W_decoder'] = init_weights((options['hidden_units'], options['hidden_units']))\n",
    "    params['bl_vector'] = init_weights((1, options['hidden_units']))\n",
    "    # classifier\n",
    "    # params['U'] = init_weights((2*options['hidden_units'], options['n_items']))\n",
    "    # params['b'] = np.zeros((options['n_items'],)).astype(config.floatX)\n",
    "    params['bili'] = init_weights((options['dim_proj'], 2 * options['hidden_units']))\n",
    "\n",
    "    return params\n",
    "\n",
    "\n",
    "def load_params(path, params):\n",
    "    pp = np.load(path)\n",
    "    for kk, vv in params.items():\n",
    "        if kk not in pp:\n",
    "            raise Warning('%s is not in the archive' % kk)\n",
    "        params[kk] = pp[kk]\n",
    "\n",
    "    return params\n",
    "\n",
    "\n",
    "def init_tparams(params):\n",
    "    tparams = OrderedDict()\n",
    "    for kk, pp in params.items():\n",
    "        tparams[kk] = theano.shared(params[kk], name=kk)\n",
    "    return tparams\n",
    "\n",
    "\n",
    "def get_layer(name):\n",
    "    fns = layers[name]\n",
    "    return fns\n",
    "\n",
    "\n",
    "def init_weights(shape):\n",
    "    sigma = np.sqrt(2. / shape[0])\n",
    "    return numpy_floatX(np.random.randn(*shape) * sigma)\n",
    "\n",
    "\n",
    "def ortho_weight(ndim):\n",
    "    W = np.random.randn(ndim, ndim)\n",
    "    u, s, v = np.linalg.svd(W)\n",
    "    return u.astype(config.floatX)\n",
    "\n",
    "\n",
    "def param_init_gru(options, params, prefix='gru'):\n",
    "    \"\"\"\n",
    "    Init the GRU parameter:\n",
    "\n",
    "    :see: init_params\n",
    "    \"\"\"\n",
    "    Wxrz = np.concatenate([init_weights((options['dim_proj'], options['hidden_units'])),\n",
    "                           init_weights((options['dim_proj'], options['hidden_units'])),\n",
    "                           init_weights((options['dim_proj'], options['hidden_units']))], axis=1)\n",
    "    params[_p(prefix, 'Wxrz')] = Wxrz\n",
    "\n",
    "    Urz = np.concatenate([ortho_weight(options['hidden_units']),\n",
    "                          ortho_weight(options['hidden_units'])], axis=1)\n",
    "    params[_p(prefix, 'Urz')] = Urz\n",
    "\n",
    "    Uh = ortho_weight(options['hidden_units'])\n",
    "    params[_p(prefix, 'Uh')] = Uh\n",
    "\n",
    "    b = np.zeros((3 * options['hidden_units'],))\n",
    "    params[_p(prefix, 'b')] = b.astype(config.floatX)\n",
    "    return params\n",
    "\n",
    "\n",
    "def gru_layer(tparams, state_below, options, prefix='gru', mask=None):\n",
    "    nsteps = state_below.shape[0]\n",
    "    if state_below.ndim == 3:\n",
    "        n_samples = state_below.shape[1]\n",
    "    else:\n",
    "        n_samples = 1\n",
    "\n",
    "    assert mask is not None\n",
    "\n",
    "    def _slice(_x, n, dim):\n",
    "        if _x.ndim == 3:\n",
    "            return _x[:, :, n * dim:(n + 1) * dim]\n",
    "        return _x[:, n * dim:(n + 1) * dim]\n",
    "\n",
    "    def _step(m_, x_, h_):\n",
    "        preact = T.dot(h_, tparams[_p(prefix, 'Urz')])\n",
    "        preact += x_[:, 0:2 * options['hidden_units']]\n",
    "\n",
    "        z = T.nnet.hard_sigmoid(_slice(preact, 0, options['hidden_units']))\n",
    "        r = T.nnet.hard_sigmoid(_slice(preact, 1, options['hidden_units']))\n",
    "        h = T.tanh(T.dot((h_ * r), tparams[_p(prefix, 'Uh')]) + _slice(x_, 2, options['hidden_units']))\n",
    "\n",
    "        h = (1.0 - z) * h_ + z * h\n",
    "        h = m_[:, None] * h + (1. - m_)[:, None] * h_\n",
    "\n",
    "        return h\n",
    "\n",
    "    state_below = (T.dot(state_below, tparams[_p(prefix, 'Wxrz')]) +\n",
    "                   tparams[_p(prefix, 'b')])\n",
    "\n",
    "    hidden_units = options['hidden_units']\n",
    "    rval, updates = theano.scan(_step,\n",
    "                                sequences=[mask, state_below],\n",
    "                                outputs_info=T.alloc(numpy_floatX(0.), n_samples, hidden_units),\n",
    "                                name=_p(prefix, '_layers'),\n",
    "                                n_steps=nsteps)\n",
    "    return rval\n",
    "\n",
    "layers = {'gru': (param_init_gru, gru_layer)}\n",
    "\n",
    "\n",
    "def adam(loss, all_params, learning_rate=0.001, b1=0.9, b2=0.999, e=1e-8, gamma=1-1e-8):\n",
    "    \"\"\"\n",
    "    ADAM update rules\n",
    "    Default values are taken from [Kingma2014]\n",
    "\n",
    "    References:\n",
    "    [Kingma2014] Kingma, Diederik, and Jimmy Ba.\n",
    "    \"Adam: A Method for Stochastic Optimization.\"\n",
    "    arXiv preprint arXiv:1412.6980 (2014).\n",
    "    http://arxiv.org/pdf/1412.6980v4.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    updates = OrderedDict()\n",
    "    all_grads = theano.grad(loss, all_params)\n",
    "    alpha = learning_rate\n",
    "    t = theano.shared(np.float32(1))\n",
    "    b1_t = b1*gamma**(t-1)   #(Decay the first moment running average coefficient)\n",
    "\n",
    "    for theta_previous, g in zip(all_params, all_grads):\n",
    "        m_previous = theano.shared(np.zeros(theta_previous.get_value().shape, dtype=config.floatX))\n",
    "        v_previous = theano.shared(np.zeros(theta_previous.get_value().shape, dtype=config.floatX))\n",
    "\n",
    "        m = b1_t*m_previous + (1 - b1_t)*g  # (Update biased first moment estimate)\n",
    "        v = b2*v_previous + (1 - b2)*g**2   # (Update biased second raw moment estimate)\n",
    "        m_hat = m / (1-b1**t)               # (Compute bias-corrected first moment estimate)\n",
    "        v_hat = v / (1-b2**t)               # (Compute bias-corrected second raw moment estimate)\n",
    "        theta = theta_previous - (alpha * m_hat) / (T.sqrt(v_hat) + e) #(Update parameters)\n",
    "\n",
    "        updates[m_previous] = m\n",
    "        updates[v_previous] = v\n",
    "        updates[theta_previous] = theta\n",
    "    updates[t] = t + 1.\n",
    "\n",
    "    return updates\n",
    "\n",
    "\n",
    "def build_model(tparams, options):\n",
    "    trng = RandomStreams(SEED)\n",
    "\n",
    "    # Used for dropout.\n",
    "    use_noise = theano.shared(numpy_floatX(0.))\n",
    "\n",
    "    x = T.matrix('x', dtype='int64')\n",
    "    mask = T.matrix('mask', dtype=config.floatX)\n",
    "    y = T.vector('y', dtype='int64')\n",
    "\n",
    "    n_timesteps = x.shape[0]\n",
    "    n_samples = x.shape[1]\n",
    "\n",
    "    emb = tparams['Wemb'][x.flatten()].reshape([n_timesteps,\n",
    "                                                n_samples,\n",
    "                                                options['dim_proj']])\n",
    "    if options['use_dropout']:\n",
    "        emb = dropout_layer(emb, use_noise, trng, drop_p=0.25)\n",
    "\n",
    "    proj = get_layer(options['encoder'])[1](tparams, emb, options,\n",
    "                                            prefix=options['encoder'],\n",
    "                                            mask=mask)\n",
    "\n",
    "    def compute_alpha(state1, state2):\n",
    "        tmp = T.nnet.hard_sigmoid(T.dot(tparams['W_encoder'], state1.T) + T.dot(tparams['W_decoder'], state2.T))\n",
    "        alpha = T.dot(tparams['bl_vector'], tmp)\n",
    "        res = T.sum(alpha, axis=0)\n",
    "        return res\n",
    "\n",
    "    last_h = proj[-1]\n",
    "\n",
    "    sim_matrix, _ = theano.scan(\n",
    "        fn=compute_alpha,\n",
    "        sequences=proj,\n",
    "        non_sequences=proj[-1]\n",
    "    )\n",
    "    att = T.nnet.softmax(sim_matrix.T * mask.T) * mask.T\n",
    "    p = att.sum(axis=1)[:, None]\n",
    "    weight = att / p\n",
    "    atttention_proj = (proj * weight.T[:, :, None]).sum(axis=0)\n",
    "\n",
    "    proj = T.concatenate([atttention_proj, last_h], axis=1)\n",
    "\n",
    "    if options['use_dropout']:\n",
    "        proj = dropout_layer(proj, use_noise, trng, drop_p=0.5)\n",
    "\n",
    "    ytem = T.dot(tparams['Wemb'], tparams['bili'])\n",
    "    pred = T.nnet.softmax(T.dot(proj, ytem.T))\n",
    "    # pred = T.nnet.softmax(T.dot(proj, tparams['U']) + tparams['b'])\n",
    "\n",
    "    f_pred_prob = theano.function([x, mask], pred, name='f_pred_prob')\n",
    "    # f_weight = theano.function([x, mask], weight, name='f_weight')\n",
    "\n",
    "    off = 1e-8\n",
    "    if pred.dtype == 'float16':\n",
    "        off = 1e-6\n",
    "\n",
    "    cost = -T.log(pred[T.arange(n_samples), y] + off).mean()\n",
    "\n",
    "    return use_noise, x, mask, y, f_pred_prob, cost\n",
    "\n",
    "\n",
    "def pred_evaluation(f_pred_prob, prepare_data, data, iterator):\n",
    "    \"\"\"\n",
    "    Compute recall@20 and mrr@20\n",
    "    f_pred_prob: Theano fct computing the prediction\n",
    "    prepare_data: usual prepare_data for that dataset.\n",
    "    \"\"\"\n",
    "    recall = 0.0\n",
    "    mrr = 0.0\n",
    "    evalutation_point_count = 0\n",
    "    ranks = \"never entered for-loop\"\n",
    "    # pred_res = []\n",
    "    # att = []\n",
    "\n",
    "    for _, valid_index in iterator:\n",
    "        x, mask, y = prepare_data([data[0][t] for t in valid_index], [data[1][t] for t in valid_index])\n",
    "        preds = f_pred_prob(x, mask)\n",
    "        # weights = f_weight(x, mask)\n",
    "        targets = y\n",
    "\n",
    "        ranks = (preds.T > np.diag(preds.T[targets])).sum(axis=0) + 1\n",
    "#         if math.isnan(ranks):\n",
    "#             print(preds)\n",
    "#             print(targets)\n",
    "        rank_ok = (ranks <= 20)\n",
    "        # pred_res += list(rank_ok)\n",
    "        recall += rank_ok.sum()\n",
    "        mrr += (1.0 / ranks[rank_ok]).sum()\n",
    "        evalutation_point_count += len(ranks)\n",
    "        # att.append(weights)\n",
    "\n",
    "    recall = numpy_floatX(recall) / evalutation_point_count\n",
    "    mrr = numpy_floatX(mrr) / evalutation_point_count\n",
    "    eval_score = (recall, mrr)\n",
    "\n",
    "    # ff = open('/storage/lijing/mydataset/res_attention_correct.pkl', 'wb')\n",
    "    # pickle.dump(pred_res, ff)\n",
    "    # ff.close()\n",
    "    # ff2 = open('/storage/lijing/mydataset/attention_weights.pkl', 'wb')\n",
    "    # pickle.dump(att, ff2)\n",
    "    # ff2.close()\n",
    "\n",
    "    return eval_score\n",
    "\n",
    "\n",
    "def train_gru(\n",
    "    dim_proj=50,  # word embeding dimension\n",
    "    hidden_units=100,  # GRU number of hidden units.\n",
    "    patience=100,  # Number of epoch to wait before early stop if no progress\n",
    "    max_epochs=30,  # The maximum number of epoch to run\n",
    "    dispFreq=100,  # Display to stdout the training progress every N updates\n",
    "    lrate=0.001,  # Learning rate\n",
    "    n_items=37484,  # Vocabulary size\n",
    "    encoder='gru',  # TODO: can be removed must be gru.\n",
    "    saveto='gru_model.npz',  # The best model will be saved there\n",
    "    is_valid=True,  # Compute the validation error after this number of update.\n",
    "    is_save=False,  # Save the parameters after every saveFreq updates\n",
    "    batch_size=512,  # The batch size during training.\n",
    "    valid_batch_size=512,  # The batch size used for validation/test set.\n",
    "    dataset='rsc2015',\n",
    "\n",
    "    # Parameter for extra option\n",
    "    use_dropout=True,  # if False slightly faster, but worst test error\n",
    "                       # This frequently need a bigger model.\n",
    "    reload_model=None,  # Path to a saved model we want to start from.\n",
    "    test_size=-1,  # If >0, we keep only this number of test example.\n",
    "):\n",
    "\n",
    "    # Model options\n",
    "    model_options = locals().copy()\n",
    "    print(\"model options\", model_options)\n",
    "\n",
    "    load_data, prepare_data = get_dataset(dataset)\n",
    "\n",
    "    print('Loading data')\n",
    "    train, valid, test = load_data()\n",
    "\n",
    "    print('Building model')\n",
    "    # This create the initial parameters as numpy ndarrays.\n",
    "    # Dict name (string) -> numpy ndarray\n",
    "    params = init_params(model_options)\n",
    "\n",
    "    if reload_model:\n",
    "        load_params('gru_model.npz', params)\n",
    "\n",
    "    # This create Theano Shared Variable from the parameters.\n",
    "    # Dict name (string) -> Theano Tensor Shared Variable\n",
    "    # params and tparams have different copy of the weights.\n",
    "    tparams = init_tparams(params)\n",
    "\n",
    "    # use_noise is for dropout\n",
    "    (use_noise, x, mask,\n",
    "     y, f_pred_prob, cost) = build_model(tparams, model_options)\n",
    "\n",
    "    all_params = list(tparams.values())\n",
    "\n",
    "    updates = adam(cost, all_params, lrate)\n",
    "\n",
    "    train_function = theano.function(inputs=[x, mask, y], outputs=cost, updates=updates)\n",
    "\n",
    "    print('Optimization')\n",
    "\n",
    "    kf_valid = get_minibatches_idx(len(valid[0]), valid_batch_size)\n",
    "    kf_test = get_minibatches_idx(len(test[0]), valid_batch_size)\n",
    "\n",
    "    print(\"%d train examples\" % len(train[0]))\n",
    "    print(\"%d valid examples\" % len(valid[0]))\n",
    "    print(\"%d test examples\" % len(test[0]))\n",
    "\n",
    "    history_errs = []\n",
    "    history_vali = []\n",
    "    best_p = None\n",
    "    bad_count = 0\n",
    "\n",
    "    uidx = 0  # the number of update done\n",
    "    estop = False  # early stop\n",
    "\n",
    "    try:\n",
    "        for eidx in range(max_epochs):\n",
    "            start_time = time.time()\n",
    "            n_samples = 0\n",
    "            epoch_loss = []\n",
    "\n",
    "            # Get new shuffled index for the training set.\n",
    "            kf = get_minibatches_idx(len(train[0]), batch_size, shuffle=True)\n",
    "\n",
    "            for _, train_index in kf:\n",
    "                uidx += 1\n",
    "                use_noise.set_value(1.)\n",
    "\n",
    "                # Select the random examples for this minibatch\n",
    "                y = [train[1][t] for t in train_index]\n",
    "                x = [train[0][t]for t in train_index]\n",
    "                \n",
    "                \n",
    "                # Get the data in numpy.ndarray format\n",
    "                # This swap the axis!\n",
    "                # Return something of shape (minibatch maxlen, n samples)\n",
    "                x, mask, y = prepare_data(x, y)\n",
    "                n_samples += x.shape[1]\n",
    "\n",
    "                loss = train_function(x, mask, y)\n",
    "                epoch_loss.append(loss)\n",
    "\n",
    "                if np.isnan(loss) or np.isinf(loss):\n",
    "                    print('bad loss detected: ', loss)\n",
    "                    return 1., 1., 1.\n",
    "\n",
    "                if np.mod(uidx, dispFreq) == 0:\n",
    "                    print('Epoch ', eidx, 'Update ', uidx, 'Loss ', np.mean(epoch_loss))\n",
    "\n",
    "            if saveto and is_save:\n",
    "                print('Saving...')\n",
    "\n",
    "                if best_p is not None:\n",
    "                    params = best_p\n",
    "                else:\n",
    "                    params = unzip(tparams)\n",
    "                np.savez(saveto, history_errs=history_errs, **params)\n",
    "                print('Saving done')\n",
    "\n",
    "            if is_valid:\n",
    "                use_noise.set_value(0.)\n",
    "                kf_valid = get_minibatches_idx(len(valid[0]), valid_batch_size)\n",
    "                kf_test = get_minibatches_idx(len(test[0]), valid_batch_size)\n",
    "\n",
    "                valid_evaluation = pred_evaluation(f_pred_prob, prepare_data, valid, kf_valid)\n",
    "                test_evaluation = pred_evaluation(f_pred_prob, prepare_data, test, kf_test)\n",
    "                history_errs.append([valid_evaluation, test_evaluation])\n",
    "\n",
    "                if best_p is None or valid_evaluation[0] >= np.array(history_vali).max():\n",
    "\n",
    "                    best_p = unzip(tparams)\n",
    "                    print('Best perfomance updated!')\n",
    "                    bad_count = 0\n",
    "\n",
    "                print('Valid Recall@20:', valid_evaluation[0], '   Valid Mrr@20:', valid_evaluation[1],\n",
    "                      '\\nTest Recall@20', test_evaluation[0], '   Test Mrr@20:', test_evaluation[1])\n",
    "\n",
    "                if len(history_vali) > 10 and valid_evaluation[0] <= np.array(history_vali).max():\n",
    "                    bad_count += 1\n",
    "                    print('===========================>Bad counter: ' + str(bad_count))\n",
    "                    print('current validation recall: ' + str(valid_evaluation[0]) +\n",
    "                          '      history max recall:' + str(np.array(history_vali).max()))\n",
    "                    if bad_count > patience:\n",
    "                        print('Early Stop!')\n",
    "                        estop = True\n",
    "\n",
    "                history_vali.append(valid_evaluation[0])\n",
    "\n",
    "            end_time = time.time()\n",
    "            print('Seen %d samples' % n_samples)\n",
    "            print(('This epoch took %.1fs' % (end_time - start_time)), file=sys.stderr)\n",
    "\n",
    "            if estop:\n",
    "                break\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Training interupted\")\n",
    "\n",
    "    if best_p is not None:\n",
    "        zipp(best_p, tparams)\n",
    "    else:\n",
    "        best_p = unzip(tparams)\n",
    "\n",
    "    use_noise.set_value(0.)\n",
    "    kf_valid = get_minibatches_idx(len(valid[0]), valid_batch_size)\n",
    "    kf_test = get_minibatches_idx(len(test[0]), valid_batch_size)\n",
    "    valid_evaluation = pred_evaluation(f_pred_prob, prepare_data, valid, kf_valid)\n",
    "    test_evaluation = pred_evaluation(f_pred_prob,  prepare_data, test, kf_test)\n",
    "\n",
    "    print('=================Best performance=================')\n",
    "    print('Valid Recall@20:', valid_evaluation[0], '   Valid Mrr@20:', valid_evaluation[1],\n",
    "          '\\nTest Recall@20', test_evaluation[0], '   Test Mrr@20:', test_evaluation[1])\n",
    "    print('==================================================')\n",
    "    if saveto and is_save:\n",
    "        np.savez('Best_performance', valid_evaluation=valid_evaluation, test_evaluation=test_evaluation, history_errs=history_errs,\n",
    "                 **best_p)\n",
    "\n",
    "    return valid_evaluation, test_evaluation\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # See function train for all possible parameter and there definition.\n",
    "    eval_valid, eval_test = train_gru(max_epochs=10,test_size=-1)\n",
    "    \n",
    "    \n",
    "    \n",
    "#    dim_proj=50,  # word embeding dimension\n",
    "#     hidden_units=100,  # GRU number of hidden units.\n",
    "#     patience=100,  # Number of epoch to wait before early stop if no progress\n",
    "#     max_epochs=30,  # The maximum number of epoch to run\n",
    "#     dispFreq=100,  # Display to stdout the training progress every N updates\n",
    "#     lrate=0.001,  # Learning rate\n",
    "#     n_items=37484,  # Vocabulary size\n",
    "#     encoder='gru',  # TODO: can be removed must be gru.\n",
    "#     saveto='gru_model.npz',  # The best model will be saved there\n",
    "#     is_valid=True,  # Compute the validation error after this number of update.\n",
    "#     is_save=False,  # Save the parameters after every saveFreq updates\n",
    "#     batch_size=512,  # The batch size during training.\n",
    "#     valid_batch_size=512,  # The batch size used for validation/test set.\n",
    "#     dataset='rsc2015',\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
