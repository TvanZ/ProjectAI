{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import csv\n",
    "import pickle\n",
    "import os\n",
    "import operator\n",
    "import pandas\n",
    "from __future__ import print_function\n",
    "\n",
    "\n",
    "from collections import Counter, OrderedDict, defaultdict, namedtuple\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import theano\n",
    "from theano import config\n",
    "import theano.tensor as T\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = pandas.read_pickle(\"./data/processedData.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2268318, 2333346], [4365585, 230380], [2951368, 3108797], [2734026, 4152983, 266784, 266784, 1305059], [2087357, 3157558], [2087357, 1340922, 4954999], [3219016, 2028434, 3219016], [4954999, 818610, 271696]]\n"
     ]
    }
   ],
   "source": [
    "userBase = sample.groupby(['USERID', 'SESSION'])['PRODUCTID'].apply(list).groupby('USERID').apply(list)\n",
    "print(userBase[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3219016, 2028434]\n",
      "\n",
      "[4954999, 818610]\n"
     ]
    }
   ],
   "source": [
    "# More efficient create examples function\n",
    "# A simple way to define a class is using namedtuple.\n",
    "Example = namedtuple(\"Example\", [\"inputs\", \"target\"])\n",
    "\n",
    "def f(userid, sessions, train):\n",
    "    #print(sessions)\n",
    "    non_empty_sessions = [ses for ses in sessions if len(ses) >= 2]\n",
    "    if len(non_empty_sessions) < 3:\n",
    "        print('Not enough sessions')\n",
    "        return\n",
    "    if train:\n",
    "        object_train = Example(\n",
    "                               inputs = non_empty_sessions[-2][:-1], target = non_empty_sessions[-2][1:])\n",
    "        return object_train\n",
    "    else:\n",
    "        return Example(\n",
    "                       inputs = non_empty_sessions[-1][:-1], \n",
    "                       target = non_empty_sessions[-1][1:])\n",
    "\n",
    "def createExamples(userBase):\n",
    "    ''' Create training and testing set '''\n",
    "    userBase = pandas.DataFrame(userBase)\n",
    "    userBase.reset_index(level = 0, inplace = True)\n",
    "    trainData = [x for x in \n",
    "                 userBase.apply(lambda x: f(x['USERID'], x['PRODUCTID'], True), axis = 1).tolist() \n",
    "                 if x is not None]\n",
    "    testData = [x for x in \n",
    "                userBase.apply(lambda x: f(x['USERID'], x['PRODUCTID'], False), axis = 1).tolist() \n",
    "                if x is not None]\n",
    "    return trainData, testData\n",
    "\n",
    "trainData, testData = createExamples(userBase)\n",
    "\n",
    "\n",
    "narmTrain = ([example.inputs for example in trainData],[example.target for example in trainData]) \n",
    "narmTest = ([example.inputs for example in testData],[example.target for example in testData])\n",
    "\n",
    "print(narmTrain[0][0])\n",
    "print('')\n",
    "print(narmTest[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "def prepare_data(seqs, labels):\n",
    "    \"\"\"Create the matrices from the datasets.\n",
    "\n",
    "    This pad each sequence to the same lenght: the lenght of the\n",
    "    longuest sequence or maxlen.\n",
    "\n",
    "    if maxlen is set, we will cut all sequence to this maximum\n",
    "    lenght.\n",
    "\n",
    "    This swap the axis!\n",
    "    \"\"\"\n",
    "    # x: a list of sentences\n",
    "\n",
    "    lengths = [len(s) for s in seqs]\n",
    "    n_samples = len(seqs)\n",
    "    maxlen = np.max(lengths)\n",
    "\n",
    "    x = np.zeros((maxlen, n_samples)).astype('int64')\n",
    "    x_mask = np.ones((maxlen, n_samples)).astype(theano.config.floatX)\n",
    "    for idx, s in enumerate(seqs):\n",
    "        x[:lengths[idx], idx] = s\n",
    "\n",
    "    x_mask *= (1 - (x == 0))\n",
    "\n",
    "    return x, x_mask, labels\n",
    "\n",
    "\n",
    "def load_data(valid_portion=0.1, maxlen=19, sort_by_len=False):\n",
    "    '''Loads the dataset\n",
    "\n",
    "    :type path: String\n",
    "    :param path: The path to the dataset (here RSC2015)\n",
    "    :type n_items: int\n",
    "    :param n_items: The number of items.\n",
    "    :type valid_portion: float\n",
    "    :param valid_portion: The proportion of the full train set used for\n",
    "        the validation set.\n",
    "    :type maxlen: None or positive int\n",
    "    :param maxlen: the max sequence length we use in the train/valid set.\n",
    "    :type sort_by_len: bool\n",
    "    :name sort_by_len: Sort by the sequence lenght for the train,\n",
    "        valid and test set. This allow faster execution as it cause\n",
    "        less padding per minibatch. Another mechanism must be used to\n",
    "        shuffle the train set at each epoch.\n",
    "\n",
    "    '''\n",
    "\n",
    "    #############\n",
    "    # LOAD DATA #\n",
    "    #############\n",
    "\n",
    "    # Load the dataset\n",
    "    #path_train_data = './data/processedData.pkl'\n",
    "    #path_test_data = './data/processedData.pkl'\n",
    "\n",
    "    #f1 = open(path_train_data, 'rb')\n",
    "    train_set = narmTrain\n",
    "    #f1.close()\n",
    "\n",
    "    #f2 = open(path_test_data, 'rb')\n",
    "    test_set = narmTest\n",
    "    #f2.close()\n",
    "\n",
    "    if maxlen:\n",
    "        new_train_set_x = []\n",
    "        new_train_set_y = []\n",
    "        for x, y in zip(train_set[0], train_set[1]):\n",
    "            if len(x) < maxlen:\n",
    "                new_train_set_x.append(x)\n",
    "                new_train_set_y.append(y)\n",
    "            else:\n",
    "                new_train_set_x.append(x[:maxlen])\n",
    "                new_train_set_y.append(y)\n",
    "        train_set = (new_train_set_x, new_train_set_y)\n",
    "        del new_train_set_x, new_train_set_y\n",
    "\n",
    "        new_test_set_x = []\n",
    "        new_test_set_y = []\n",
    "        for xx, yy in zip(test_set[0], test_set[1]):\n",
    "            if len(xx) < maxlen:\n",
    "                new_test_set_x.append(xx)\n",
    "                new_test_set_y.append(yy)\n",
    "            else:\n",
    "                new_test_set_x.append(xx[:maxlen])\n",
    "                new_test_set_y.append(yy)\n",
    "        test_set = (new_test_set_x, new_test_set_y)\n",
    "        del new_test_set_x, new_test_set_y\n",
    "\n",
    "    # split training set into validation set\n",
    "    train_set_x, train_set_y = train_set\n",
    "    n_samples = len(train_set_x)\n",
    "    sidx = np.arange(n_samples, dtype='int32')\n",
    "    np.random.shuffle(sidx)\n",
    "    n_train = int(np.round(n_samples * (1. - valid_portion)))\n",
    "    valid_set_x = [train_set_x[s] for s in sidx[n_train:]]\n",
    "    valid_set_y = [train_set_y[s] for s in sidx[n_train:]]\n",
    "    train_set_x = [train_set_x[s] for s in sidx[:n_train]]\n",
    "    train_set_y = [train_set_y[s] for s in sidx[:n_train]]\n",
    "\n",
    "    train_set = (train_set_x, train_set_y)\n",
    "    valid_set = (valid_set_x, valid_set_y)\n",
    "\n",
    "    test_set_x, test_set_y = test_set\n",
    "    valid_set_x, valid_set_y = valid_set\n",
    "    train_set_x, train_set_y = train_set\n",
    "\n",
    "    def len_argsort(seq):\n",
    "        return sorted(range(len(seq)), key=lambda x: len(seq[x]))\n",
    "\n",
    "    if sort_by_len:\n",
    "        sorted_index = len_argsort(test_set_x)\n",
    "        test_set_x = [test_set_x[i] for i in sorted_index]\n",
    "        test_set_y = [test_set_y[i] for i in sorted_index]\n",
    "\n",
    "        sorted_index = len_argsort(valid_set_x)\n",
    "        valid_set_x = [valid_set_x[i] for i in sorted_index]\n",
    "        valid_set_y = [valid_set_y[i] for i in sorted_index]\n",
    "\n",
    "    train = (train_set_x, train_set_y)\n",
    "    valid = (valid_set_x, valid_set_y)\n",
    "    test = (test_set_x, test_set_y)\n",
    "\n",
    "    return train, valid, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model options {'dim_proj': 50, 'hidden_units': 100, 'patience': 100, 'max_epochs': 30, 'dispFreq': 100, 'lrate': 0.001, 'n_items': 37484, 'encoder': 'gru', 'saveto': 'gru_model.npz', 'is_valid': True, 'is_save': False, 'batch_size': 512, 'valid_batch_size': 512, 'dataset': 'rsc2015', 'use_dropout': True, 'reload_model': None, 'test_size': -1}\n",
      "Loading data\n",
      "Building model\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-db97b1d9787a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    506\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m     \u001b[0;31m# See function train for all possible parameter and there definition.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 508\u001b[0;31m     \u001b[0meval_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_gru\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-db97b1d9787a>\u001b[0m in \u001b[0;36mtrain_gru\u001b[0;34m(dim_proj, hidden_units, patience, max_epochs, dispFreq, lrate, n_items, encoder, saveto, is_valid, is_save, batch_size, valid_batch_size, dataset, use_dropout, reload_model, test_size)\u001b[0m\n\u001b[1;32m    379\u001b[0m     \u001b[0;31m# use_noise is for dropout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m     (use_noise, x, mask,\n\u001b[0;32m--> 381\u001b[0;31m      y, f_pred_prob, cost) = build_model(tparams, model_options)\n\u001b[0m\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0mall_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-db97b1d9787a>\u001b[0m in \u001b[0;36mbuild_model\u001b[0;34m(tparams, options)\u001b[0m\n\u001b[1;32m    280\u001b[0m     \u001b[0;31m# pred = T.nnet.softmax(T.dot(proj, tparams['U']) + tparams['b'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m     \u001b[0mf_pred_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'f_pred_prob'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m     \u001b[0;31m# f_weight = theano.function([x, mask], weight, name='f_weight')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/theano/compile/function.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(inputs, outputs, mode, updates, givens, no_default_updates, accept_inplace, name, rebuild_strict, allow_input_downcast, profile, on_unused_input)\u001b[0m\n\u001b[1;32m    315\u001b[0m                    \u001b[0mon_unused_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mon_unused_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m                    \u001b[0mprofile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m                    output_keys=output_keys)\n\u001b[0m\u001b[1;32m    318\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/theano/compile/pfunc.py\u001b[0m in \u001b[0;36mpfunc\u001b[0;34m(params, outputs, mode, updates, givens, no_default_updates, accept_inplace, name, rebuild_strict, allow_input_downcast, profile, on_unused_input, output_keys)\u001b[0m\n\u001b[1;32m    484\u001b[0m                          \u001b[0maccept_inplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_inplace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m                          \u001b[0mprofile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_unused_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mon_unused_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m                          output_keys=output_keys)\n\u001b[0m\u001b[1;32m    487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36morig_function\u001b[0;34m(inputs, outputs, mode, accept_inplace, name, profile, on_unused_input, output_keys)\u001b[0m\n\u001b[1;32m   1839\u001b[0m                   name=name)\n\u001b[1;32m   1840\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchange_flags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompute_test_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"off\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1841\u001b[0;31m             \u001b[0mfn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1842\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1843\u001b[0m         \u001b[0mt2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, input_storage, trustme, storage_map)\u001b[0m\n\u001b[1;32m   1713\u001b[0m             \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraceback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlimit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraceback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile_limit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1714\u001b[0m             _fn, _i, _o = self.linker.make_thunk(\n\u001b[0;32m-> 1715\u001b[0;31m                 input_storage=input_storage_lists, storage_map=storage_map)\n\u001b[0m\u001b[1;32m   1716\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1717\u001b[0m             \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraceback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlimit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlimit_orig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/theano/gof/link.py\u001b[0m in \u001b[0;36mmake_thunk\u001b[0;34m(self, input_storage, output_storage, storage_map)\u001b[0m\n\u001b[1;32m    697\u001b[0m         return self.make_all(input_storage=input_storage,\n\u001b[1;32m    698\u001b[0m                              \u001b[0moutput_storage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_storage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m                              storage_map=storage_map)[:3]\n\u001b[0m\u001b[1;32m    700\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmake_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/theano/gof/vm.py\u001b[0m in \u001b[0;36mmake_all\u001b[0;34m(self, profiler, input_storage, output_storage, storage_map)\u001b[0m\n\u001b[1;32m   1089\u001b[0m                                                  \u001b[0mcompute_map\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1090\u001b[0m                                                  \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1091\u001b[0;31m                                                  impl=impl))\n\u001b[0m\u001b[1;32m   1092\u001b[0m                 \u001b[0mlinker_make_thunk_time\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mthunk_start\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1093\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthunks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lazy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/theano/gof/op.py\u001b[0m in \u001b[0;36mmake_thunk\u001b[0;34m(self, node, storage_map, compute_map, no_recycling, impl)\u001b[0m\n\u001b[1;32m    953\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m                 return self.make_c_thunk(node, storage_map, compute_map,\n\u001b[0;32m--> 955\u001b[0;31m                                          no_recycling)\n\u001b[0m\u001b[1;32m    956\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mNotImplementedError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMethodNotDefined\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m                 \u001b[0;31m# We requested the c code, so don't catch the error.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/theano/gof/op.py\u001b[0m in \u001b[0;36mmake_c_thunk\u001b[0;34m(self, node, storage_map, compute_map, no_recycling)\u001b[0m\n\u001b[1;32m    856\u001b[0m         \u001b[0m_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Trying CLinker.make_thunk'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m         outputs = cl.make_thunk(input_storage=node_input_storage,\n\u001b[0;32m--> 858\u001b[0;31m                                 output_storage=node_output_storage)\n\u001b[0m\u001b[1;32m    859\u001b[0m         \u001b[0mthunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_input_filters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_output_filters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/theano/gof/cc.py\u001b[0m in \u001b[0;36mmake_thunk\u001b[0;34m(self, input_storage, output_storage, storage_map, keep_lock)\u001b[0m\n\u001b[1;32m   1215\u001b[0m         cthunk, module, in_storage, out_storage, error_storage = self.__compile__(\n\u001b[1;32m   1216\u001b[0m             \u001b[0minput_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_map\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m             keep_lock=keep_lock)\n\u001b[0m\u001b[1;32m   1218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_CThunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcthunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_tasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/theano/gof/cc.py\u001b[0m in \u001b[0;36m__compile__\u001b[0;34m(self, input_storage, output_storage, storage_map, keep_lock)\u001b[0m\n\u001b[1;32m   1155\u001b[0m                                             \u001b[0moutput_storage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1156\u001b[0m                                             \u001b[0mstorage_map\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1157\u001b[0;31m                                             keep_lock=keep_lock)\n\u001b[0m\u001b[1;32m   1158\u001b[0m         return (thunk,\n\u001b[1;32m   1159\u001b[0m                 \u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/theano/gof/cc.py\u001b[0m in \u001b[0;36mcthunk_factory\u001b[0;34m(self, error_storage, in_storage, out_storage, storage_map, keep_lock)\u001b[0m\n\u001b[1;32m   1622\u001b[0m                 \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1623\u001b[0m             module = get_module_cache().module_from_key(\n\u001b[0;32m-> 1624\u001b[0;31m                 key=key, lnk=self, keep_lock=keep_lock)\n\u001b[0m\u001b[1;32m   1625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1626\u001b[0m         \u001b[0mvars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morphans\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/theano/gof/cmodule.py\u001b[0m in \u001b[0;36mmodule_from_key\u001b[0;34m(self, key, lnk, keep_lock)\u001b[0m\n\u001b[1;32m   1187\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m                 \u001b[0mlocation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdlimport_workdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1189\u001b[0;31m                 \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlnk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile_cmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1190\u001b[0m                 \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m                 \u001b[0;32massert\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/theano/gof/cc.py\u001b[0m in \u001b[0;36mcompile_cmodule\u001b[0;34m(self, location)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0mlib_dirs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib_dirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 \u001b[0mlibs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlibs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1527\u001b[0;31m                 preargs=preargs)\n\u001b[0m\u001b[1;32m   1528\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m             \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/theano/gof/cmodule.py\u001b[0m in \u001b[0;36mcompile_str\u001b[0;34m(module_name, src_code, location, include_dirs, lib_dirs, libs, preargs, py_module, hide_symbols)\u001b[0m\n\u001b[1;32m   2349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2350\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2351\u001b[0;31m             \u001b[0mp_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_subprocess_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2352\u001b[0m             \u001b[0mcompile_stderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2353\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/theano/misc/windows.py\u001b[0m in \u001b[0;36moutput_subprocess_Popen\u001b[0;34m(command, **params)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;31m# we need to use communicate to make sure we don't deadlock around\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;31m# the stdout/stderr pipe.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36mcommunicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 920\u001b[0;31m                 \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_communicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    921\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m                 \u001b[0;31m# https://bugs.python.org/issue25942\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36m_communicate\u001b[0;34m(self, input, endtime, orig_timeout)\u001b[0m\n\u001b[1;32m   1656\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutExpired\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1658\u001b[0;31m                     \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1659\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''\n",
    "Build NARM model\n",
    "'''\n",
    "\n",
    "\n",
    "datasets = {'rsc2015': (load_data, prepare_data)}\n",
    "\n",
    "#datasets = {'rsc2015': (narmTrain, narmTest)}\n",
    "\n",
    "# Set the random number generators' seeds for consistency\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "\n",
    "def numpy_floatX(data):\n",
    "    return np.asarray(data, dtype=config.floatX)\n",
    "\n",
    "\n",
    "def get_minibatches_idx(n, minibatch_size, shuffle=False):\n",
    "    \"\"\"\n",
    "    Used to shuffle the dataset at each iteration.\n",
    "    \"\"\"\n",
    "\n",
    "    idx_list = np.arange(n, dtype=\"int32\")\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.shuffle(idx_list)\n",
    "\n",
    "    minibatches = []\n",
    "    minibatch_start = 0\n",
    "    for i in range(n // minibatch_size):\n",
    "        minibatches.append(idx_list[minibatch_start:\n",
    "                                    minibatch_start + minibatch_size])\n",
    "        minibatch_start += minibatch_size\n",
    "\n",
    "    if minibatch_start != n:\n",
    "        # Make a minibatch out of what is left\n",
    "        minibatches.append(idx_list[minibatch_start:])\n",
    "\n",
    "    return zip(range(len(minibatches)), minibatches)\n",
    "\n",
    "\n",
    "def get_dataset(name):\n",
    "    return datasets[name][0], datasets[name][1]\n",
    "\n",
    "\n",
    "def zipp(params, tparams):\n",
    "    \"\"\"\n",
    "    When we reload the model. Needed for the GPU stuff.\n",
    "    \"\"\"\n",
    "    for kk, vv in params.items():\n",
    "        tparams[kk].set_value(vv)\n",
    "\n",
    "\n",
    "def unzip(zipped):\n",
    "    \"\"\"\n",
    "    When we pickle the model. Needed for the GPU stuff.\n",
    "    \"\"\"\n",
    "    new_params = OrderedDict()\n",
    "    for kk, vv in zipped.items():\n",
    "        new_params[kk] = vv.get_value()\n",
    "    return new_params\n",
    "\n",
    "\n",
    "def dropout_layer(state_before, use_noise, trng, drop_p=0.5):\n",
    "    retain = 1. - drop_p\n",
    "    proj = T.switch(use_noise, (state_before * trng.binomial(state_before.shape,\n",
    "                                                             p=retain, n=1,\n",
    "                                                             dtype=state_before.dtype)), state_before * retain)\n",
    "    return proj\n",
    "\n",
    "\n",
    "def _p(pp, name):\n",
    "    return '%s_%s' % (pp, name)\n",
    "\n",
    "\n",
    "def init_params(options):\n",
    "    \"\"\"\n",
    "    Global (not GRU) parameter. For the embeding and the classifier.\n",
    "    \"\"\"\n",
    "    params = OrderedDict()\n",
    "    # embedding\n",
    "    params['Wemb'] = init_weights((options['n_items'], options['dim_proj']))\n",
    "    params = get_layer(options['encoder'])[0](options,\n",
    "                                              params,\n",
    "                                              prefix=options['encoder'])\n",
    "    # attention\n",
    "    params['W_encoder'] = init_weights((options['hidden_units'], options['hidden_units']))\n",
    "    params['W_decoder'] = init_weights((options['hidden_units'], options['hidden_units']))\n",
    "    params['bl_vector'] = init_weights((1, options['hidden_units']))\n",
    "    # classifier\n",
    "    # params['U'] = init_weights((2*options['hidden_units'], options['n_items']))\n",
    "    # params['b'] = np.zeros((options['n_items'],)).astype(config.floatX)\n",
    "    params['bili'] = init_weights((options['dim_proj'], 2 * options['hidden_units']))\n",
    "\n",
    "    return params\n",
    "\n",
    "\n",
    "def load_params(path, params):\n",
    "    pp = np.load(path)\n",
    "    for kk, vv in params.items():\n",
    "        if kk not in pp:\n",
    "            raise Warning('%s is not in the archive' % kk)\n",
    "        params[kk] = pp[kk]\n",
    "\n",
    "    return params\n",
    "\n",
    "\n",
    "def init_tparams(params):\n",
    "    tparams = OrderedDict()\n",
    "    for kk, pp in params.items():\n",
    "        tparams[kk] = theano.shared(params[kk], name=kk)\n",
    "    return tparams\n",
    "\n",
    "\n",
    "def get_layer(name):\n",
    "    fns = layers[name]\n",
    "    return fns\n",
    "\n",
    "\n",
    "def init_weights(shape):\n",
    "    sigma = np.sqrt(2. / shape[0])\n",
    "    return numpy_floatX(np.random.randn(*shape) * sigma)\n",
    "\n",
    "\n",
    "def ortho_weight(ndim):\n",
    "    W = np.random.randn(ndim, ndim)\n",
    "    u, s, v = np.linalg.svd(W)\n",
    "    return u.astype(config.floatX)\n",
    "\n",
    "\n",
    "def param_init_gru(options, params, prefix='gru'):\n",
    "    \"\"\"\n",
    "    Init the GRU parameter:\n",
    "\n",
    "    :see: init_params\n",
    "    \"\"\"\n",
    "    Wxrz = np.concatenate([init_weights((options['dim_proj'], options['hidden_units'])),\n",
    "                           init_weights((options['dim_proj'], options['hidden_units'])),\n",
    "                           init_weights((options['dim_proj'], options['hidden_units']))], axis=1)\n",
    "    params[_p(prefix, 'Wxrz')] = Wxrz\n",
    "\n",
    "    Urz = np.concatenate([ortho_weight(options['hidden_units']),\n",
    "                          ortho_weight(options['hidden_units'])], axis=1)\n",
    "    params[_p(prefix, 'Urz')] = Urz\n",
    "\n",
    "    Uh = ortho_weight(options['hidden_units'])\n",
    "    params[_p(prefix, 'Uh')] = Uh\n",
    "\n",
    "    b = np.zeros((3 * options['hidden_units'],))\n",
    "    params[_p(prefix, 'b')] = b.astype(config.floatX)\n",
    "    return params\n",
    "\n",
    "\n",
    "def gru_layer(tparams, state_below, options, prefix='gru', mask=None):\n",
    "    nsteps = state_below.shape[0]\n",
    "    if state_below.ndim == 3:\n",
    "        n_samples = state_below.shape[1]\n",
    "    else:\n",
    "        n_samples = 1\n",
    "\n",
    "    assert mask is not None\n",
    "\n",
    "    def _slice(_x, n, dim):\n",
    "        if _x.ndim == 3:\n",
    "            return _x[:, :, n * dim:(n + 1) * dim]\n",
    "        return _x[:, n * dim:(n + 1) * dim]\n",
    "\n",
    "    def _step(m_, x_, h_):\n",
    "        preact = T.dot(h_, tparams[_p(prefix, 'Urz')])\n",
    "        preact += x_[:, 0:2 * options['hidden_units']]\n",
    "\n",
    "        z = T.nnet.hard_sigmoid(_slice(preact, 0, options['hidden_units']))\n",
    "        r = T.nnet.hard_sigmoid(_slice(preact, 1, options['hidden_units']))\n",
    "        h = T.tanh(T.dot((h_ * r), tparams[_p(prefix, 'Uh')]) + _slice(x_, 2, options['hidden_units']))\n",
    "\n",
    "        h = (1.0 - z) * h_ + z * h\n",
    "        h = m_[:, None] * h + (1. - m_)[:, None] * h_\n",
    "\n",
    "        return h\n",
    "\n",
    "    state_below = (T.dot(state_below, tparams[_p(prefix, 'Wxrz')]) +\n",
    "                   tparams[_p(prefix, 'b')])\n",
    "\n",
    "    hidden_units = options['hidden_units']\n",
    "    rval, updates = theano.scan(_step,\n",
    "                                sequences=[mask, state_below],\n",
    "                                outputs_info=T.alloc(numpy_floatX(0.), n_samples, hidden_units),\n",
    "                                name=_p(prefix, '_layers'),\n",
    "                                n_steps=nsteps)\n",
    "    return rval\n",
    "\n",
    "layers = {'gru': (param_init_gru, gru_layer)}\n",
    "\n",
    "\n",
    "def adam(loss, all_params, learning_rate=0.001, b1=0.9, b2=0.999, e=1e-8, gamma=1-1e-8):\n",
    "    \"\"\"\n",
    "    ADAM update rules\n",
    "    Default values are taken from [Kingma2014]\n",
    "\n",
    "    References:\n",
    "    [Kingma2014] Kingma, Diederik, and Jimmy Ba.\n",
    "    \"Adam: A Method for Stochastic Optimization.\"\n",
    "    arXiv preprint arXiv:1412.6980 (2014).\n",
    "    http://arxiv.org/pdf/1412.6980v4.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    updates = OrderedDict()\n",
    "    all_grads = theano.grad(loss, all_params)\n",
    "    alpha = learning_rate\n",
    "    t = theano.shared(np.float32(1))\n",
    "    b1_t = b1*gamma**(t-1)   #(Decay the first moment running average coefficient)\n",
    "\n",
    "    for theta_previous, g in zip(all_params, all_grads):\n",
    "        m_previous = theano.shared(np.zeros(theta_previous.get_value().shape, dtype=config.floatX))\n",
    "        v_previous = theano.shared(np.zeros(theta_previous.get_value().shape, dtype=config.floatX))\n",
    "\n",
    "        m = b1_t*m_previous + (1 - b1_t)*g  # (Update biased first moment estimate)\n",
    "        v = b2*v_previous + (1 - b2)*g**2   # (Update biased second raw moment estimate)\n",
    "        m_hat = m / (1-b1**t)               # (Compute bias-corrected first moment estimate)\n",
    "        v_hat = v / (1-b2**t)               # (Compute bias-corrected second raw moment estimate)\n",
    "        theta = theta_previous - (alpha * m_hat) / (T.sqrt(v_hat) + e) #(Update parameters)\n",
    "\n",
    "        updates[m_previous] = m\n",
    "        updates[v_previous] = v\n",
    "        updates[theta_previous] = theta\n",
    "    updates[t] = t + 1.\n",
    "\n",
    "    return updates\n",
    "\n",
    "\n",
    "def build_model(tparams, options):\n",
    "    trng = RandomStreams(SEED)\n",
    "\n",
    "    # Used for dropout.\n",
    "    use_noise = theano.shared(numpy_floatX(0.))\n",
    "\n",
    "    x = T.matrix('x', dtype='int64')\n",
    "    mask = T.matrix('mask', dtype=config.floatX)\n",
    "    y = T.vector('y', dtype='int64')\n",
    "\n",
    "    n_timesteps = x.shape[0]\n",
    "    n_samples = x.shape[1]\n",
    "\n",
    "    emb = tparams['Wemb'][x.flatten()].reshape([n_timesteps,\n",
    "                                                n_samples,\n",
    "                                                options['dim_proj']])\n",
    "    if options['use_dropout']:\n",
    "        emb = dropout_layer(emb, use_noise, trng, drop_p=0.25)\n",
    "\n",
    "    proj = get_layer(options['encoder'])[1](tparams, emb, options,\n",
    "                                            prefix=options['encoder'],\n",
    "                                            mask=mask)\n",
    "\n",
    "    def compute_alpha(state1, state2):\n",
    "        tmp = T.nnet.hard_sigmoid(T.dot(tparams['W_encoder'], state1.T) + T.dot(tparams['W_decoder'], state2.T))\n",
    "        alpha = T.dot(tparams['bl_vector'], tmp)\n",
    "        res = T.sum(alpha, axis=0)\n",
    "        return res\n",
    "\n",
    "    last_h = proj[-1]\n",
    "\n",
    "    sim_matrix, _ = theano.scan(\n",
    "        fn=compute_alpha,\n",
    "        sequences=proj,\n",
    "        non_sequences=proj[-1]\n",
    "    )\n",
    "    att = T.nnet.softmax(sim_matrix.T * mask.T) * mask.T\n",
    "    p = att.sum(axis=1)[:, None]\n",
    "    weight = att / p\n",
    "    atttention_proj = (proj * weight.T[:, :, None]).sum(axis=0)\n",
    "\n",
    "    proj = T.concatenate([atttention_proj, last_h], axis=1)\n",
    "\n",
    "    if options['use_dropout']:\n",
    "        proj = dropout_layer(proj, use_noise, trng, drop_p=0.5)\n",
    "\n",
    "    ytem = T.dot(tparams['Wemb'], tparams['bili'])\n",
    "    pred = T.nnet.softmax(T.dot(proj, ytem.T))\n",
    "    # pred = T.nnet.softmax(T.dot(proj, tparams['U']) + tparams['b'])\n",
    "\n",
    "    f_pred_prob = theano.function([x, mask], pred, name='f_pred_prob')\n",
    "    # f_weight = theano.function([x, mask], weight, name='f_weight')\n",
    "\n",
    "    off = 1e-8\n",
    "    if pred.dtype == 'float16':\n",
    "        off = 1e-6\n",
    "\n",
    "    cost = -T.log(pred[T.arange(n_samples), y] + off).mean()\n",
    "\n",
    "    return use_noise, x, mask, y, f_pred_prob, cost\n",
    "\n",
    "\n",
    "def pred_evaluation(f_pred_prob, prepare_data, data, iterator):\n",
    "    \"\"\"\n",
    "    Compute recall@20 and mrr@20\n",
    "    f_pred_prob: Theano fct computing the prediction\n",
    "    prepare_data: usual prepare_data for that dataset.\n",
    "    \"\"\"\n",
    "    recall = 0.0\n",
    "    mrr = 0.0\n",
    "    evalutation_point_count = 0\n",
    "    # pred_res = []\n",
    "    # att = []\n",
    "\n",
    "    for _, valid_index in iterator:\n",
    "        x, mask, y = prepare_data([data[0][t] for t in valid_index],\n",
    "                                  np.array(data[1])[valid_index])\n",
    "        preds = f_pred_prob(x, mask)\n",
    "        # weights = f_weight(x, mask)\n",
    "        targets = y\n",
    "        ranks = (preds.T > np.diag(preds.T[targets])).sum(axis=0) + 1\n",
    "        rank_ok = (ranks <= 20)\n",
    "        # pred_res += list(rank_ok)\n",
    "        recall += rank_ok.sum()\n",
    "        mrr += (1.0 / ranks[rank_ok]).sum()\n",
    "        evalutation_point_count += len(ranks)\n",
    "        # att.append(weights)\n",
    "\n",
    "    recall = numpy_floatX(recall) / evalutation_point_count\n",
    "    mrr = numpy_floatX(mrr) / evalutation_point_count\n",
    "    eval_score = (recall, mrr)\n",
    "\n",
    "    # ff = open('/storage/lijing/mydataset/res_attention_correct.pkl', 'wb')\n",
    "    # pickle.dump(pred_res, ff)\n",
    "    # ff.close()\n",
    "    # ff2 = open('/storage/lijing/mydataset/attention_weights.pkl', 'wb')\n",
    "    # pickle.dump(att, ff2)\n",
    "    # ff2.close()\n",
    "\n",
    "    return eval_score\n",
    "\n",
    "\n",
    "def train_gru(\n",
    "    dim_proj=50,  # word embeding dimension\n",
    "    hidden_units=100,  # GRU number of hidden units.\n",
    "    patience=100,  # Number of epoch to wait before early stop if no progress\n",
    "    max_epochs=30,  # The maximum number of epoch to run\n",
    "    dispFreq=100,  # Display to stdout the training progress every N updates\n",
    "    lrate=0.001,  # Learning rate\n",
    "    n_items=37484,  # Vocabulary size\n",
    "    encoder='gru',  # TODO: can be removed must be gru.\n",
    "    saveto='gru_model.npz',  # The best model will be saved there\n",
    "    is_valid=True,  # Compute the validation error after this number of update.\n",
    "    is_save=False,  # Save the parameters after every saveFreq updates\n",
    "    batch_size=512,  # The batch size during training.\n",
    "    valid_batch_size=512,  # The batch size used for validation/test set.\n",
    "    dataset='rsc2015',\n",
    "\n",
    "    # Parameter for extra option\n",
    "    use_dropout=True,  # if False slightly faster, but worst test error\n",
    "                       # This frequently need a bigger model.\n",
    "    reload_model=None,  # Path to a saved model we want to start from.\n",
    "    test_size=-1,  # If >0, we keep only this number of test example.\n",
    "):\n",
    "\n",
    "    # Model options\n",
    "    model_options = locals().copy()\n",
    "    print(\"model options\", model_options)\n",
    "\n",
    "    load_data, prepare_data = get_dataset(dataset)\n",
    "\n",
    "    print('Loading data')\n",
    "    train, valid, test = load_data()\n",
    "\n",
    "    print('Building model')\n",
    "    # This create the initial parameters as numpy ndarrays.\n",
    "    # Dict name (string) -> numpy ndarray\n",
    "    params = init_params(model_options)\n",
    "\n",
    "    if reload_model:\n",
    "        load_params('gru_model.npz', params)\n",
    "\n",
    "    # This create Theano Shared Variable from the parameters.\n",
    "    # Dict name (string) -> Theano Tensor Shared Variable\n",
    "    # params and tparams have different copy of the weights.\n",
    "    tparams = init_tparams(params)\n",
    "\n",
    "    # use_noise is for dropout\n",
    "    (use_noise, x, mask,\n",
    "     y, f_pred_prob, cost) = build_model(tparams, model_options)\n",
    "\n",
    "    all_params = list(tparams.values())\n",
    "\n",
    "    updates = adam(cost, all_params, lrate)\n",
    "\n",
    "    train_function = theano.function(inputs=[x, mask, y], outputs=cost, updates=updates)\n",
    "\n",
    "    print('Optimization')\n",
    "\n",
    "    kf_valid = get_minibatches_idx(len(valid[0]), valid_batch_size)\n",
    "    kf_test = get_minibatches_idx(len(test[0]), valid_batch_size)\n",
    "\n",
    "    print(\"%d train examples\" % len(train[0]))\n",
    "    print(\"%d valid examples\" % len(valid[0]))\n",
    "    print(\"%d test examples\" % len(test[0]))\n",
    "\n",
    "    history_errs = []\n",
    "    history_vali = []\n",
    "    best_p = None\n",
    "    bad_count = 0\n",
    "\n",
    "    uidx = 0  # the number of update done\n",
    "    estop = False  # early stop\n",
    "\n",
    "    try:\n",
    "        for eidx in range(max_epochs):\n",
    "            start_time = time.time()\n",
    "            n_samples = 0\n",
    "            epoch_loss = []\n",
    "\n",
    "            # Get new shuffled index for the training set.\n",
    "            kf = get_minibatches_idx(len(train[0]), batch_size, shuffle=True)\n",
    "\n",
    "            for _, train_index in kf:\n",
    "                uidx += 1\n",
    "                use_noise.set_value(1.)\n",
    "\n",
    "                # Select the random examples for this minibatch\n",
    "                y = [train[1][t] for t in train_index]\n",
    "                x = [train[0][t]for t in train_index]\n",
    "\n",
    "                # Get the data in numpy.ndarray format\n",
    "                # This swap the axis!\n",
    "                # Return something of shape (minibatch maxlen, n samples)\n",
    "                x, mask, y = prepare_data(x, y)\n",
    "                n_samples += x.shape[1]\n",
    "\n",
    "                loss = train_function(x, mask, y)\n",
    "                epoch_loss.append(loss)\n",
    "\n",
    "                if np.isnan(loss) or np.isinf(loss):\n",
    "                    print('bad loss detected: ', loss)\n",
    "                    return 1., 1., 1.\n",
    "\n",
    "                if np.mod(uidx, dispFreq) == 0:\n",
    "                    print('Epoch ', eidx, 'Update ', uidx, 'Loss ', np.mean(epoch_loss))\n",
    "\n",
    "            if saveto and is_save:\n",
    "                print('Saving...')\n",
    "\n",
    "                if best_p is not None:\n",
    "                    params = best_p\n",
    "                else:\n",
    "                    params = unzip(tparams)\n",
    "                np.savez(saveto, history_errs=history_errs, **params)\n",
    "                print('Saving done')\n",
    "\n",
    "            if is_valid:\n",
    "                use_noise.set_value(0.)\n",
    "\n",
    "                valid_evaluation = pred_evaluation(f_pred_prob, prepare_data, valid, kf_valid)\n",
    "                test_evaluation = pred_evaluation(f_pred_prob, prepare_data, test, kf_test)\n",
    "                history_errs.append([valid_evaluation, test_evaluation])\n",
    "\n",
    "                if best_p is None or valid_evaluation[0] >= np.array(history_vali).max():\n",
    "\n",
    "                    best_p = unzip(tparams)\n",
    "                    print('Best perfomance updated!')\n",
    "                    bad_count = 0\n",
    "\n",
    "                print('Valid Recall@20:', valid_evaluation[0], '   Valid Mrr@20:', valid_evaluation[1],\n",
    "                      '\\nTest Recall@20', test_evaluation[0], '   Test Mrr@20:', test_evaluation[1])\n",
    "\n",
    "                if len(history_vali) > 10 and valid_evaluation[0] <= np.array(history_vali).max():\n",
    "                    bad_count += 1\n",
    "                    print('===========================>Bad counter: ' + str(bad_count))\n",
    "                    print('current validation recall: ' + str(valid_evaluation[0]) +\n",
    "                          '      history max recall:' + str(np.array(history_vali).max()))\n",
    "                    if bad_count > patience:\n",
    "                        print('Early Stop!')\n",
    "                        estop = True\n",
    "\n",
    "                history_vali.append(valid_evaluation[0])\n",
    "\n",
    "            end_time = time.time()\n",
    "            print('Seen %d samples' % n_samples)\n",
    "            print(('This epoch took %.1fs' % (end_time - start_time)), file=sys.stderr)\n",
    "\n",
    "            if estop:\n",
    "                break\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Training interupted\")\n",
    "\n",
    "    if best_p is not None:\n",
    "        zipp(best_p, tparams)\n",
    "    else:\n",
    "        best_p = unzip(tparams)\n",
    "\n",
    "    use_noise.set_value(0.)\n",
    "    valid_evaluation = pred_evaluation(f_pred_prob, prepare_data, valid, kf_valid)\n",
    "    test_evaluation = pred_evaluation(f_pred_prob,  prepare_data, test, kf_test)\n",
    "\n",
    "    print('=================Best performance=================')\n",
    "    print('Valid Recall@20:', valid_evaluation[0], '   Valid Mrr@20:', valid_evaluation[1],\n",
    "          '\\nTest Recall@20', test_evaluation[0], '   Test Mrr@20:', test_evaluation[1])\n",
    "    print('==================================================')\n",
    "    if saveto and is_save:\n",
    "        np.savez('Best_performance', valid_evaluation=valid_evaluation, test_evaluation=test_evaluation, history_errs=history_errs,\n",
    "                 **best_p)\n",
    "\n",
    "    return valid_evaluation, test_evaluation\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # See function train for all possible parameter and there definition.\n",
    "    eval_valid, eval_test = train_gru(max_epochs=30, test_size=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
